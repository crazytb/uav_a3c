% Methodology Section
% This file can be included in the main paper.tex using \input{sections/methodology}

\subsection{Problem Formulation}

The multi-UAV task offloading problem is formulated as a sequential decision-making process where each UAV agent must choose from three possible actions for incoming computational tasks: (1) local processing, (2) offloading to mobile edge computing (MEC) servers, or (3) task discard. The objective is to maximize the cumulative reward while minimizing energy consumption and processing delays.

The state space includes:
\begin{itemize}
\item Available computation units (normalized)
\item Remaining epochs for task completion
\item MEC server computation units and processing times
\item Queue status (computation units and processing times)
\item Success indicators for local and offload operations
\item Context features (agent velocity and computation capacity)
\end{itemize}

The action space consists of discrete actions: $A = \{0, 1, 2\}$ representing LOCAL, OFFLOAD, and DISCARD respectively.

The reward function incorporates multiple factors including energy costs, processing delays, and failure penalties:

\begin{equation}
R = -\alpha \cdot E_{local} - \beta \cdot T_{offload} - \gamma \cdot D_{transmission} + \text{rewards} - \text{penalties}
\end{equation}

where $\alpha$, $\beta$, and $\gamma$ are weighting coefficients for local processing energy cost, offload time cost, and transmission delay respectively.

\subsection{A3C Architecture}

The A3C implementation employs both feedforward and recurrent neural network configurations to handle the sequential nature of the task offloading decisions.

\subsubsection{Network Architecture}

The actor-critic architecture consists of:
\begin{itemize}
\item \textbf{Actor Network}: Outputs action probabilities using a softmax policy
\item \textbf{Critic Network}: Estimates state values for advantage calculation
\item \textbf{Hidden Layers}: 128 neurons with ReLU activation functions
\item \textbf{Recurrent Component}: GRU layers for sequence modeling (when enabled)
\end{itemize}

For the recurrent variant (RecurrentActorCritic), the network processes sequences of observations through GRU layers before the final actor and critic heads, enabling the model to maintain memory of past decisions and environmental states.

\subsubsection{Training Strategies}

We evaluate two distinct training paradigms:

\textbf{A3C Global Training:} In this centralized approach, all workers share updates to a single global model. Each worker interacts with its assigned environment and computes gradients based on the advantage actor-critic loss:

\begin{equation}
L = L_{policy} + \beta \cdot L_{value} - \alpha \cdot H(\pi)
\end{equation}

where $L_{policy}$ is the policy loss, $L_{value}$ is the value function loss, and $H(\pi)$ is the policy entropy term for exploration.

The global model aggregates updates from all workers asynchronously, promoting knowledge sharing across different environmental conditions and worker experiences.

\textbf{Individual Worker Training:} In this decentralized approach, each worker develops its own specialized policy independently. Workers train separate neural networks without sharing parameters, allowing for environment-specific adaptation and specialized behavior development.

\subsection{Environment Configuration}

The experimental framework utilizes a custom UAV environment with the following key parameters:
\begin{itemize}
\item Maximum computation units: 200
\item Maximum computation units for cloud: 1000
\item Maximum epoch size: 100
\item Maximum queue size: 20
\item Agent velocities: 50 units
\end{itemize}

Five distinct environmental configurations are evaluated, each representing different operational scenarios with varying computational demands, network conditions, and resource availability patterns.
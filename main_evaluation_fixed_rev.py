# main_evaluation.py  (RNN í‰ê°€ìš©)

import torch
import numpy as np
import os
import csv
from torch.distributions import Categorical
from typing import Callable, Dict, Any

from drl_framework.networks import RecurrentActorCritic  # ë³€ê²½: RNN ëª¨ë¸ ì‚¬ìš©
from drl_framework.custom_env import make_env
from drl_framework.utils import flatten_dict_values
import drl_framework.params as params
import copy

# íƒ€ì„ìŠ¤íƒ¬í”„
stamp = "20250820_141900"  # ì˜ˆì‹œ íƒ€ì„ìŠ¤íƒ¬í”„, í•„ìš”ì— ë”°ë¼ ë³€ê²½

def _deep_copy_obs(obs: Dict[str, Any]) -> Dict[str, Any]:
    """ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ê´€ì¸¡ê°’ì„ ê¹Šì€ ë³µì‚¬í•˜ì—¬ ë°˜í™˜"""
    out = {}
    for k, v in obs.items():
        if isinstance(v, np.ndarray):
            out[k] = v.copy()
        else:
            out[k] = copy.deepcopy(v)
    return out

device = params.device
ENV_PARAMS = params.ENV_PARAMS
REWARD_PARAMS = params.REWARD_PARAMS
hidden_dim = params.hidden_dim
n_workers = params.n_workers

# ì„ì‹œ envë¡œ ìƒíƒœ/í–‰ë™ ì°¨ì› íŒŒì•…
temp_env_fn = make_env(**ENV_PARAMS)
temp_env = temp_env_fn()
sample_obs, _ = temp_env.reset()
state_dim = len(flatten_dict_values(sample_obs))
action_dim = temp_env.action_space.n
temp_env.close()

# í‰ê°€ìš© í™˜ê²½ íŒŒë¼ë¯¸í„° ë¦¬ìŠ¤íŠ¸ (ì›ë³¸ ë¡œì§ ìœ ì§€)
env_param_list = []
for _ in range(n_workers):
    e = copy.deepcopy(ENV_PARAMS)
    e["max_comp_units"] = np.random.randint(40, 161)
    # e["agent_velocities"] = np.random.randint(30, 101)
    env_param_list.append(e)

@torch.no_grad()
def evaluate_model_on_env(model_path, env_kwargs, n_episodes=100, greedy=True, render=False,
                          input_transform: Callable[[Dict[str, Any]], Dict[str, Any]] = None):
    """
    ë‹¨ì¼ í™˜ê²½ì—ì„œ pth ëª¨ë¸ í‰ê°€.
    input_transform(obs_dict) -> obs_dict' ë¥¼ ì£¼ë©´ 'ì—ì´ì „íŠ¸ ì…ë ¥'ë§Œ êµë€(í¼ë®¤í…Œì´ì…˜/ê³ ì • ë“±)í•´ í‰ê°€.
    í™˜ê²½ ë‚´ë¶€ ìƒíƒœëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ë˜ì–´ ì•ˆì •ì ì´ë‹¤.
    """
    # ëª¨ë¸ ë¡œë“œ (RNN)
    model = RecurrentActorCritic(state_dim, action_dim, hidden_dim).to(device)
    sd = torch.load(model_path, map_location=device)
    try:
        model.load_state_dict(sd, strict=True)
    except Exception as e:
        # í˜¸í™˜ ë¡œë”ë¡œ ì¬ì‹œë„ (shared.* â†’ feature.* ë“±)
        load_weights_compat(model, sd, rename_rules=[("shared.", "feature.")], verbose=True)
    model.eval()

    # í™˜ê²½
    env_fn = make_env(**env_kwargs, reward_params=REWARD_PARAMS)
    env = env_fn()

    episode_rewards, episode_lengths = [], []

    for _ in range(n_episodes):
        if input_transform is not None and hasattr(input_transform, "reset"):
            input_transform.reset()  # â† ì—í”¼ì†Œë“œ ì‹œì‘ë§ˆë‹¤ ë„ë„ˆ ì‹œí€€ìŠ¤ ë¦¬ì…‹
        
        obs, _ = env.reset()
        done = False
        total_reward = 0.0
        steps = 0

        hx = model.init_hidden(batch_size=1, device=device)

        while not done and steps < env.max_epoch_size:
            # --- ê´€ì°° dict -> (ì„ íƒ) ì…ë ¥ êµë€ -> í…ì„œ ---
            obs_for_model = _deep_copy_obs(obs)
            if input_transform is not None:
                obs_for_model = input_transform(obs_for_model)

            obs_tensor = torch.as_tensor(
                flatten_dict_values(obs_for_model), dtype=torch.float32, device=device
            ).unsqueeze(0)  # (1, state_dim)

            logits, value, hx = model.step(obs_tensor, hx)
            if greedy:
                action = torch.argmax(logits, dim=-1).item()
            else:
                action = Categorical(logits=logits).sample().item()

            next_obs, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            total_reward += float(reward)
            steps += 1
            obs = next_obs

            if render:
                env.render()

            if done:
                hx = hx * 0.0

        episode_rewards.append(total_reward)
        episode_lengths.append(steps)

    env.close()

    return {
        "mean_reward": float(np.mean(episode_rewards)),
        "std_reward": float(np.std(episode_rewards)),
        "min_reward": float(np.min(episode_rewards)),
        "max_reward": float(np.max(episode_rewards)),
        "mean_length": float(np.mean(episode_lengths)),
        "rewards": episode_rewards,
    }

def collect_feature_buffers(model_path, env_kwargs, keys=None, n_episodes=30, greedy=True,
                            max_per_key=5000, seed=0):
    """
    í¼ë®¤í…Œì´ì…˜ ì„í¬í„´ìŠ¤ìš©ìœ¼ë¡œ, ì—ì´ì „íŠ¸ê°€ ì‹¤ì œë¡œ ë³´ê²Œ ë˜ëŠ” ê´€ì°°ê°’ ë¶„í¬ë¥¼ í‚¤ë³„ë¡œ ìˆ˜ì§‘.
    - í™˜ê²½ ìƒíƒœëŠ” ê·¸ëŒ€ë¡œ, 'ì—ì´ì „íŠ¸ ì…ë ¥'ë§Œ ë°”ê¿” í‰ê°€í•˜ë¯€ë¡œ shape/type mismatch ìœ„í—˜ì´ ì ë‹¤.
    """
    rng = np.random.default_rng(seed)

    # ëª¨ë¸ ë¡œë“œ (í‰ê°€ ì „íŒŒë§Œ)
    model = RecurrentActorCritic(state_dim, action_dim, hidden_dim).to(device)
    sd = torch.load(model_path, map_location=device)
    try:
        model.load_state_dict(sd)
    except Exception as e:
        # í˜¸í™˜ ë¡œë”ë¡œ ì¬ì‹œë„ (shared.* â†’ feature.* ë“± í‚¤ ë¦¬ë§¤í•‘)
        load_weights_compat(
            model,
            sd,  # ë˜ëŠ” model_path (ë¬¸ìì—´). ë‘˜ ë‹¤ ì§€ì›
            rename_rules=[("shared.", "feature.")],
            verbose=True,
        )
    model.eval()

    env_fn = make_env(**env_kwargs, reward_params=REWARD_PARAMS)
    env = env_fn()

    buffers: Dict[str, list] = {}
    episodes = 0
    with torch.no_grad():
        while episodes < n_episodes:
            obs, _ = env.reset()
            done, steps = False, 0
            hx = model.init_hidden(batch_size=1, device=device)

            # í‚¤ ì´ˆê¸°í™”
            if keys is None:
                keys = list(obs.keys())

            while not done and steps < env.max_epoch_size:
                # ê´€ì°°ê°’ë“¤ì„ ë²„í¼ì— ì €ì¥
                for k in keys:
                    v = obs[k]
                    if k not in buffers:
                        buffers[k] = []
                    # ìš©ëŸ‰ ì œí•œ
                    if len(buffers[k]) < max_per_key:
                        buffers[k].append(v.copy() if isinstance(v, np.ndarray) else v)

                # í–‰ë™
                x = torch.as_tensor(flatten_dict_values(obs), dtype=torch.float32, device=device).unsqueeze(0)
                logits, _, hx = model.step(x, hx)
                if greedy:
                    a = torch.argmax(logits, dim=-1).item()
                else:
                    a = Categorical(logits=logits).sample().item()

                obs, r, terminated, truncated, _ = env.step(a)
                done = terminated or truncated
                steps += 1

            episodes += 1

    env.close()

    # numpy ë°°ì—´ë¡œ ë°”ê¿” ë‘ë©´ ì¶”í›„ ìƒ˜í”Œë§ì´ ë¹ ë¦„
    for k in buffers:
        buffers[k] = np.array(buffers[k], dtype=object)  # ë‹¤ì–‘í•œ shapeì„ ë‹´ê¸° ìœ„í•´ object ë°°ì—´
    return buffers

def collect_feature_buffers_ep(model_path, env_kwargs, keys=None, n_episodes=30, greedy=True, seed=0):
    """
    í¼ë®¤í…Œì´ì…˜ì„ 'ì—í”¼ì†Œë“œ ë‹¨ìœ„'ë¡œ í•˜ê¸° ìœ„í•´, ê° keyì˜ 'ì‹œí€€ìŠ¤(ì—í”¼ì†Œë“œ ì „ì²´)'ë¥¼ ëª¨ìë‹ˆë‹¤.
    ë°˜í™˜: {key: [seq0, seq1, ...]}, seqëŠ” ë¦¬ìŠ¤íŠ¸(ìŠ¤í… ê¸¸ì´ë§Œí¼)ì˜ ê°’ë“¤
    """
    rng = np.random.default_rng(seed)

    # ëª¨ë¸ ë¡œë“œ
    model = RecurrentActorCritic(state_dim, action_dim, hidden_dim).to(device)
    sd = torch.load(model_path, map_location=device)
    try:
        model.load_state_dict(sd, strict=True)
    except Exception:
        load_weights_compat(model, sd, rename_rules=[("shared.", "feature.")], verbose=False)
    model.eval()

    env_fn = make_env(**env_kwargs, reward_params=REWARD_PARAMS)
    env = env_fn()

    buffers_ep = {}
    episodes = 0
    with torch.no_grad():
        while episodes < n_episodes:
            obs, _ = env.reset()
            done, steps = False, 0
            hx = model.init_hidden(batch_size=1, device=device)

            # í‚¤ ì´ˆê¸°í™”
            if keys is None:
                keys = list(obs.keys())
            seq = {k: [] for k in keys}

            while not done and steps < env.max_epoch_size:
                for k in keys:
                    v = obs[k]
                    seq[k].append(v.copy() if isinstance(v, np.ndarray) else v)

                x = torch.as_tensor(flatten_dict_values(obs), dtype=torch.float32, device=device).unsqueeze(0)
                logits, _, hx = model.step(x, hx)
                a = torch.argmax(logits, dim=-1).item() if greedy else Categorical(logits=logits).sample().item()

                obs, r, term, trunc, _ = env.step(a)
                done = term or trunc
                steps += 1

            for k in keys:
                buffers_ep.setdefault(k, []).append(seq[k])
            episodes += 1

    env.close()
    return buffers_ep


# ====== [ì¶”ê°€] ë‹¨ì¼ í‚¤ í¼ë®¤í…Œì´ì…˜ ë³€í™˜ê¸° ë§Œë“¤ê¸° ======

def make_permute_transform(buffers: Dict[str, np.ndarray], key: str, seed: int = 0):
    """
    ì£¼ì–´ì§„ keyë§Œ ë‹¤ë¥¸ ì‹œì ì˜ ê°’ìœ¼ë¡œ ë¬´ì‘ìœ„ ì¹˜í™˜í•˜ëŠ” transform(obs)->obs'.
    - obs dictë¥¼ ë°›ì•„ ê°™ì€ íƒ€ì…/shapeì˜ ê°’ìœ¼ë¡œ êµì²´.
    """
    rng = np.random.default_rng(seed)

    if key not in buffers or len(buffers[key]) == 0:
        raise ValueError(f"[permute] buffer for key '{key}' is empty.")

    def transform(obs: Dict[str, Any]) -> Dict[str, Any]:
        out = obs  # ì´ë¯¸ ìƒìœ„ì—ì„œ deepcopyë¨
        cand = buffers[key][rng.integers(len(buffers[key]))]
        # dtype/shape ë§ì¶¤
        if isinstance(out[key], np.ndarray):
            cand = np.asarray(cand)
            if cand.shape != out[key].shape:
                # shapeì´ ë‹¤ë¥´ë©´ ê°€ëŠ¥í•œ ë²”ìœ„ì—ì„œ ì˜ë¼ë‚´ê±°ë‚˜ padí•˜ì§€ ë§ê³ , ì›ë˜ ê°’ ìœ ì§€
                # (ì„œë¡œ ë‹¤ë¥¸ í™˜ê²½ êµ¬ì„±ì—ì„œ shapeê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „ì¥ì¹˜)
                return out
            out[key] = cand.astype(out[key].dtype, copy=False)
        else:
            # ìŠ¤ì¹¼ë¼/íŒŒì´ì¬ íƒ€ì…
            out[key] = type(out[key])(cand)
        return out

    return transform


# ====== [ì¶”ê°€] í¼ë®¤í…Œì´ì…˜ ì„í¬í„´ìŠ¤ (ë‹¨ì¼ í™˜ê²½) ======

def permutation_importance_on_env(model_path, env_kwargs, keys=None,
                                  collect_episodes=30, eval_episodes=80,
                                  greedy=True, seed=0):
    """
    ë‹¨ì¼ í™˜ê²½ì—ì„œ í¼ë®¤í…Œì´ì…˜ ì„í¬í„´ìŠ¤ ê³„ì‚°.
    ë°˜í™˜: {'baseline': float, 'drops': {key: drop}, 'scores': {key: mean_reward_when_permuted}}
    """
    # 0) baseline
    base = evaluate_model_on_env(model_path, env_kwargs, n_episodes=eval_episodes, greedy=greedy)
    baseline = base["mean_reward"]

    # 1) buffers ìˆ˜ì§‘
    buffers = collect_feature_buffers(model_path, env_kwargs, keys=keys,
                                      n_episodes=collect_episodes, greedy=greedy, seed=seed)

    # 2) í‚¤ ëª©ë¡
    if keys is None:
        keys = list(buffers.keys())

    # 3) í‚¤ë³„ ì„±ëŠ¥ ì¸¡ì •
    drops, scores = {}, {}
    for k in keys:
        transform = make_permute_transform(buffers, k, seed=seed+123)
        res = evaluate_model_on_env(model_path, env_kwargs, n_episodes=eval_episodes,
                                    greedy=greedy, input_transform=transform)
        score = res["mean_reward"]
        drops[k] = baseline - score
        scores[k] = score

    return {"baseline": baseline, "drops": drops, "scores": scores}


# ====== [ì¶”ê°€] ì—¬ëŸ¬ í™˜ê²½ í‰ê·  í¼ë®¤í…Œì´ì…˜ ì„í¬í„´ìŠ¤ ======

def permutation_importance_over_envs(model_path, env_param_list, keys=None,
                                     collect_episodes=20, eval_episodes=50,
                                     greedy=True, max_envs=None, seed=0):
    """
    ì—¬ëŸ¬ í™˜ê²½ íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ í¼ë®¤í…Œì´ì…˜ ì„í¬í„´ìŠ¤ë¥¼ ê³„ì‚°í•˜ê³  í‰ê· /í‘œì¤€í¸ì°¨ë¥¼ ë¦¬í„´.
    """
    if max_envs is None:
        max_envs = len(env_param_list)

    all_keys = None
    baselines, per_key_scores = [], {}

    for idx, env_kwargs in enumerate(env_param_list[:max_envs]):
        out = permutation_importance_on_env(model_path, env_kwargs, keys=keys,
                                            collect_episodes=collect_episodes,
                                            eval_episodes=eval_episodes,
                                            greedy=greedy, seed=seed + idx*7)
        baselines.append(out["baseline"])

        if all_keys is None:
            all_keys = list(out["drops"].keys())

        for k in all_keys:
            per_key_scores.setdefault(k, []).append(out["drops"][k])

    # í‰ê· /í‘œì¤€í¸ì°¨ ìš”ì•½
    mean_drop = {k: float(np.mean(per_key_scores[k])) for k in per_key_scores}
    std_drop  = {k: float(np.std(per_key_scores[k]))  for k in per_key_scores}

    summary = {
        "baseline_mean": float(np.mean(baselines)),
        "baseline_std":  float(np.std(baselines)),
        "mean_drop": mean_drop,
        "std_drop": std_drop,
        "sorted": sorted(mean_drop.items(), key=lambda x: x[1], reverse=True),
    }
    return summary

def evaluate_over_envs(model_path, env_param_list, n_episodes=100, greedy=True):
    """ì—¬ëŸ¬ í™˜ê²½ íŒŒë¼ë¯¸í„°ì—ì„œ ê°™ì€ ëª¨ë¸ì„ í‰ê°€í•˜ê³  í‰ê· """
    per_env = []
    for idx, e in enumerate(env_param_list):
        print(f"  - evaluating on env {idx+1}/{len(env_param_list)} ...")
        res = evaluate_model_on_env(model_path, e, n_episodes=n_episodes, greedy=greedy)
        res["env_id"] = idx
        per_env.append(res)

    # í™˜ê²½ ê°„ í‰ê· (ìš”ì•½)
    all_rewards = np.concatenate([np.array(r["rewards"]) for r in per_env]) if per_env else np.array([0.0])
    summary = {
        "mean_reward": float(np.mean([r["mean_reward"] for r in per_env])) if per_env else 0.0,
        "std_reward": float(np.std(all_rewards)) if per_env else 0.0,
        "min_reward": float(np.min(all_rewards)) if per_env else 0.0,
        "max_reward": float(np.max(all_rewards)) if per_env else 0.0,
        "mean_length": float(np.mean([r["mean_length"] for r in per_env])) if per_env else 0.0,
        "per_env": per_env,
    }
    return summary


def compare_all_models(env_param_list, n_episodes=100, greedy=True):
    """ì—¬ëŸ¬ ëª¨ë¸(pth)ë“¤ì„ ë™ì¼í•œ env ì„¸íŠ¸ì—ì„œ ë¹„êµ"""
    results = {}

    # A3C ê¸€ë¡œë²Œ ëª¨ë¸
    gpath = f"runs/a3c_{stamp}/models/global_final.pth"
    if os.path.exists(gpath):
        print("Evaluating A3C Global Model...")
        results["A3C_Global"] = evaluate_over_envs(gpath, env_param_list, n_episodes, greedy)

    # ê°œë³„(Individual) ëª¨ë¸ë“¤
    individual_results = []
    for i in range(params.n_workers):
        mpath = f"runs/individual_{stamp}/models/individual_worker_{i}_final.pth"
        if os.path.exists(mpath):
            print(f"Evaluating Individual Worker {i}...")
            r = evaluate_over_envs(mpath, env_param_list, n_episodes, greedy)
            r["worker_id"] = i
            individual_results.append(r)
    results["Individual_Workers"] = individual_results

    return results


def print_results(results):
    print("\n" + "=" * 60)
    print("MODEL EVALUATION RESULTS")
    print("=" * 60)

    # A3C Global
    if "A3C_Global" in results:
        r = results["A3C_Global"]
        print(f"\nğŸ“Š A3C Global Model (avg over envs):")
        print(f"  Mean Reward: {r['mean_reward']:.2f} (std over all eps: {r['std_reward']:.2f})")
        print(f"  Range: [{r['min_reward']:.2f}, {r['max_reward']:.2f}]")
        print(f"  Mean Episode Length: {r['mean_length']:.1f}")

    # Individual Workers
    if "Individual_Workers" in results and results["Individual_Workers"]:
        print(f"\nğŸ“Š Individual Workers (avg over envs):")
        all_individual_means = []
        for r in results["Individual_Workers"]:
            wid = r["worker_id"]
            print(f"  Worker {wid}: mean {r['mean_reward']:.2f}")
            all_individual_means.append(r["mean_reward"])

        if all_individual_means:
            print(f"\n  ğŸ“ˆ Individual Workers Overall:")
            print(f"    Mean of means: {np.mean(all_individual_means):.2f}")
            print(f"    Std of means : {np.std(all_individual_means):.2f}")

    # ë¹„êµ
    if "A3C_Global" in results and results["Individual_Workers"]:
        a3c_mean = results["A3C_Global"]["mean_reward"]
        individual_means = [r["mean_reward"] for r in results["Individual_Workers"]]
        best_ind = max(individual_means) if individual_means else 0.0
        avg_ind = float(np.mean(individual_means)) if individual_means else 0.0

        print(f"\nğŸ† COMPARISON (avg over envs):")
        print(f"  A3C Global vs Best Individual: {a3c_mean:.2f} vs {best_ind:.2f}")
        print(f"  A3C Global vs Avg Individual : {a3c_mean:.2f} vs {avg_ind:.2f}")
        improvement = ((a3c_mean - avg_ind) / (abs(avg_ind) + 1e-8)) * 100.0 if avg_ind != 0 else 0.0
        print(f"  A3C Improvement: {improvement:+.1f}%")

def save_detailed_results(results, filename="evaluation_results.csv"):
    with open(filename, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["Model", "Mean_Reward", "Std_Reward", "Min_Reward", "Max_Reward", "Mean_Length"])

        if "A3C_Global" in results:
            r = results["A3C_Global"]
            writer.writerow(["A3C_Global", r["mean_reward"], r["std_reward"],
                             r["min_reward"], r["max_reward"], r["mean_length"]])

        if "Individual_Workers" in results:
            for r in results["Individual_Workers"]:
                wid = r["worker_id"]
                writer.writerow([f"Individual_Worker_{wid}", r["mean_reward"], r["std_reward"],
                                 r["min_reward"], r["max_reward"], r["mean_length"]])

def load_weights_compat(model, raw_state_dict, *, 
                        rename_rules=None, strip_prefix="module.", verbose=True):
    """
    Load a checkpoint into `model` tolerantly:
      - supports {'state_dict': ...} wrappers
      - strips 'module.' (DataParallel) prefix
      - applies simple rename rules, e.g. [('shared.', 'feature.')]
      - loads only keys that exist in the model with matching shapes (strict=False)
    Returns a dict with summary counts.
    """
    # 1) extract actual state_dict
    sd = raw_state_dict
    if isinstance(sd, str):
        sd = torch.load(sd, map_location="cpu")
    if isinstance(sd, dict) and "state_dict" in sd and isinstance(sd["state_dict"], dict):
        sd = sd["state_dict"]

    # 2) strip prefix
    def strip(k):
        return k[len(strip_prefix):] if strip_prefix and k.startswith(strip_prefix) else k

    # 3) rename rules (default: map 'shared.' â†’ 'feature.')
    if rename_rules is None:
        rename_rules = [("shared.", "feature.")]
    def apply_rules(k):
        k2 = k
        for src, dst in rename_rules:
            if k2.startswith(src):
                k2 = dst + k2[len(src):]
        return k2

    remapped = {}
    for k, v in sd.items():
        k2 = apply_rules(strip(k))
        remapped[k2] = v

    # 4) keep only intersecting keys with matching shapes
    msd = model.state_dict()
    loadable = {k: v for k, v in remapped.items() if (k in msd and msd[k].shape == v.shape)}
    missing = [k for k in msd.keys() if k not in loadable]
    unexpected = [k for k in remapped.keys() if k not in msd]
    shape_mismatch = [k for k in remapped.keys() if (k in msd and msd[k].shape != remapped[k].shape)]

    # 5) load (non-strict)
    model.load_state_dict(loadable, strict=False)

    if verbose:
        print("[load_weights_compat] loaded:", len(loadable),
              "| missing:", len(missing),
              "| unexpected:", len(unexpected),
              "| shape_mismatch:", len(shape_mismatch))
        # ê°€ì¥ í”í•œ ì¼€ì´ìŠ¤ ì•ˆë‚´ (sharedâ†’feature)
        if any(k.startswith("shared.") for k in sd.keys()):
            print("  note: remapped 'shared.*' â†’ 'feature.*' for RecurrentActorCritic.")

    return {
        "loaded": list(loadable.keys()),
        "missing": missing,
        "unexpected": unexpected,
        "shape_mismatch": shape_mismatch,
    }

def permutation_importance_groups_on_env(model_path, env_kwargs, groups,
                                         collect_episodes=30, eval_episodes=200,
                                         greedy=True, seed=0):
    """
    ê·¸ë£¹ ë‹¨ìœ„(êµ¬ì¡° ë³´ì¡´) í¼ë®¤í…Œì´ì…˜ ì„í¬í„´ìŠ¤.
    """
    # baseline
    base = evaluate_model_on_env(model_path, env_kwargs, n_episodes=eval_episodes, greedy=greedy)
    baseline = base["mean_reward"]

    # ì—í”¼ì†Œë“œ-ë²„í¼ ìˆ˜ì§‘
    buffers_ep = collect_feature_buffers_ep(model_path, env_kwargs, keys=list(set(sum([list(g) for g in groups], []))),
                                            n_episodes=collect_episodes, greedy=greedy, seed=seed)

    drops = {}
    for g in groups:
        transform = EpisodeGroupPermuter(buffers_ep, g, seed=seed+7)
        res = evaluate_model_on_env(model_path, env_kwargs, n_episodes=eval_episodes,
                                    greedy=greedy, input_transform=transform)
        drops[g] = baseline - res["mean_reward"]

    return baseline, drops


def occlusion_importance_on_env(model_path, env_kwargs, groups_or_keys,
                                how="mean", collect_episodes=30, eval_episodes=200,
                                greedy=True, seed=0):
    """
    ê·¸ë£¹/í‚¤ë¥¼ ìƒìˆ˜ë¡œ ê³ ì •í–ˆì„ ë•Œ Î”reward ì¸¡ì •.
    how: "mean" ë˜ëŠ” "zero"
    """
    # baseline
    base = evaluate_model_on_env(model_path, env_kwargs, n_episodes=eval_episodes, greedy=greedy)
    baseline = base["mean_reward"]

    # í‰ê·  ìƒìˆ˜ ë§Œë“¤ê¸°
    buffers_ep = collect_feature_buffers_ep(model_path, env_kwargs, keys=list(set(sum([list(g) if isinstance(g, (list, tuple)) else [g] for g in groups_or_keys], []))),
                                            n_episodes=collect_episodes, greedy=greedy, seed=seed)

    # ì—í”¼ì†Œë“œ ë²„í¼ë¥¼ í‰íƒ„í™”í•´ì„œ í‰ê· ì„ ê³„ì‚°
    const_dict = {}
    for g in groups_or_keys:
        keys = g if isinstance(g, (list, tuple)) else (g,)
        for k in keys:
            seqs = buffers_ep[k]
            flat = []
            for s in seqs: flat += s
            v0 = flat[0]
            if isinstance(v0, np.ndarray):
                arr = np.stack([np.asarray(v) for v in flat], axis=0)
                const = (arr.mean(axis=0) if how=="mean" else np.zeros_like(arr.mean(axis=0)))
            else:
                vals = np.array(flat, dtype=np.float32)
                const = (float(vals.mean()) if how=="mean" else 0.0)
            const_dict[k] = const

    transform = KeyConstantOccluder(const_dict)
    res = evaluate_model_on_env(model_path, env_kwargs, n_episodes=eval_episodes,
                                greedy=greedy, input_transform=transform)
    return baseline, baseline - res["mean_reward"]


class EpisodeGroupPermuter:
    """
    ì§€ì •ëœ group_keysë¥¼ 'í•˜ë‚˜ì˜ ë„ë„ˆ ì—í”¼ì†Œë“œ' ì‹œí€€ìŠ¤ë¡œ ì¹˜í™˜.
    evaluate_model_on_env()ê°€ ë§¤ ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œ reset()ì„ í˜¸ì¶œí•´ì•¼ í•¨.
    """
    def __init__(self, buffers_ep: dict, group_keys, seed=0):
        self.buffers_ep = buffers_ep
        self.group_keys = tuple(group_keys)
        self.rng = np.random.default_rng(seed)
        self.donor_idx = None
        self.t = 0

    def reset(self):
        # ë„ë„ˆ ì—í”¼ì†Œë“œ í•˜ë‚˜ë¥¼ ë½‘ê³ , ìŠ¤í… ì¸ë±ìŠ¤ 0ìœ¼ë¡œ ë¦¬ì…‹
        any_key = self.group_keys[0]
        n_eps = len(self.buffers_ep[any_key])
        self.donor_idx = int(self.rng.integers(n_eps))
        self.t = 0

    def __call__(self, obs: Dict[str, Any]) -> Dict[str, Any]:
        out = _deep_copy_obs(obs)
        if self.donor_idx is None:
            self.reset()
        for k in self.group_keys:
            seq = self.buffers_ep[k][self.donor_idx]
            val = seq[min(self.t, len(seq)-1)]
            if isinstance(out[k], np.ndarray):
                out[k] = np.asarray(val, dtype=out[k].dtype)
            else:
                out[k] = type(out[k])(val)
        self.t += 1
        return out

class KeyConstantOccluder:
    """
    íŠ¹ì • key(ë˜ëŠ” ê·¸ë£¹)ë¥¼ ìƒìˆ˜ë¡œ ê³ ì •(í‰ê· /0 ë“±). ê·¸ë£¹ì´ë©´ ê°™ì€ ê°’ìœ¼ë¡œ ëª¨ë‘ ê³ ì •.
    """
    def __init__(self, const_dict: Dict[str, Any]):
        self.const_dict = const_dict

    def reset(self):  # evaluateì—ì„œ í˜¸ì¶œí•´ë„ ë¬´í•´
        pass

    def __call__(self, obs: Dict[str, Any]) -> Dict[str, Any]:
        out = _deep_copy_obs(obs)
        for k, const in self.const_dict.items():
            v = out[k]
            out[k] = (np.asarray(const, dtype=v.dtype)
                      if isinstance(v, np.ndarray) else type(v)(const))
        return out


# ----------------------------
# Policy dataset & bin stats
# ----------------------------
import numpy as np
import torch
from torch.distributions import Categorical

@torch.no_grad()
def collect_policy_dataset(model_path, env_kwargs, n_episodes=200, greedy=True):
    """
    ì •ì±…ì´ ì‹¤ì œë¡œ ë‚´ë¦¬ëŠ” í–‰ë™/í™•ë¥ ê³¼ ê´€ì°°ì„ ìˆ˜ì§‘.
    rows[i] = { 'obs': <obs_dict>, 'p1': P(a=1), 'a': action(int) }
    """
    # ëª¨ë¸ ë¡œë“œ
    model = RecurrentActorCritic(state_dim, action_dim, hidden_dim).to(device)
    sd = torch.load(model_path, map_location=device)
    try:
        model.load_state_dict(sd, strict=True)
    except Exception:
        load_weights_compat(model, sd, rename_rules=[("shared.", "feature.")], verbose=False)
    model.eval()

    # í™˜ê²½
    env = make_env(**env_kwargs, reward_params=REWARD_PARAMS)()
    rows = []

    for _ in range(n_episodes):
        obs, _ = env.reset()
        done = False
        hx = model.init_hidden(batch_size=1, device=device)
        t = 0
        while not done and t < env.max_epoch_size:
            x = torch.as_tensor(
                flatten_dict_values(obs), dtype=torch.float32, device=device
            ).unsqueeze(0)
            logits, _, hx = model.step(x, hx)
            probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]
            if greedy:
                a = int(np.argmax(probs))
            else:
                a = int(Categorical(logits=logits).sample().item())
            rows.append({
                "obs": {k: (v.copy() if isinstance(v, np.ndarray) else v) for k, v in obs.items()},
                "p1": float(probs[1]),
                "a": a,
            })
            obs, r, term, trunc, _ = env.step(a)
            done = term or trunc
            t += 1

    env.close()
    return rows


def _scalarize(value, how="first"):
    """ë²¡í„° í”¼ì²˜ë¥¼ binningì— ì“°ê¸° ìœ„í•œ ìŠ¤ì¹¼ë¼ ìš”ì•½."""
    arr = np.asarray(value).reshape(-1)
    if how == "first": return float(arr[0])
    if how == "mean":  return float(arr.mean())
    if how == "max":   return float(arr.max())
    if how == "min":   return float(arr.min())
    return float(arr[0])


def bin_stat(rows, key, n_bins=10, how="first"):
    """
    p(a=1 | feature-bin) ê³„ì‚°.
    - key: ê´€ì°° ë”•ì…”ë„ˆë¦¬ì˜ í‚¤
    - how: ë²¡í„°ì¼ ê²½ìš° ì‚¬ìš©í•  ìš”ì•½ ë°©ì‹ ('first'|'mean'|'max'|'min')
    ë°˜í™˜: mids(ë¹ˆ ì¤‘ì•™), p_mean(ê° binì—ì„œ p1 í‰ê· ), cnt(ìƒ˜í”Œ ìˆ˜)
    """
    vals = np.array([_scalarize(r["obs"][key], how=how) for r in rows], dtype=np.float32)
    p1   = np.array([r["p1"] for r in rows], dtype=np.float32)

    # ê°’ì´ [0,1] ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ìë™ min-max ì •ê·œí™” (ì•ˆì „ì¥ì¹˜)
    vmin, vmax = np.nanmin(vals), np.nanmax(vals)
    if vmax > 1.0 or vmin < 0.0:
        rng = (vmax - vmin) if (vmax > vmin) else 1.0
        vals = (vals - vmin) / rng

    bins = np.linspace(0.0, 1.0, n_bins + 1)
    mids, p_mean, cnt = [], [], []
    for i in range(n_bins):
        m = (vals >= bins[i]) & (vals < bins[i+1])
        mids.append((bins[i] + bins[i+1]) / 2.0)
        if m.sum() == 0:
            p_mean.append(np.nan); cnt.append(0)
        else:
            p_mean.append(float(p1[m].mean()))
            cnt.append(int(m.sum()))
    return np.array(mids), np.array(p_mean), np.array(cnt)

# ----------------------------
# Gradient-based importance
# ----------------------------

def _key_slices_from_example_obs(obs):
    """
    flatten_dict_valuesì˜ 'í‚¤ ìˆœì„œ ê·¸ëŒ€ë¡œ' ìŠ¬ë¼ì´ìŠ¤ ì¸ë±ìŠ¤ ê³„ì‚°.
    (Python 3.7+ dictëŠ” ì…ë ¥ ìˆœì„œ ë³´ì¥)
    """
    key_slices = {}
    idx = 0
    for k, v in obs.items():
        L = int(np.asarray(v).size)
        key_slices[k] = slice(idx, idx + L)
        idx += L
    return key_slices, idx  # ì „ì²´ state_dim


def gradient_importance(model_path, env_kwargs, sample_steps=2048, greedy=True):
    """
    ë¡œì§“ ì°¨ (logit1 - logit0)ì˜ ì…ë ¥ ë¯¼ê°ë„ |d(logit diff)/dx| í‰ê· ì„ í‚¤ë³„ë¡œ ì§‘ê³„.
    ê°’ì´ í´ìˆ˜ë¡ í˜„ì¬ ì‹œì ì˜ ì˜ì‚¬ê²°ì •ì— ë¯¼ê°.
    """
    # ëª¨ë¸ ë¡œë“œ
    model = RecurrentActorCritic(state_dim, action_dim, hidden_dim).to(device)
    sd = torch.load(model_path, map_location=device)
    try:
        model.load_state_dict(sd, strict=True)
    except Exception:
        load_weights_compat(model, sd, rename_rules=[("shared.", "feature.")], verbose=False)
    model.eval()

    # í™˜ê²½
    env = make_env(**env_kwargs, reward_params=REWARD_PARAMS)()

    # ìƒ˜í”Œ ìˆ˜ì§‘ (x, hx) í˜ì–´
    xs, hxs = [], []
    obs, _ = env.reset()
    hx = model.init_hidden(batch_size=1, device=device)
    key_slices, flat_dim = _key_slices_from_example_obs(obs)

    with torch.no_grad():
        steps = 0
        done = False
        while steps < sample_steps:
            x = torch.as_tensor(
                flatten_dict_values(obs), dtype=torch.float32, device=device
            ).unsqueeze(0)  # (1, state_dim)
            logits, _, hx = model.step(x, hx)
            a = int(torch.argmax(logits, dim=-1).item()) if greedy else int(Categorical(logits=logits).sample().item())
            xs.append(x.squeeze(0).detach())  # (state_dim,)
            hxs.append(hx.detach())            # (1,1,H)
            obs, r, term, trunc, _ = env.step(a)
            done = term or trunc
            steps += 1
            if done:
                obs, _ = env.reset()
                hx = model.init_hidden(batch_size=1, device=device)
                done = False

    # ìƒ˜í”Œë³„ë¡œ ê°œë³„ backprop (RNN hiddenì„ ê³ ì • ìƒìˆ˜ë¡œ ë³´ê³  í˜„ì¬ ì‹œì  ë¯¼ê°ë„ë§Œ ê³„ì‚°)
    grad_accum = torch.zeros(flat_dim, dtype=torch.float32, device=device)
    for x, h in zip(xs, hxs):
        x = x.clone().detach().requires_grad_(True)          # (state_dim,)
        logits, _, _ = model.step(x.unsqueeze(0), h)         # (1,2)
        score = (logits[0,1] - logits[0,0])                  # logit_diff
        score.backward()
        grad_accum += x.grad.detach().abs().squeeze(0)

    grad_mean = (grad_accum / max(1, len(xs))).cpu().numpy()  # (state_dim,)

    # í‚¤ë³„ë¡œ ì§‘ê³„
    per_key = {}
    for k, sl in key_slices.items():
        per_key[k] = float(grad_mean[sl].mean())
    env.close()

    # í° ìˆœì„œë¡œ ì •ë ¬í•´ ë¦¬í„´
    return dict(sorted(per_key.items(), key=lambda x: x[1], reverse=True))

# ----------------------------
# Sparse surrogate (L1 logistic)
# ----------------------------
import torch.nn as nn
import torch.nn.functional as F

def fit_sparse_surrogate(rows, l1=5e-3, epochs=2000, lr=1e-2, standardize=True):
    """
    ì •ì±…ì˜ (obs -> action) ë§¤í•‘ì„ L1 ë¡œì§€ìŠ¤í‹±ìœ¼ë¡œ ê·¼ì‚¬í•˜ì—¬
    í”¼ì²˜ ì¤‘ìš”ë„(ë¶€í˜¸ í¬í•¨)ë¥¼ ë½‘ëŠ”ë‹¤.
    ë°˜í™˜: {'per_key': {key: weight_avg}, 'acc': float, 'keys': [order]}
    """
    # í”¼ì²˜ ìˆœì„œ(ë”•ì…”ë„ˆë¦¬ ì²« ìƒ˜í”Œì˜ í‚¤ ìˆœì„œ)
    keys = list(rows[0]["obs"].keys())

    # X, y êµ¬ì„±
    feats, labels = [], []
    for r in rows:
        vec = []
        for k in keys:
            v = np.asarray(r["obs"][k]).reshape(-1)
            vec.extend(v.tolist())
        feats.append(vec)
        labels.append(r["a"])
    X = torch.tensor(np.array(feats, dtype=np.float32))
    y = torch.tensor(np.array(labels, dtype=np.float32)).unsqueeze(1)

    # í‘œì¤€í™” (ì„ íƒ)
    if standardize:
        mean = X.mean(dim=0, keepdim=True)
        std = X.std(dim=0, keepdim=True).clamp_min(1e-6)
        Xn = (X - mean) / std
    else:
        Xn = X

    clf = nn.Linear(Xn.shape[1], 1)
    opt = torch.optim.Adam(clf.parameters(), lr=lr)
    bce = nn.BCEWithLogitsLoss()

    for _ in range(epochs):
        opt.zero_grad()
        logits = clf(Xn)
        loss = bce(logits, y) + l1 * clf.weight.abs().sum()
        loss.backward()
        opt.step()

    with torch.no_grad():
        logits = clf(Xn)
        pred = (torch.sigmoid(logits) > 0.5).float()
        acc = float((pred.eq(y)).float().mean().item())

    w = clf.weight.detach().cpu().numpy()[0]  # (state_dim,)

    # í‚¤ë³„ í‰ê·  ê°€ì¤‘ì¹˜(ë²¡í„° í‚¤ëŠ” í‰ê· /í•© ì¤‘ íƒ1; ì—¬ê¸°ì„  í‰ê· )
    per_key = {}
    idx = 0
    for k in keys:
        L = int(np.asarray(rows[0]["obs"][k]).size)
        per_key[k] = float(np.mean(w[idx:idx+L]))
        idx += L

    # ì ˆëŒ“ê°’ í° ìˆœìœ¼ë¡œ ì •ë ¬í•œ dict
    per_key_sorted = dict(sorted(per_key.items(), key=lambda x: abs(x[1]), reverse=True))
    return {"per_key": per_key_sorted, "acc": acc, "keys": keys}

def print_bin_report(rows, keys_binspec):
    """
    keys_binspec: [(key, n_bins, how), ...]
    """
    for key, n_bins, how in keys_binspec:
        mids, p_mean, cnt = bin_stat(rows, key, n_bins=n_bins, how=how)
        print(f"\n[bin] key='{key}' (how={how})")
        for m, p, c in zip(mids, p_mean, cnt):
            if np.isnan(p): continue
            print(f"  bin@{m: .2f}: p(a=1)={p: .3f}  (n={int(c)})")



if __name__ == "__main__":
    print("Starting model evaluation...")
    results = compare_all_models(env_param_list, n_episodes=100, greedy=True)
    print_results(results)
    save_detailed_results(results)
    print("\nDetailed results saved to evaluation_results.csv")

    print("Starting permutation-importance analysis ...")
    # ëŒ€í‘œ ëª¨ë¸ í•˜ë‚˜ì— ëŒ€í•´, env ë¦¬ìŠ¤íŠ¸ í‰ê·  ì„í¬í„´ìŠ¤ ê³„ì‚°
    target_model = "models/global_final.pth"  # í•„ìš” ì‹œ ë³€ê²½
    if os.path.exists(target_model):
        imp = permutation_importance_over_envs(
            model_path=target_model,
            env_param_list=env_param_list,
            keys=None,                  # Noneì´ë©´ ê´€ì°° ì „ í‚¤ ì „ì²´
            collect_episodes=20,        # ìˆ˜ì§‘(ë²„í¼) ì—í”¼ì†Œë“œ
            eval_episodes=50,           # í‰ê°€ ì—í”¼ì†Œë“œ
            greedy=True,
            max_envs=min(5, len(env_param_list)),  # ì‹œê°„ ë‹¨ì¶•ìš©
            seed=0,
        )

        print("\n=== Permutation Importance (avg over envs) ===")
        print(f"Baseline mean reward: {imp['baseline_mean']:.2f} Â± {imp['baseline_std']:.2f}")
        for k, d in imp["sorted"]:
            print(f"  {k:>20s} : Î”reward = {d:.2f} (std {imp['std_drop'][k]:.2f})")
    else:
        print(f"Model not found: {target_model}")

    groups = [
        ("mec_comp_units", "mec_proc_times"),
        ("queue_comp_units", "queue_proc_times"),
        ("ctx_vel", "ctx_comp"),
        ("available_computation_units",),
        ("channel_quality",),
    ]

    # target_model = f"runs/a3c_{stamp}/models/global_final.pth"  # ê²½ë¡œëŠ” í™˜ê²½ì— ë§ê²Œ
    target_model = f"runs/individual_{stamp}/models/individual_worker_0_final.pth"  # ê²½ë¡œëŠ” í™˜ê²½ì— ë§ê²Œ
    if os.path.exists(target_model):
        print("\n=== Grouped Permutation Importance ===")
        base_list, group_drops = [], {}
        for idx, env_kwargs in enumerate(env_param_list[:min(5, len(env_param_list))]):
            b, d = permutation_importance_groups_on_env(
                target_model, env_kwargs, groups,
                collect_episodes=20, eval_episodes=200, greedy=True, seed=idx*11
            )
            base_list.append(b)
            for g, v in d.items():
                group_drops.setdefault(g, []).append(v)

        print(f"Baseline mean: {np.mean(base_list):.2f} Â± {np.std(base_list):.2f}")
        for g in groups:
            vals = group_drops.get(g, [])
            if not vals: continue
            print(f"{str(g):>40s} : Î”reward = {np.mean(vals):.2f} (std {np.std(vals):.2f})")

        print("\n=== Occlusion (mean-constant) ===")
        b, d = occlusion_importance_on_env(
            target_model, env_param_list[0], groups, how="mean",
            collect_episodes=20, eval_episodes=200, greedy=True, seed=0
        )
        print(f"Baseline: {b:.2f}, Î”reward(mean-const, grouped) = {d:.2f}")

    # ë¶„ì„ íƒ€ê¹ƒ ëª¨ë¸ê³¼ í™˜ê²½ í•˜ë‚˜ ê³ ë¥´ê¸°
    # target_model = "models/global_final.pth"     # ê²½ë¡œ ë§ê²Œ ì¡°ì •
    env_kwargs   = env_param_list[0]             # ëŒ€í‘œ í™˜ê²½ í•˜ë‚˜

    if os.path.exists(target_model):
        print("\n=== Policy-factor analysis ===")
        # 1) ë°ì´í„° ìˆ˜ì§‘
        rows = collect_policy_dataset(target_model, env_kwargs, n_episodes=200, greedy=True)
        print(f"Collected {len(rows)} decisions.")

        # 2) ì¡°ê±´ë¶€ í™•ë¥  p(a=1 | feature-bin)
        keys_binspec = [
            ("channel_quality", 10, "first"),
            ("queue_proc_times", 10, "first"),
            ("queue_comp_units", 10, "first"),
            ("available_computation_units", 10, "first"),
            ("ctx_vel", 10, "first"),
            ("ctx_comp", 10, "first"),
        ]
        print_bin_report(rows, keys_binspec)

        # 3) ê·¸ë˜ë””ì–¸íŠ¸ ë¯¼ê°ë„ (í˜„ì¬ ì‹œì )
        gi = gradient_importance(target_model, env_kwargs, sample_steps=2048, greedy=True)
        print("\n[Gradient importance] top-8 (bigger = more sensitive now)")
        for k, v in list(gi.items())[:8]:
            print(f"  {k:>28s} : {v:.6f}")

        # 4) í¬ì†Œ ëŒ€ë¦¬ëª¨ë¸ë¡œ ê·œì¹™/ë¶€í˜¸
        sur = fit_sparse_surrogate(rows, l1=5e-3, epochs=2000, lr=1e-2, standardize=True)
        print(f"\n[Sparse surrogate] train acc: {sur['acc']*100:.1f}%")
        print("  top-8 weights (sign: + â†’ offload, - â†’ local):")
        i = 0
        for k, w in sur["per_key"].items():
            print(f"    {k:>28s} : {w:+.4f}")
            i += 1
            if i >= 8: break
    else:
        print(f"[skip] model not found: {target_model}")

\documentclass[journal]{IEEEtran}
% \documentclass[lettersize,journal]{IEEEtran}

% Fix font issues
% \renewcommand{\rmdefault}{cmr}
% \renewcommand{\sfdefault}{cmss}
% \renewcommand{\ttdefault}{cmtt}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{graphicx}
\usepackage{array}
\usepackage{textcomp}
\usepackage{bigstrut}
\usepackage{numprint}
\usepackage{multirow}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{makecell}
\usepackage[dvipsnames]{xcolor}
\usepackage[multiple]{footmisc}
\usepackage{subcaption}

\newfloat{algorithm}{t}{lop}
\newcommand{\St}{{\mathchoice{}{}{\scriptscriptstyle}{}S}}
\newcommand{\Ac}{{\mathchoice{}{}{\scriptscriptstyle}{}A}}

% Begin document
\begin{document}

\title{ATLAS: Adaptive Task Learning Allocation System for UAV Task Offloading in Edge Computing}

\author{Taewon Song and Taeyoon Kim
\thanks{T. Song is with the Department of Internet of Things, College of SW Convergence, Soonchunhyang University, 22 Soonchunhyang-ro, Shinchang-myeon, Asan-si, Chungcheongnam-do, 31538, Korea (e-mail: twsong@sch.ac.kr) and T. Kim is with ... (e-mail: 2000kty@dankook.ac.kr), Corresponding author: T. Kim.}% <-this % stops a space

\thanks{This work was supported in part by ..., and in part by ..., and in part by ...}
% \thanks{Manuscript received XXX, XX, 2015; revised XXX, XX, 2015.}
}

% \markboth{IEEE Transactions on Vehicular Technology,~Vol.~XX, No.~XX, XXX~2015}
{}
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}

\maketitle

\begin{abstract}
Multi-UAV task offloading in edge computing environments requires coordinated decision-making under dynamic network conditions and limited computational resources. This paper presents an A3C-based multi-UAV task offloading system that leverages parameter sharing among distributed workers to achieve superior generalization performance across diverse operational conditions. We formulate the problem as a partially observable Markov decision process where UAV agents select optimal offloading actions, specifically, local processing, MEC offloading, or task discard, based on computational state, queue status, and network conditions. The proposed system employs a recurrent actor-critic architecture with layer normalization to handle sequential dependencies and stabilize asynchronous training across multiple workers. Experimental evaluation demonstrates that the A3C-based approach with parameter sharing significantly outperforms independent learning strategies, achieving enhanced mean performance, improved stability, and superior worst-case robustness. Through systematic ablation studies, we validate our architectural design choices and identify critical system parameters that ensure A3C's advantages. The results provide practical guidelines for deploying distributed reinforcement learning in multi-UAV edge computing systems, demonstrating when coordinated learning approaches provide genuine benefits over independent policies.
\end{abstract}


\begin{IEEEkeywords}
UAV, Task Offloading, A3C, Deep Reinforcement Learning, Multi-Agent Systems, Edge Computing
\end{IEEEkeywords}


\IEEEpeerreviewmaketitle

% Introduction
\section{Introduction}
\label{sec:introduction}

% General problem area
The proliferation of Unmanned Aerial Vehicles (UAVs) in applications ranging from surveillance to emergency response has created unprecedented opportunities for distributed edge computing. Multi-UAV systems must make real-time decisions about computational task offloading while operating under stringent energy constraints, dynamic network conditions, and limited onboard resources. As UAV deployments scale to swarms of tens or hundreds of agents, the challenge of coordinating task distribution across heterogeneous edge computing infrastructure becomes increasingly critical for system performance and mission success.

% Specific problem being addressed
While existing approaches have explored various task offloading strategies~\cite{seid2021multi,wang2023stackelberg,guo2024multi,hao2024joint,li2024robust}, a fundamental gap remains in understanding how to effectively coordinate learning among distributed UAV agents. Traditional centralized optimization methods struggle with the dynamic, partially observable nature of multi-UAV environments, while independent learning approaches fail to leverage collective experience across the swarm. The key challenge lies in designing a distributed reinforcement learning system that can share knowledge effectively among heterogeneous workers operating in diverse conditions, maintain stable performance across varying network topologies and resource availability, and generalize to previously unseen operational scenarios without requiring complete retraining.

% Example
Consider a disaster response scenario where multiple UAVs must coordinate to process sensor data, execute path planning, and transmit critical information to ground stations. Each UAV is equipped with a local mobile edge computing (MEC) server and faces a continuous stream of computational tasks. For each incoming task, the UAV must decide whether to (1) process it locally using its onboard computing resources (consuming battery energy but avoiding communication delays), (2) offload it to cloud servers on the core network through wireless backhaul links (leveraging superior computational capacity but incurring transmission costs, network latency, and potential communication failures), or (3) discard the task when neither local nor cloud processing is feasible (sacrificing task completion to conserve critical resources). The optimal decision depends on the current system state observed by each UAV, which includes local computational state, queue state, cloud server state, local computation capacity, and the wireless channel conditions. Moreover, environmental heterogeneity means that UAVs operating in different regions encounter vastly different network conditions and resource availability, making coordinated learning through parameter sharing essential for achieving robust system-wide performance.

% Thesis statement
In this paper, we present an A3C-based multi-UAV task offloading system that leverages parameter sharing among distributed workers to achieve superior generalization performance across diverse operational conditions. Our system employs a recurrent actor-critic architecture with layer normalization to handle sequential decision dependencies and stabilize asynchronous training across multiple heterogeneous workers.

% Approach
We formulate the multi-UAV task offloading problem as a partially observable Markov decision process (POMDP) where each UAV agent selects actions based on the observations. The proposed architecture integrates recurrent neural networks to capture temporal dependencies in wireless channel conditions and resource availability on the cloud size, which might be hidden on the UAV side, while layer normalization ensures stable gradient updates across asynchronously training workers. Through systematic experimentation across varying resource constraints, network velocities, and exploration parameters, we validate our design choices and identify the operational regimes where distributed learning provides genuine advantages over independent policies.

% Contributions
The main contributions of this paper are as follows:
\begin{itemize}
    \item We design and implement an A3C-based multi-UAV task offloading system with a carefully architected recurrent actor-critic network that achieves superior generalization performance through parameter sharing among distributed workers.
    \item We conduct comprehensive ablation studies across architectural components, hyperparameters, and environmental factors to validate our design choices and establish practical deployment guidelines.
    % \item We demonstrate that A3C's performance advantage stems from its algorithmic design (parameter sharing and asynchronous updates) rather than architectural sophistication, providing fundamental insights into when and why distributed learning outperforms independent approaches.
    \item We identify critical operational regimes---including resource availability, exploration requirements, and environmental dynamics---that determine whether A3C provides genuine benefits, offering practitioners clear criteria for algorithm selection in multi-UAV deployments.
\end{itemize}

% Paper structure
The remainder of this paper is organized as follows. Section~\ref{sec:related_work} reviews related work in UAV task offloading and multi-agent reinforcement learning. Section~\ref{sec:POMDP} formulates the problem as a POMDP and describes our system architecture. Section~\ref{sec:experimental_setup} details the experimental setup and evaluation methodology. Section~\ref{sec:results} presents performance evaluation results and ablation study findings. Section~\ref{sec:discussion} discusses implications and limitations, and Section~\ref{sec:conclusion} concludes the paper.

\section{Related Works}
\label{sec:related_work}

The intersection of UAV-assisted mobile edge computing and deep reinforcement learning has emerged as a critical research area, with various approaches addressing the challenges of distributed task offloading optimization. This section reviews existing work across three key themes: reinforcement learning methods for task offloading, multi-agent coordination and parameter sharing strategies, and architectural components for handling partial observability in sequential decision-making systems.

\subsection{Reinforcement Learning for UAV Task Offloading}

Deep reinforcement learning has been extensively applied to UAV task offloading problems due to its ability to handle complex, dynamic environments without requiring explicit models of system dynamics.

\cite{seid2021multi},
\cite{zhao2022multiagent},
\cite{song2024drqn},
\cite{hao2024joint}, 
\cite{li2024robust},
\cite{guo2025cognitive},


% Value-based methods (DQN, DRQN)
% TODO: Add references on DQN-based task offloading
% TODO: Add references on DRQN for partial observability

% Actor-critic methods
% TODO: Add references on actor-critic approaches
% TODO: Add references on policy gradient methods

% Research gap for this subsection
While these approaches demonstrate the effectiveness of deep RL for task offloading, they primarily focus on single-agent scenarios or do not systematically compare centralized parameter sharing versus independent learning strategies. Moreover, limited attention has been given to A3C algorithms specifically designed for multi-UAV coordination with parameter sharing. \textbf{Gap 1:} Existing work does not systematically compare A3C's global parameter sharing against independent learning strategies, nor does it identify the operational regimes where distributed learning provides genuine advantages.

\subsection{Distributed Learning and Parameter Sharing}

Multi-agent reinforcement learning for edge computing systems requires careful consideration of how knowledge is shared among distributed agents.

\cite{zhuo2019federated},
\cite{wang2020federated},
\cite{tehrani2021federated},
\cite{yu2021when},
\cite{moon2022federated},
\cite{zhao2024federated},

% A3C and asynchronous methods
% TODO: Add original A3C paper and variants
% TODO: Add A3C applications in different domains

% Centralized vs Decentralized training
% TODO: Add references on CTDE (Centralized Training Decentralized Execution)
% TODO: Add references on federated learning approaches

% Parameter sharing strategies
% TODO: Add references on parameter sharing benefits
% TODO: Add references on multi-agent coordination

% Research gap for this subsection
While various distributed learning paradigms have been explored, existing work lacks systematic comparison between A3C with global parameter sharing and fully independent worker training in the context of UAV task offloading. \textbf{Gap 2:} Current research provides limited guidance on when distributed parameter sharing provides genuine advantages over independent learning. The operational regimes---including resource availability, exploration requirements, and environmental dynamics---that determine algorithm effectiveness remain unclear, particularly across heterogeneous operational environments.

\subsection{Architecture Components in Deep Reinforcement Learning}

The design of neural network architectures significantly impacts the performance and stability of deep RL systems, particularly in partially observable environments.

% Recurrent architectures for RL
% TODO: Add references on RNN/LSTM/GRU in RL
% TODO: Add references on handling partial observability

% Normalization techniques
% TODO: Add references on layer normalization in deep RL
% TODO: Add references on batch normalization variants

% Ablation studies in RL
% TODO: Add references on systematic architecture ablation studies
% TODO: Add references on component analysis in deep RL

% Research gap for this subsection
While individual architectural components have been studied in isolation, comprehensive ablation studies that systematically validate design choices across multiple dimensions remain limited. \textbf{Gap 3:} Existing studies lack comprehensive ablation analysis that validates architectural design choices (RNN, layer normalization) in the context of multi-agent asynchronous training across architectural components, hyperparameters, and environmental factors. The interaction effects between recurrent components and normalization techniques in multi-agent asynchronous training have not been thoroughly investigated, particularly regarding their impact on training stability and generalization performance.

Addressing these three research gaps, this paper presents a carefully designed A3C-based multi-UAV task offloading system with comprehensive ablation studies that provide both system design contributions and fundamental insights into when and why distributed learning outperforms independent approaches.

% POMDP Formulation
\section{POMDP Formulation}
\label{sec:POMDP}

\subsection{Problem Formulation}

We formulate the multi-UAV task offloading optimization problem as a Partially Observable Markov Decision Process (POMDP), where each UAV agent operates as an intelligent decision maker in a dynamic edge computing environment. The POMDP framework enables UAV agents to make sequential decisions for incoming computational tasks while adapting to changing environmental conditions and resource availability. Furthermore, the POMDP formulation captures the inherent uncertainty in UAV operations, such as fluctuating network conditions and the computation capability of the cloud servers for task offloading.

Formally, the ATLAS framework models the task offloading problem as a tuple $\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $\mathcal{P}$ is the state transition function, $\mathcal{R}$ is the reward function, and $\gamma \in [0,1]$ is the discount factor. Each UAV agent observes the current system state $s_t \in \mathcal{S}$, selects an action $a_t \in \mathcal{A}$ according to its policy $\pi(a_t|s_t)$, and receives a reward $r_t = \mathcal{R}(s_t, a_t)$ while transitioning to the next state $s_{t+1} \sim \mathcal{P}(s_{t+1}|s_t, a_t)$.

The primary objective is to learn an optimal policy $\pi^*$ that maximizes the expected cumulative discounted reward:
\begin{equation}
\pi^* = \arg\max_\pi \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r_t \mid \pi\right]
\end{equation}

This formulation enables UAV agents to balance multiple competing objectives including energy efficiency, processing latency minimization, and task completion success rates in dynamic edge computing scenarios.

\subsection{State Space}

The state space $\mathcal{S}$ in the ATLAS framework captures comprehensive information about the UAV's computational environment, task queue status, and contextual factors that influence offloading decisions. We define the state space as a Cartesian product of four fundamental components:

\begin{equation}
\mathcal{S} = \mathcal{C} \times \mathcal{Q} \times \mathcal{E} \times \mathcal{V}
\end{equation}

where each component represents distinct aspects of the system state:

\textbf{Computational State ($\mathcal{C}$):} This component characterizes the UAV's local computational resources and task processing status:
\begin{equation}
\mathcal{C} = \{c_{avail}, c_{epochs}\}
\end{equation}
where $c_{avail} \in [0, 1]$ represents the normalized available computation units on the UAV, and $c_{epochs} \in [0, 1]$ denotes the remaining normalized epochs for task completion.

\textbf{Queue State ($\mathcal{Q}$):} This captures the current status of the UAV's task buffer and processing queue:
\begin{equation}
\mathcal{Q} = \{q_{units}, q_{times}, q_{success}\}
\end{equation}
where $q_{units} \in [0, 1]$ represents the normalized queue computation requirements, $q_{times} \in [0, 1]$ indicates normalized processing times, and $q_{success} \in \{0, 1\}^2$ provides binary success indicators for local and offload operations.

\textbf{Environment State ($\mathcal{E}$):} This component models the external MEC infrastructure and network conditions:
\begin{equation}
\mathcal{E} = \{e_{mec\_units}, e_{mec\_times}\}
\end{equation}
where $e_{mec\_units} \in [0, 1]$ represents normalized available MEC server computation units, and $e_{mec\_times} \in [0, 1]$ denotes normalized MEC processing times.

\textbf{Vehicle Context ($\mathcal{V}$):} This encompasses UAV-specific contextual information:
\begin{equation}
\mathcal{V} = \{v_{velocity}, v_{capacity}\}
\end{equation}
where $v_{velocity} \in [0, 1]$ represents normalized agent velocity, and $v_{capacity} \in [0, 1]$ indicates normalized computation capacity.

The complete state vector at time $t$ is represented as:
\begin{equation}
s_t = (c_{avail}^{(t)}, c_{epochs}^{(t)}, q_{units}^{(t)}, q_{times}^{(t)}, q_{success}^{(t)}, e_{mec\_units}^{(t)}, e_{mec\_times}^{(t)}, v_{velocity}^{(t)}, v_{capacity}^{(t)})
\end{equation}

All state components are normalized to $[0, 1]$ to ensure stable neural network training and enable effective knowledge transfer across different environmental configurations.

\subsection{Action Space}

The action space $\mathcal{A}$ in the ATLAS framework defines the discrete set of decisions available to each UAV agent when processing incoming computational tasks. We formulate the action space as a finite discrete set:

\begin{equation}
\mathcal{A} = \{a_{LOC}, a_{OFF}, a_{DIS}\} = \{0, 1, 2\}
\end{equation}

Each action corresponds to a distinct task processing strategy with specific operational characteristics:

\textbf{Local Processing ($a_{LOC} = 0$):} When this action is selected, the UAV processes the computational task using its onboard computing resources. This action is optimal when:
\begin{itemize}
\item Local computational capacity is sufficient for the task requirements
\item Network connectivity to MEC servers is poor or unavailable  
\item Energy consumption for local processing is acceptable
\item Processing latency requirements can be met locally
\end{itemize}

The local processing action triggers immediate computation on the UAV, consuming local energy resources but avoiding transmission delays and potential network failures.

\textbf{Offloading ($a_{OFF} = 1$):} This action involves transmitting the computational task to available MEC servers for remote processing. Offloading is advantageous when:
\begin{itemize}
\item Local computational resources are insufficient or overloaded
\item MEC servers have available capacity and favorable processing times
\item Network conditions support reliable task transmission
\item Total offloading latency (transmission + processing + result retrieval) is acceptable
\end{itemize}

The offloading action incurs transmission energy costs and introduces network-dependent delays, but potentially reduces local computational load and enables processing of resource-intensive tasks.

\textbf{Task Discard ($a_{DIS} = 2$):} When system conditions are unfavorable for both local processing and offloading, the agent may choose to discard the task. This action is selected when:
\begin{itemize}
\item Local resources are critically low 
\item MEC servers are overloaded or unreachable
\item Task deadline cannot be met through any processing option
\item Energy conservation is prioritized over task completion
\end{itemize}

Task discard minimizes immediate resource consumption but results in task failure penalties in the reward function.

The discrete action formulation enables straightforward policy optimization through categorical probability distributions, allowing the neural network to output action probabilities $\pi(a_t|s_t)$ for effective exploration and exploitation during training.

\subsection{State Transition Function}

The state transition function $\mathcal{P}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$ defines the probability distribution over next states given the current state and action. In the ATLAS framework, state transitions are governed by the dynamics of the UAV environment, task processing outcomes, and system resource evolution.

The transition probability can be expressed as:
\begin{equation}
\mathcal{P}(s_{t+1}|s_t, a_t) = P(s_{t+1} = (c', q', e', v')|s_t = (c, q, e, v), a_t)
\end{equation}

The state transition dynamics are influenced by several key factors:

\textbf{Computational Resource Evolution:} The computational state transitions depend on task processing outcomes and resource consumption:
\begin{equation}
c'_{avail} = \begin{cases}
\max(0, c_{avail} - \Delta c_{local}) & \text{if } a_t = a_{LOC} \\
c_{avail} & \text{if } a_t = a_{OFF} \\
c_{avail} & \text{if } a_t = a_{DIS}
\end{cases}
\end{equation}

where $\Delta c_{local}$ represents the normalized computational resources consumed for local processing.

\textbf{Queue State Dynamics:} The queue state evolves based on task arrivals, processing actions, and completion events:
\begin{equation}
q'_{units} = f_{queue}(q_{units}, a_t, \text{new\_tasks}, \text{completed\_tasks})
\end{equation}

Queue updates incorporate the stochastic nature of task arrivals and the deterministic effects of processing actions.

\textbf{Environment State Changes:} MEC server states evolve based on system load and external factors:
\begin{equation}
e'_{mec\_units} \sim P_{MEC}(e_{mec\_units}|e_{mec\_units}, \text{system\_load})
\end{equation}

The MEC state transitions follow a semi-Markov process reflecting the dynamic nature of edge computing infrastructure.

\textbf{Vehicle Mobility:} UAV contextual states evolve according to mobility patterns and mission requirements:
\begin{equation}
v'_{velocity} = v_{velocity} + \epsilon_{velocity}
\end{equation}

where $\epsilon_{velocity}$ represents bounded random variations in UAV velocity based on environmental conditions and flight dynamics.

The complete transition function captures the stochastic nature of the UAV environment while maintaining computational tractability for reinforcement learning optimization.

\subsection{Reward Function}

The reward function $\mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ provides immediate feedback to guide the UAV agent's learning process by quantifying the desirability of taking action $a_t$ in state $s_t$. The ATLAS framework designs a multi-objective reward function that balances energy efficiency, processing latency, and task completion success.

The reward function is formulated as a weighted combination of multiple cost and benefit components:

\begin{equation}
\mathcal{R}(s_t, a_t) = -\alpha \cdot C_{energy}(s_t, a_t) - \beta \cdot C_{latency}(s_t, a_t) - \gamma \cdot C_{failure}(s_t, a_t) + \delta \cdot B_{completion}(s_t, a_t)
\end{equation}

where $\alpha$, $\beta$, $\gamma$, and $\delta$ are positive weighting coefficients that control the relative importance of each objective component.

\textbf{Energy Cost ($C_{energy}$):} This component penalizes energy consumption for different actions:
\begin{equation}
C_{energy}(s_t, a_t) = \begin{cases}
E_{local} \cdot q_{units} & \text{if } a_t = a_{LOC} \\
E_{transmission} \cdot q_{units} & \text{if } a_t = a_{OFF} \\
0 & \text{if } a_t = a_{DIS}
\end{cases}
\end{equation}

where $E_{local}$ and $E_{transmission}$ represent normalized energy costs for local processing and task transmission, respectively.

\textbf{Latency Cost ($C_{latency}$):} This component accounts for processing and communication delays:
\begin{equation}
C_{latency}(s_t, a_t) = \begin{cases}
T_{local}(q_{units}, c_{avail}) & \text{if } a_t = a_{LOC} \\
T_{offload}(q_{units}, e_{mec\_times}) + T_{transmission} & \text{if } a_t = a_{OFF} \\
0 & \text{if } a_t = a_{DIS}
\end{cases}
\end{equation}

where $T_{local}$ and $T_{offload}$ are functions computing processing times based on task requirements and available resources.

\textbf{Failure Penalty ($C_{failure}$):} This component penalizes unsuccessful task processing:
\begin{equation}
C_{failure}(s_t, a_t) = \begin{cases}
P_{fail\_local} \cdot \rho_{failure} & \text{if } a_t = a_{LOC} \\
P_{fail\_offload} \cdot \rho_{failure} & \text{if } a_t = a_{OFF} \\
\rho_{discard} & \text{if } a_t = a_{DIS}
\end{cases}
\end{equation}

where $P_{fail\_local}$ and $P_{fail\_offload}$ are failure probabilities, and $\rho_{failure}$ and $\rho_{discard}$ are penalty magnitudes.

\textbf{Completion Benefit ($B_{completion}$):} This component provides positive rewards for successful task completion:
\begin{equation}
B_{completion}(s_t, a_t) = \begin{cases}
(1 - P_{fail\_local}) \cdot \eta_{success} & \text{if } a_t = a_{LOC} \\
(1 - P_{fail\_offload}) \cdot \eta_{success} & \text{if } a_t = a_{OFF} \\
0 & \text{if } a_t = a_{DIS}
\end{cases}
\end{equation}

where $\eta_{success}$ represents the positive reward magnitude for successful task completion.

This multi-objective reward design enables the ATLAS system to learn policies that optimize the trade-off between energy consumption, processing latency, and task success rates across diverse operational conditions.

\subsection{A3C Architecture}

The A3C implementation employs both feedforward and recurrent neural network configurations to handle the sequential nature of the task offloading decisions.

\subsubsection{Network Architecture}

The actor-critic architecture consists of:
\begin{itemize}
\item \textbf{Actor Network}: Outputs action probabilities using a softmax policy
\item \textbf{Critic Network}: Estimates state values for advantage calculation
\item \textbf{Hidden Layers}: 128 neurons with ReLU activation functions
\item \textbf{Recurrent Component}: GRU layers for sequence modeling (when enabled)
\end{itemize}

For the recurrent variant (RecurrentActorCritic), the network processes sequences of observations through GRU layers before the final actor and critic heads, enabling the model to maintain memory of past decisions and environmental states.

\subsubsection{Training Strategies}

We evaluate two distinct training paradigms:

\textbf{A3C Global Training:} In this centralized approach, all workers share updates to a single global model. Each worker interacts with its assigned environment and computes gradients based on the advantage actor-critic loss:

\begin{equation}
L = L_{policy} + \beta \cdot L_{value} - \alpha \cdot H(\pi)
\end{equation}

where $L_{policy}$ is the policy loss, $L_{value}$ is the value function loss, and $H(\pi)$ is the policy entropy term for exploration.

The global model aggregates updates from all workers asynchronously, promoting knowledge sharing across different environmental conditions and worker experiences.

\textbf{Individual Worker Training:} In this decentralized approach, each worker develops its own specialized policy independently. Workers train separate neural networks without sharing parameters, allowing for environment-specific adaptation and specialized behavior development.

\subsection{Environment Configuration}

The experimental framework utilizes a custom UAV environment with the following key parameters:
\begin{itemize}
\item Maximum computation units: 200
\item Maximum computation units for cloud: 1000
\item Maximum epoch size: 100
\item Maximum queue size: 20
\item Agent velocities: 50 units
\end{itemize}

Five distinct environmental configurations are evaluated, each representing different operational scenarios with varying computational demands, network conditions, and resource availability patterns.

\subsection{Performance Metrics}
Our evaluation framework includes the following key metrics:
\begin{itemize}
\item Episode-level total rewards
\item Convergence characteristics across training episodes
\item Statistical significance of performance differences
\item Cross-environment generalization performance
\end{itemize}

% Experimental Setup
\section{Experimental Setup}
\label{sec:experimental_setup}

\subsection{Implementation Details}
% Implementation details from params.py

\subsection{Environment Configurations}
Five distinct environmental configurations were evaluated, each representing different operational scenarios with varying computational demands and network conditions.

\subsection{Training Parameters}
% Training parameters from the codebase

% Results and Analysis
\section{Results and Analysis}
\label{sec:results}

\subsection{Performance Comparison}
% Analysis based on environment_comparison results

\subsection{Episode-level Analysis}
% Analysis based on episode_curves results

\subsection{Distribution Analysis}
% Analysis based on distribution_analysis results

\subsection{Cross-Environment Performance}
% Analysis based on performance_heatmap results

\subsection{Statistical Significance}
% Statistical analysis results

% Discussion
\section{Discussion}
\label{sec:discussion}

% Discussion of results, implications, limitations

% Conclusion
\section{Conclusion}
\label{sec:conclusion}

% Conclusion and future work

% References
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
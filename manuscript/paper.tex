% LTeX: enabled=false

\documentclass[journal]{IEEEtran}
% \documentclass[lettersize,journal]{IEEEtran}

% Fix font issues
% \renewcommand{\rmdefault}{cmr}
% \renewcommand{\sfdefault}{cmss}
% \renewcommand{\ttdefault}{cmtt}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{graphicx}
\usepackage{array}
\usepackage{textcomp}
\usepackage{bigstrut}
\usepackage{numprint}
\usepackage{multirow}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{makecell}
\usepackage[dvipsnames]{xcolor}
\usepackage[multiple]{footmisc}
\usepackage{subcaption}
\usepackage{siunitx}

\newfloat{algorithm}{t}{lop}
\newcommand{\St}{{\mathchoice{}{}{\scriptscriptstyle}{}S}}
\newcommand{\Ac}{{\mathchoice{}{}{\scriptscriptstyle}{}A}}

% Begin document
\begin{document}

\title{ATLAS: Adaptive Task Offloading System in Multi-UAV-assisted Multi-Access Edge Computing}

\author{Taewon Song and Taeyoon Kim
\thanks{T. Song is with the Department of Internet of Things, College of SW Convergence, Soonchunhyang University, 22 Soonchunhyang-ro, Shinchang-myeon, Asan-si, Chungcheongnam-do, 31538, Korea (e-mail: twsong@sch.ac.kr) and T. Kim is with ... (e-mail: 2000kty@dankook.ac.kr), Corresponding author: T. Kim.}% <-this % stops a space

\thanks{This work was supported in part by ..., and in part by ..., and in part by ...}
% \thanks{Manuscript received XXX, XX, 2015; revised XXX, XX, 2015.}
}

% \markboth{IEEE Transactions on Vehicular Technology,~Vol.~XX, No.~XX, XXX~2015}
{}
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}

\maketitle

\begin{abstract}
Multi-UAV task offloading in edge computing environments requires coordinated decision-making under dynamic network conditions and limited computational resources. This paper presents an A3C-based multi-UAV task offloading system that leverages parameter sharing among distributed workers to achieve superior generalization performance across diverse operational conditions. We formulate the problem as a partially observable Markov decision process where UAV agents select optimal offloading actions, specifically, local processing, MEC offloading, or task discard, based on computational state, queue status, and network conditions. The proposed system employs a recurrent actor-critic architecture with layer normalization to handle sequential dependencies and stabilize asynchronous training across multiple workers. Experimental evaluation demonstrates that the A3C-based approach with parameter sharing significantly outperforms independent learning strategies, achieving enhanced mean performance, improved stability, and superior worst-case robustness. Through systematic ablation studies, we validate our architectural design choices and identify critical system parameters that ensure A3C's advantages. The results provide practical guidelines for deploying distributed reinforcement learning in multi-UAV edge computing systems, demonstrating when coordinated learning approaches provide genuine benefits over independent policies.
\end{abstract}


\begin{IEEEkeywords}
UAV, Task Offloading, A3C, Deep Reinforcement Learning, Multi-Agent Systems, Multi-Access Edge Computing
\end{IEEEkeywords}


\IEEEpeerreviewmaketitle

% Introduction
\section{Introduction}
\label{sec:introduction}

% General problem area
The proliferation of Unmanned Aerial Vehicles (UAVs) in applications ranging from surveillance to emergency response has created unprecedented opportunities for distributed edge computing. Multi-UAV systems must make real-time decisions about computational task offloading while operating under stringent energy constraints, dynamic network conditions, and limited onboard resources. As UAV deployments scale to swarms of tens or hundreds of agents, the challenge of coordinating task distribution across heterogeneous edge computing infrastructure becomes increasingly critical for system performance and mission success.

% Specific problem being addressed
While existing approaches have explored various task offloading strategies~\cite{seid2021multi,wang2023stackelberg,guo2024multi,hao2024joint,li2024robust}, a fundamental gap remains in understanding how to effectively coordinate learning among distributed UAV agents. Traditional centralized optimization methods struggle with the dynamic, partially observable nature of multi-UAV environments, while independent learning approaches fail to leverage collective experience across the swarm. The key challenge lies in designing a distributed reinforcement learning system that can share knowledge effectively among heterogeneous workers operating in diverse conditions, maintain stable performance across varying network topologies and resource availability, and generalize to previously unseen operational scenarios without requiring complete retraining.

% Example
Consider a disaster response scenario where multiple UAVs must coordinate to process sensor data, execute path planning, and transmit critical information to ground stations. Each UAV is equipped with a local mobile edge computing (MEC) server and faces a continuous stream of computational tasks. For each incoming task, the UAV must decide whether to (1) process it locally using its onboard computing resources (consuming battery energy but avoiding communication delays), (2) offload it to cloud servers on the core network through wireless backhaul links (leveraging superior computational capacity but incurring transmission costs, network latency, and potential communication failures), or (3) discard the task when neither local nor cloud processing is feasible (sacrificing task completion to conserve critical resources). The optimal decision depends on the current system state observed by each UAV, which includes local computational state, queue state, cloud server state, local computation capacity, and the wireless channel conditions. Moreover, environmental heterogeneity means that UAVs operating in different regions encounter vastly different network conditions and resource availability, making coordinated learning through parameter sharing essential for achieving robust system-wide performance.

% Thesis statement
In this paper, we present an A3C-based multi-UAV task offloading system that leverages parameter sharing among distributed workers to achieve superior generalization performance across diverse operational conditions. Our system employs a recurrent actor-critic architecture with layer normalization to handle sequential decision dependencies and stabilize asynchronous training across multiple heterogeneous workers.

% Approach
We formulate the multi-UAV task offloading problem as a partially observable Markov decision process (POMDP) where each UAV agent selects actions based on the observations. The proposed architecture integrates recurrent neural networks to capture temporal dependencies in wireless channel conditions and resource availability on the cloud size, which might be hidden on the UAV side, while layer normalization ensures stable gradient updates across asynchronously training workers. Through systematic experimentation across varying resource constraints, network velocities, and exploration parameters, we validate our design choices and identify the operational regimes where distributed learning provides genuine advantages over independent policies.

% Contributions
The main contributions of this paper are as follows:
\begin{itemize}
    \item We design and implement an A3C-based multi-UAV task offloading system with a carefully architected recurrent actor-critic network that achieves superior generalization performance through parameter sharing among distributed workers.
    \item We conduct comprehensive ablation studies across architectural components, hyperparameters, and environmental factors to validate our design choices and establish practical deployment guidelines.
    % \item We demonstrate that A3C's performance advantage stems from its algorithmic design (parameter sharing and asynchronous updates) rather than architectural sophistication, providing fundamental insights into when and why distributed learning outperforms independent approaches.
    \item We identify critical operational regimes---including resource availability, exploration requirements, and environmental dynamics---that determine whether A3C provides genuine benefits, offering practitioners clear criteria for algorithm selection in multi-UAV deployments.
\end{itemize}

% Paper structure
The remainder of this paper is organized as follows. Section~\ref{sec:related_work} reviews related work in UAV task offloading and multi-agent reinforcement learning. Section~\ref{sec:POMDP} formulates the problem as a POMDP and describes our system architecture. Section~\ref{sec:experimental_setup} details the experimental setup and evaluation methodology. Section~\ref{sec:results} presents performance evaluation results and ablation study findings. Section~\ref{sec:discussion} discusses implications and limitations, and Section~\ref{sec:conclusion} concludes the paper.

\section{Related Works}
\label{sec:related_work}

The intersection of UAV-assisted mobile edge computing and deep reinforcement learning has emerged as a critical research area, with various approaches addressing the challenges of distributed task offloading optimization. This section reviews existing work across three key themes: reinforcement learning methods for task offloading, multi-agent coordination and parameter sharing strategies, and architectural components for handling partial observability in sequential decision-making systems.

\subsection{Reinforcement Learning for UAV Task Offloading}

Deep reinforcement learning has been extensively applied to UAV task offloading problems due to its ability to handle complex, dynamic environments without requiring explicit models of system dynamics.

\cite{seid2021multi},
\cite{zhao2022multiagent},
\cite{song2024drqn},
\cite{hao2024joint}, 
\cite{li2024robust},
\cite{guo2025cognitive},


% Value-based methods (DQN, DRQN)
% TODO: Add references on DQN-based task offloading
% TODO: Add references on DRQN for partial observability

% Actor-critic methods
% TODO: Add references on actor-critic approaches
% TODO: Add references on policy gradient methods

% Research gap for this subsection
While these approaches demonstrate the effectiveness of deep RL for task offloading, they primarily focus on single-agent scenarios or do not systematically compare centralized parameter sharing versus independent learning strategies. Moreover, limited attention has been given to A3C algorithms specifically designed for multi-UAV coordination with parameter sharing. \textbf{Gap 1:} Existing work does not systematically compare A3C's global parameter sharing against independent learning strategies, nor does it identify the operational regimes where distributed learning provides genuine advantages.

\subsection{Distributed Learning and Parameter Sharing}

Multi-agent reinforcement learning for edge computing systems requires careful consideration of how knowledge is shared among distributed agents.

\cite{zhuo2019federated},
\cite{wang2020federated},
\cite{tehrani2021federated},
\cite{yu2021when},
\cite{moon2022federated},
\cite{zhao2024federated},

% A3C and asynchronous methods
% TODO: Add original A3C paper and variants
% TODO: Add A3C applications in different domains

% Centralized vs Decentralized training
% TODO: Add references on CTDE (Centralized Training Decentralized Execution)
% TODO: Add references on federated learning approaches

% Parameter sharing strategies
% TODO: Add references on parameter sharing benefits
% TODO: Add references on multi-agent coordination

% Research gap for this subsection
While various distributed learning paradigms have been explored, existing work lacks systematic comparison between A3C with global parameter sharing and fully independent worker training in the context of UAV task offloading. \textbf{Gap 2:} Current research provides limited guidance on when distributed parameter sharing provides genuine advantages over independent learning. The operational regimes---including resource availability, exploration requirements, and environmental dynamics---that determine algorithm effectiveness remain unclear, particularly across heterogeneous operational environments.

\subsection{Architecture Components in Deep Reinforcement Learning}

The design of neural network architectures significantly impacts the performance and stability of deep RL systems, particularly in partially observable environments.

% Recurrent architectures for RL
% TODO: Add references on RNN/LSTM/GRU in RL
% TODO: Add references on handling partial observability

% Normalization techniques
% TODO: Add references on layer normalization in deep RL
% TODO: Add references on batch normalization variants

% Ablation studies in RL
% TODO: Add references on systematic architecture ablation studies
% TODO: Add references on component analysis in deep RL

% Research gap for this subsection
While individual architectural components have been studied in isolation, comprehensive ablation studies that systematically validate design choices across multiple dimensions remain limited. \textbf{Gap 3:} Existing studies lack comprehensive ablation analysis that validates architectural design choices (RNN, layer normalization) in the context of multi-agent asynchronous training across architectural components, hyperparameters, and environmental factors. The interaction effects between recurrent components and normalization techniques in multi-agent asynchronous training have not been thoroughly investigated, particularly regarding their impact on training stability and generalization performance.

Addressing these three research gaps, this paper presents a carefully designed A3C-based multi-UAV task offloading system with comprehensive ablation studies that provide both system design contributions and fundamental insights into when and why distributed learning outperforms independent approaches.

% POMDP Formulation
\section{POMDP Formulation}
\label{sec:POMDP}

\subsection{Problem Formulation}
\label{subsec:problem}

We formulate the multi-UAV task offloading optimization problem as a Partially Observable Markov Decision Process (POMDP), where each UAV agent operates as an intelligent decision maker in a dynamic edge computing environment. The POMDP framework enables UAV agents to make sequential decisions for incoming computational tasks while adapting to changing environmental conditions and resource availability. Furthermore, the POMDP formulation captures the inherent uncertainty in UAV operations, such as fluctuating network conditions and the computation capability of the cloud servers for task offloading.

Formally, the ATLAS framework models the task offloading problem as a tuple $\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $\mathcal{P}$ is the state transition function, $\mathcal{R}$ is the reward function, and $\gamma \in [0,1]$ is the discount factor. At time $t$, each UAV agent can be in a state $s_t \in \mathcal{S}$, then selects an action $a_t \in \mathcal{A}$ according to its policy $\pi(a_t|s_t)$, and receives a reward $r_t = \mathcal{R}(s_t, a_t)$ while transitioning to the next state $s_{t+1} \sim \mathcal{P}(s_{t+1}|s_t, a_t)$.

The primary objective is to learn an optimal policy $\pi^*$ that maximizes the expected cumulative discounted reward:
\begin{equation}
\pi^* = \arg\max_\pi \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r_t \mid \pi\right].
\end{equation}
This formulation enables UAV agents to balance multiple competing objectives including energy efficiency, processing latency minimization, and task completion success rates in dynamic edge computing scenarios.

\subsection{State Space}
\label{subsec:state}

We formulate the multi-UAV task offloading problem as a partially observable Markov decision process (POMDP), where each UAV agent makes decisions based on locally observable information while the underlying environment dynamics are governed by additional exogenous factors that are not directly observable.

We define the environment state space $\mathcal{S}$ as
\begin{equation}
\label{eq:S}
\mathcal{S} = \mathcal{O} \times \mathcal{Z}
\end{equation}
where $\mathcal{O}$ denotes the set of state components observable by individual UAV agents, and $\mathcal{Z}$ represents latent environment state components that are not included in the agents’ local observations. Importantly, these hidden components are part of the environment state and are introduced to preserve the Markov property, rather than being latent variables inferred through belief updates. 
% In this work, we define $\mathcal{S}_{hid}$ as follows:
% \begin{equation}
% \label{eq:S_hid}
% \mathcal{S}_{hid} \triangleq Z = S_{cloud} \times S_{channel}
% \end{equation}
% where $Z$ captures exogenous cloud and wireless channel conditions that influence state transitions and rewards but are not directly observable from the persepective of individual UAV agents.


\subsubsection{Observable State Components}

The observable state space $\mathcal{O}$ consists of system information directly measurable by each UAV agent and is defined as
\begin{equation}
\label{eq:S_obs}
\mathcal{O} = S_{local} \times S_{context} \times S_{mec}.
\end{equation}


\textbf{Local Device State (${S_{local}}$):} This component characterizes the UAV's local computational resources, task queue, and processing feedback:
\begin{equation}
{S_{local}} = {C_l} \times {E_r} \times {C_q} \times {T_q} \times {I_l} \times {I_o}
\end{equation}
where ${C_l}$ represents a set of available computation units, ${E_r}$ denotes a set of remaining epochs during one episode, ${C_q}$ and ${T_q}$ capture sets of queue computation units and processing times, and ${I_l}, {I_o}$ provide binary success indicators for local and offload operations. Above-mentioned values are normalized within $[0, 1]$ to ensure stable neural network training and effective learning across different operational scales.


\textbf{Environmental Context (${S_{context}}$):} This encompasses dynamic environmental conditions directly measurable by the UAV:
\begin{equation}
{S_{context}} = {V} \times {C}
\end{equation}
% where $\mathbf{V}$ represents normalized velocity context, and $\mathbf{C}$ indicates normalized local computation capacity context. We set the minimum and the maximum velocities to \SI{30}{km/h} and \SI{100}{km/h}~\cite{3GPP_TS_22.125}, respectively, and $\mathbf{V}$ is normalized according to these marginal velocity values.
where ${V}$ represents normalized velocity context, and ${C}$ indicates normalized local computation capacity context. We set the minimum and the maximum velocities to \SI{30}{km/h} and \SI{100}{km/h}~\cite{3GPP_TS_22.125}, respectively, and the computation capacity ranges from 0 to 200 units. Both ${V}$ and ${C}$ are normalized to $[0, 1]$ according to their respective marginal values.


\textbf{MEC Server State (${S_{mec}}$):} This component captures the observable state of nearby, or physically attached mobile edge computing servers through feedback mechanisms:
\begin{equation}
{S_{mec}} = {C_m} \times {T_m}
\end{equation}
where ${C_m} = \prod_{k \in K} {C_{m,k}}$ represents ongoing computation units for each task $k \in K$, and ${T_m} = \prod_{k \in K} {T_{m,k}}$ denotes Corresponding processing times for each task $k \in K$. UAV agents can observe MEC state through acknowledgement messages and status updates from edge infrastructure.

\subsubsection{Hidden State Components}

The hidden state space $\mathcal{Z}$ captures environment state components that are not directly observable by individual UAV agents, primarily due to aggregation effects and limited sensing capabilities, while still influencing system dynamics and rewards.

\textbf{Cloud Server State (${S_{cloud}}$):} This component models the remote cloud infrastructure availability in the core network, which is hidden from direct UAV observation because of the aggregated nature of cloud resources serving numerous MEC servers simultaneously:
\begin{equation}
{S_{{cloud}}} = {C_c} \times {T_c}
\end{equation}
where ${C_c} = \prod_{k \in K} {C_{c,k}}$ represents ongoing computation units for each task $k \in K$, and ${T_c} = \prod_{k \in K} {T_{c,k}}$ denotes processing times for each task $k \in K$. The shared and distributed architecture of cloud infrastructure, coupled with the large number of attached MEC nodes, makes direct state monitoring infeasible from individual UAV perspectives.

\textbf{Channel State (${S_{channel}}$):} This captures the wireless communication quality between UAV and infrastructure as
\begin{equation}
{S_{{channel}}} = {Q}
\end{equation}
where ${Q} \in \{0, 1\}$ denotes binary channel quality indicators, with 0 and 1 representing bad and good channel states, respectively. The channel state follows a finite-state Markov channel (FSMC) formulation, adopting a two-state representation derived from the Gilbert–Elliott model~\cite{gilbert1960capacity,elliott1963estimates} for Rayleigh fading channels~\cite{zhang1999finite}.

Although the hidden state components $\mathcal{Z}$ are not included in the UAVs’ local observations, they are explicitly incorporated into the environment state to ensure Markovian dynamics. UAV agents execute their policies based solely on observable states in $\mathcal{O}$, resulting in a POMDP formulation. The recurrent neural network architecture employed in this work is designed to mitigate the effects of partial observability at the agent level by exploiting temporal correlations in observable state sequences, rather than explicitly reconstructing the hidden environment state.

% \subsubsection{Complete State Representation}

% The complete state vector at time $t$ integrates both observable and hidden components:
% \begin{equation}
% s_t = (\underbrace{C_l^{(t)}, E_r^{(t)}, C_q^{(t)}, T_q^{(t)}, I_l^{(t)}, I_o^{(t)}, V^{(t)}, C^{(t)}, C_m^{(t)}, T_m^{(t)}}_{\text{Observable: } \mathcal{O}}, \underbrace{C_c^{(t)}, T_c^{(t)}, Q^{(t)}}_{\text{Hidden: } \mathcal{H}})
% \end{equation}

% All continuous state components are normalized to $[0, 1]$ to ensure stable neural network training and enable effective knowledge transfer across different environmental configurations. The recurrent neural network architecture enables the agent to maintain internal memory of past observations and action outcomes, allowing implicit inference of hidden state components ($\mathcal{H}$) through temporal patterns in observable measurements ($\mathcal{O}$).


\subsection{Action Space}

The action space $\mathcal{A}$ in the ATLAS framework defines the discrete set of decisions available to each UAV agent when processing incoming computational tasks. We formulate the action space as a finite discrete set:

\begin{equation}
\mathcal{A} = \{0, 1, 2\}
\end{equation}
where where 0, 1, and 2 stand for each defined action, respectively. A specific procedure will be stated as follows.

\begin{itemize}
\item \textbf{Local Processing ($a = 0$):} When this action is selected, the UAV processes the computational task using its associating MEC server. The local processing action triggers immediate computation on the UAV, consuming local computational resources but avoiding transmission delays and potential network failures.
\item \textbf{Offloading ($a = 1$):} This action involves transmitting the computational task to available cloud servers for remote processing. The offloading action incurs transmission energy costs and may introduces network-dependent failures, but potentially reduces local computational load and enables processing of resource-intensive tasks.
\item \textbf{Task Discard ($a = 2$):} When system conditions are unfavorable for both local processing and offloading, because of insufficient local resources, saturated server queues, or poor channel conditions that prevent successful task completion, the agent may choose to discard the task. Task discard minimizes immediate resource consumption but results in task failure penalties in the reward function.
\end{itemize}

\subsection{State Transition Function}

The state transition function $\mathcal{P}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$ characterizes the probabilistic evolution of the environment state in response to the agent’s actions.
Following the state definition in Section~\ref{subsec:problem}, the environment state is given by
\begin{equation}
    s = (o, z)
\end{equation}
where $o \in \mathcal{O}$ denotes the observation state available to UAV agents, and $z \in \mathcal{Z}$ represents hidden exogenous environmental states. We also define $s'$ notation to represent the state of the next time epoch, e.g., $s' = (o', z')$.
% where $\mathbf{c}_m, \mathbf{t}_m, \in \mathbb{R}^K$ and $\mathbf{c}_c, \mathbf{t}_c \in \mathbb{R}^K$ represent local-side MEC and cloud server-side queue state vectors, respectively. 
% Each component of the state vector is an element of a predefined domain in~\ref{subsec:state},
% e.g., $c_l \in \mathbf{C}_l$, and $c_l'$ denotes its value in the next state.

Although components of $z$, i.e., cloud information and channel quality, evolve on time scales that are not synchronized with the UAV decision process, the state transition function is defined with respect to decision epochs rather than physical time.
Specifically, the value of each exogenous component is sampled at each decision epoch and incorporated into the environment state, ensuring that the transition $P(s'| s, a)$ is well-defined and Markovian with respect to the decision-aligned state.

The complete transition function can be decomposed as
\begin{equation}
    P(s'|s, a) = P(o'|o, z, a) \cdot P(z' | z),
\end{equation}
where the observable and exogenous components evolve according to distinct mechanisms. This decomposition reflects the fact that UAV actions directly influence task execution, queue dynamics, and local resource utilization, while the evolution of cloud and channel states is governed by external processes independent of individual UAV decisions.

% where $\mathcal{O}' \in \mathcal{O}$ and $s_{hid}' \in \mathcal{S}_{hid}$, respectively, making sure that the dynamics of exogenous states are independent from the agent's action. The endogenous components evolve according to controllable system dynamics influenced by the agent's actions, while the exogenous components evolve independently, modeling external task arrivals and environmental conditions. This decomposition explicitly separates controllable resource allocation from uncontrollable workload variations, enabling the agent to learn robust policies under stochastic task arrivals.

% where $\mathcal{O}' \in \mathcal{O}$ represents the observable state components directly accessible to UAV agents, and external factors, such as channel quality $Q$ and cloud information, $C_c$ and $T_c$ influence state transitions without being part of the state representation. Observable state components include both action-dependent transitions and action-independent transitions. This formulation explicitly captures the partial observability arising from hidden wireless channel conditions and asynchronous multi-agent cloud resource competition, enabling agents to learn robust policies through implicit belief state inference via recurrent neural networks.


\subsubsection{Observable State Transitions}

The state transitions of $P(o'|o, z, a)$ are governed by the agent's action selection and are fully observable by the UAV. These include resource consumption, queue management, and action outcome indicators. In contrast to hidden state transitions, observable components evolve deterministically given the action and current resource availability. $P(o'|o, z, a)$ can be represented as follows:

\begin{equation}
\begin{split}
&P(o' | o, z, a) \\
&= P(c_l' | c_l, c_q, t_q, c_m, t_m, a) \\
&\times P(e_r' | e_r) \\
&\times P(c_q') \\
&\times P(t_q') \\
&\times P(i_l' | i_l, c_l, c_q, c_m, a) \\
&\times P(i_o' | i_o, c_q, z, a) \\
&\times P(c_m' | c_l, c_q, t_q, c_m, t_m, a) \\
&\times P(t_m' | c_l, c_q, t_q, c_m, t_m, a),
\end{split}
\end{equation}
where $c_l \in C_l$, $e_r \in E_r$, $c_q \in C_q$, $t_q \in T_q$, $i_l \in I_l$, $i_o \in I_o$, $c_m \in C_m$, and $t_m \in T_m$ denote the realizations of the corresponding state variables, and their primed counterparts represent the values at the next decision epoch.


\textbf{Local Computation Units ($c_l$):} Available local computation units evolve through resource consumption upon successful local processing and recovery upon MEC task completion.
Define the local processing feasibility indicator:
\begin{equation}
\phi_l \triangleq \mathbf{1}[c_l \geq c_q]\cdot\mathbf{1}\!\left[\exists\, k \in \{1,\dots,K\}: c_{m,k} = 0\right]\cdot\mathbf{1}[c_q > 0],
\end{equation}
where $\mathbf{1}[\cdot]$ denotes the indicator function, yielding 1 when the enclosed condition holds and 0 otherwise. This indicator means that $\phi_l = 1$ indicates that local processing is feasible: sufficient local resources are available, the MEC queue has capacity, and a pending task exists. Then the transition is given by
\begin{equation}
c_l' = c_l - c_q\cdot\mathbf{1}[a{=}0]\cdot\phi_l + \sum_{k=1}^{K}\tilde{c}_{m,k}\cdot\mathbf{1}[\tilde{t}_{m,k}=1],
\end{equation}
where $\tilde{c}_{m,k}$ and $\tilde{t}_{m,k}$ denote the intermediate MEC state after task enqueue but prior to completion processing, as formally defined below.
The first negative term captures resource consumption when a task is successfully scheduled for local processing, while the summation captures resource recovery when MEC-scheduled tasks complete their processing.

\textbf{Remaining Epochs ($e_r$):} The remaining epoch counter decrements deterministically at each decision epoch, independent of the agent's action:
\begin{equation}
e_r' = e_r - 1.
\end{equation}

\textbf{Queue Task Arrivals ($c_q, t_q$):} A new computational task arrives at each decision epoch with computation units and processing time drawn independently from discrete uniform distributions:
\begin{equation}
\begin{split}
c_q' \sim \text{Uniform}(1, \texttt{MAX\_COMP\_UNITS}), \\
t_q' \sim \text{Uniform}(1, \texttt{MAX\_PROC\_TIMES}).
\end{split}
\end{equation}
where $\texttt{MAX\_COMP\_UNITS}$ and $\texttt{MAX\_PROC\_TIMES}$ denote the upper bounds for the task's computation units and processing time, and are predefined as 200 and 50, respectively.
These arrivals are mutually independent and independent of all other state variables, modeling the unpredictable workload in UAV task offloading scenarios.

\textbf{Success Indicators ($i_l, i_o$):} Each indicator is updated only when the corresponding action is taken and retains its previous value otherwise.
The offload feasibility additionally depends on hidden state components $Q \in \mathcal{Z}$ and ${C}_c \in \mathcal{Z}$.
Define the offload feasibility indicator:
\begin{equation}
\phi_o \triangleq \mathbf{1}\!\left[\exists\, k: c_{c,k} = 0\right]\cdot\mathbf{1}[c_q > 0]\cdot\mathbf{1}[Q = 1]\cdot\mathbf{1}[C_c^{\mathrm{avail}} \geq c_q],
\end{equation}
where $C_c^{\mathrm{avail}}$ denotes the remaining shared cloud computation capacity across all UAV agents.
The indicator transitions are
\begin{equation}
i_l' = \begin{cases}\phi_l & \text{if } a = 0,\\ i_l & \text{otherwise,}\end{cases}
\qquad
i_o' = \begin{cases}\phi_o & \text{if } a = 1,\\ i_o & \text{otherwise.}\end{cases}
\end{equation}

\textbf{MEC Server State (${c}_m, {t}_m$):} The MEC queue undergoes a three-phase update per decision epoch: enqueue, completion removal, and processing-time decrement.
Let $\kappa = \min\{k: c_{m,k} = 0\}$ denote the first available queue slot.
The intermediate queue state after task enqueue is
\begin{equation}
\begin{aligned}
\tilde{c}_{m,k} &= c_{m,k} + c_q\cdot\mathbf{1}[a{=}0]\cdot\phi_l\cdot\mathbf{1}[k{=}\kappa], \\
\tilde{t}_{m,k} &= t_{m,k} + t_q\cdot\mathbf{1}[a{=}0]\cdot\phi_l\cdot\mathbf{1}[k{=}\kappa].
\end{aligned}
\end{equation}
Tasks satisfying $\tilde{t}_{m,k}=1$ are identified as completed; their computation units contribute to local resource recovery as described in the $c_l'$ transition above.
The final MEC state after completion removal and one-epoch decrement is
\begin{equation}
\begin{aligned}
c_{m,k}' &= \tilde{c}_{m,k}\;\cdot\;\mathbf{1}[\tilde{t}_{m,k}\neq 1], \\
t_{m,k}' &= \max\!\bigl(\tilde{t}_{m,k}-1,\;0\bigr),
\end{aligned}
\end{equation}
for each $k \in \{1,\dots,K\}$, after which active tasks are compacted to the leading positions of the queue vector.

\textbf{Environmental Context ($v, c$):} Velocity and computation capacity are sampled once at episode initialization and remain constant throughout:
\begin{equation}
v' = v, \quad c' = c.
\end{equation}

\subsubsection{Hidden State Transitions}

The hidden state space $\mathcal{Z} = S_{cloud} \times S_{channel}$ evolves independently of the agent's actions.
Among the hidden components, the channel state $Q \in S_{channel}$ evolves as an autonomous Markov process:
\begin{equation}
P(z' | z) = P(q' | q).
\end{equation}

\textbf{Channel State ($q$):} The binary channel quality evolves as a two-state finite-state Markov channel (FSMC) based on the Gilbert--Elliott model~\cite{gilbert1960capacity,elliott1963estimates}, with transition rates governed by the Doppler effect~\cite{zhang1999finite}.
The Doppler frequency is given by
\begin{equation}
f_d = \frac{v \cdot f_0}{c_{\text{light}}},
\end{equation}
where $f_0 = 5.9\,\text{GHz}$ is the carrier frequency and $c_{\text{light}}$ is the speed of light.
The transition probabilities are parameterized by the normalized Doppler spread $\bar{f}_d = f_d \cdot T_p$, where $T_p$ denotes the decision epoch duration:
\begin{equation}
\begin{aligned}
P(q'{=}1 \mid q{=}0) &= \frac{\bar{f}_d\,\sqrt{2\pi\,\gamma_{\text{thr}}/\bar{\gamma}}}{\exp\!\bigl(\gamma_{\text{thr}}/\bar{\gamma}\bigr) - 1}, \\[4pt]
P(q'{=}0 \mid q{=}1) &= \bar{f}_d\,\sqrt{2\pi\,\gamma_{\text{thr}}/\bar{\gamma}},
\end{aligned}
\end{equation}
where $\gamma_{\text{thr}}$ and $\bar{\gamma}$ denote the SNR threshold and average SNR, respectively, and the complementary probabilities are $P(q'{=}0\mid q{=}0) = 1 - P(q'{=}1\mid q{=}0)$ and $P(q'{=}1\mid q{=}1) = 1 - P(q'{=}0\mid q{=}1)$.
Both transition rates increase with velocity through $\bar{f}_d$, reflecting increased channel variability at higher UAV speeds.

The complete state transition function combines the observable and hidden components:
\begin{equation}
P(s' | s, a) = P(o' | o, z, a) \cdot P(z' | z).
\end{equation}


% The discrete action formulation enables straightforward policy optimization through categorical probability distributions, allowing the neural network to output action probabilities $\pi(a_t|s_t)$ for effective exploration and exploitation during training.

% \subsection{State Transition Function}

% The state transition function $\mathcal{P}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$ defines the probability distribution over next states given the current state and action. In the ATLAS framework, state transitions are governed by the dynamics of the UAV environment, task processing outcomes, and system resource evolution.

% We denote two arbitrary states, the current state $s$ and the next state $s'$ in $\mathcal{S}$ as
% \begin{equation}
%     s = \{c_l, e_r, c_q, t_q, i_l, i_o, v, c, c_m, t_m, c_c, t_c, q\}
% \end{equation}
% and
% \begin{equation}
%     s' = \{c_l', e_r', c_q', t_q', i_l', i_o', v', c', c_m', t_m', c_c', t_c', q'\}
% \end{equation}
% where all components belong to their respective state spaces as defined in the state space formulation.

% % The transition probability can be expressed as:
% % \begin{equation}
% % \mathcal{P}(s_{t+1}|s_t, a_t) = P(s_{t+1} = (c', q', e', v')|s_t = (c, q, e, v), a_t)
% % \end{equation}

% \subsection{State Transition Function}

% The state transition function $\mathcal{P}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$ defines the probability distribution over next states given the current state and action. In the ATLAS framework, state transitions are governed by the dynamics of the UAV environment, task processing outcomes, and system resource evolution.

% We denote two arbitrary states, the current state $s$ and the next state $s'$ in $\mathcal{S}$ as
% \begin{equation}
%     s = \{c_l, e_r, c_q, t_q, i_l, i_o, \mathbf{c}_m, \mathbf{t}_m, v, c, q, \mathbf{c}_c, \mathbf{t}_c\}
% \end{equation}
% and
% \begin{equation}
%     s' = \{c_l', e_r', c_q', t_q', i_l', i_o', \mathbf{c}_m', \mathbf{t}_m', v', c', q', \mathbf{c}_c', \mathbf{t}_c'\}
% \end{equation}
% where $\mathbf{c}_m, \mathbf{t}_m, \mathbf{c}_c, \mathbf{t}_c \in \mathbb{R}^K$ are $K$-dimensional vectors representing MEC and cloud server queue states, and all other components are scalars. Note that channel quality $q$ and cloud states $\mathbf{c}_c, \mathbf{t}_c$ are hidden (not directly observable), while the remaining components constitute the observable state space.

% The state transition function can be decomposed as:
% \begin{equation}
% \begin{split}
% \Pr[s' | s, a] &= \Pr[c_l' | c_l, c_q, a] \\
% &\times \Pr[e_r' | e_r] \\
% &\times \Pr[c_q' | c_q] \\
% &\times \Pr[t_q' | t_q] \\
% &\times \Pr[i_l' | c_l, c_q, \mathbf{c}_m, a] \\
% &\times \Pr[i_o' | \mathbf{c}_c, c_q, q, a] \\
% &\times \prod_{k=1}^{K} \Pr[c_{m,k}' | c_{m,k}, t_{m,k}, a] \\
% &\times \prod_{k=1}^{K} \Pr[t_{m,k}' | t_{m,k}] \\
% &\times \Pr[v'] \times \Pr[c'] \\
% &\times \Pr[q' | q] \\
% &\times \prod_{k=1}^{K} \Pr[c_{c,k}' | c_{c,k}, t_{c,k}, a] \\
% &\times \prod_{k=1}^{K} \Pr[t_{c,k}' | t_{c,k}]
% \end{split}
% \end{equation}
% where the first nine terms represent observable state transitions and the last three terms capture hidden state dynamics.



% The state transition function is the probability of reaching state $s'$ from state $s$ when the agent takes action $a \in \{0, 1, 2\}$ (LOCAL, OFFLOAD, DISCARD). For clarity, we explain each state component's transition dynamics.

% The state transition function can be expressed as:

% \begin{equation}
%     \begin{split}
%         \Pr[s' | s, a] &= \Pr[C_l' | C_l, C_q, a] \\
%         &\times \Pr[E_r' | E_r] \\
%         &\times \Pr[C_q' | C_q] \\
%         &\times \Pr[T_q' | T_q] \\
%         &\times \Pr[I_l' | C_l, C_q, C_m, a] \\
%         &\times \Pr[I_o' | C_c, C_q, Q, a] \\
%         &\times \prod_{k=1}^{K} \Pr[C_{m,k}' | C_{m,k}, T_{m,k}, a] \\
%         &\times \prod_{k=1}^{K} \Pr[T_{m,k}' | T_{m,k}] \\
%         &\times \Pr[V'] \\
%         &\times \Pr[C']
%     \end{split}
% \end{equation}



The state transition dynamics are influenced by several key factors:

\textbf{Computational Resource Evolution:} The computational state transitions depend on task processing outcomes and resource consumption:
\begin{equation}
c'_{avail} = \begin{cases}
\max(0, c_{avail} - \Delta c_{local}) & \text{if } a_t = a_{LOC} \\
c_{avail} & \text{if } a_t = a_{OFF} \\
c_{avail} & \text{if } a_t = a_{DIS}
\end{cases}
\end{equation}

where $\Delta c_{local}$ represents the normalized computational resources consumed for local processing.

\textbf{Queue State Dynamics:} The queue state evolves based on task arrivals, processing actions, and completion events:
\begin{equation}
q'_{units} = f_{queue}(q_{units}, a_t, \text{new\_tasks}, \text{completed\_tasks})
\end{equation}

Queue updates incorporate the stochastic nature of task arrivals and the deterministic effects of processing actions.

\textbf{Environment State Changes:} MEC server states evolve based on system load and external factors:
\begin{equation}
e'_{mec\_units} \sim P_{MEC}(e_{mec\_units}|e_{mec\_units}, \text{system\_load})
\end{equation}

The MEC state transitions follow a semi-Markov process reflecting the dynamic nature of edge computing infrastructure.

\textbf{Vehicle Mobility:} UAV contextual states evolve according to mobility patterns and mission requirements:
\begin{equation}
v'_{velocity} = v_{velocity} + \epsilon_{velocity}
\end{equation}

where $\epsilon_{velocity}$ represents bounded random variations in UAV velocity based on environmental conditions and flight dynamics.

The complete transition function captures the stochastic nature of the UAV environment while maintaining computational tractability for reinforcement learning optimization.

\subsection{Reward Function}

The reward function $\mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ provides immediate feedback to guide the UAV agent's learning process by quantifying the desirability of taking action $a_t$ in state $s_t$. The ATLAS framework designs a multi-objective reward function that balances energy efficiency, processing latency, and task completion success.

The reward function is formulated as a weighted combination of multiple cost and benefit components:

\begin{equation}
\mathcal{R}(s_t, a_t) = -\alpha \cdot C_{energy}(s_t, a_t) - \beta \cdot C_{latency}(s_t, a_t) - \gamma \cdot C_{failure}(s_t, a_t) + \delta \cdot B_{completion}(s_t, a_t)
\end{equation}

where $\alpha$, $\beta$, $\gamma$, and $\delta$ are positive weighting coefficients that control the relative importance of each objective component.

\textbf{Energy Cost ($C_{energy}$):} This component penalizes energy consumption for different actions:
\begin{equation}
C_{energy}(s_t, a_t) = \begin{cases}
E_{local} \cdot q_{units} & \text{if } a_t = a_{LOC} \\
E_{transmission} \cdot q_{units} & \text{if } a_t = a_{OFF} \\
0 & \text{if } a_t = a_{DIS}
\end{cases}
\end{equation}

where $E_{local}$ and $E_{transmission}$ represent normalized energy costs for local processing and task transmission, respectively.

\textbf{Latency Cost ($C_{latency}$):} This component accounts for processing and communication delays:
\begin{equation}
C_{latency}(s_t, a_t) = \begin{cases}
T_{local}(q_{units}, c_{avail}) & \text{if } a_t = a_{LOC} \\
T_{offload}(q_{units}, e_{mec\_times}) + T_{transmission} & \text{if } a_t = a_{OFF} \\
0 & \text{if } a_t = a_{DIS}
\end{cases}
\end{equation}

where $T_{local}$ and $T_{offload}$ are functions computing processing times based on task requirements and available resources.

\textbf{Failure Penalty ($C_{failure}$):} This component penalizes unsuccessful task processing:
\begin{equation}
C_{failure}(s_t, a_t) = \begin{cases}
P_{fail\_local} \cdot \rho_{failure} & \text{if } a_t = a_{LOC} \\
P_{fail\_offload} \cdot \rho_{failure} & \text{if } a_t = a_{OFF} \\
\rho_{discard} & \text{if } a_t = a_{DIS}
\end{cases}
\end{equation}

where $P_{fail\_local}$ and $P_{fail\_offload}$ are failure probabilities, and $\rho_{failure}$ and $\rho_{discard}$ are penalty magnitudes.

\textbf{Completion Benefit ($B_{completion}$):} This component provides positive rewards for successful task completion:
\begin{equation}
B_{completion}(s_t, a_t) = \begin{cases}
(1 - P_{fail\_local}) \cdot \eta_{success} & \text{if } a_t = a_{LOC} \\
(1 - P_{fail\_offload}) \cdot \eta_{success} & \text{if } a_t = a_{OFF} \\
0 & \text{if } a_t = a_{DIS}
\end{cases}
\end{equation}

where $\eta_{success}$ represents the positive reward magnitude for successful task completion.

This multi-objective reward design enables the ATLAS system to learn policies that optimize the trade-off between energy consumption, processing latency, and task success rates across diverse operational conditions.

\subsection{A3C Architecture}

The A3C implementation employs both feedforward and recurrent neural network configurations to handle the sequential nature of the task offloading decisions.

\subsubsection{Network Architecture}

The actor-critic architecture consists of:
\begin{itemize}
\item \textbf{Actor Network}: Outputs action probabilities using a softmax policy
\item \textbf{Critic Network}: Estimates state values for advantage calculation
\item \textbf{Hidden Layers}: 128 neurons with ReLU activation functions
\item \textbf{Recurrent Component}: GRU layers for sequence modeling (when enabled)
\end{itemize}

For the recurrent variant (RecurrentActorCritic), the network processes sequences of observations through GRU layers before the final actor and critic heads, enabling the model to maintain memory of past decisions and environmental states.

\subsubsection{Training Strategies}

We evaluate two distinct training paradigms:

\textbf{A3C Global Training:} In this centralized approach, all workers share updates to a single global model. Each worker interacts with its assigned environment and computes gradients based on the advantage actor-critic loss:

\begin{equation}
L = L_{policy} + \beta \cdot L_{value} - \alpha \cdot H(\pi)
\end{equation}

where $L_{policy}$ is the policy loss, $L_{value}$ is the value function loss, and $H(\pi)$ is the policy entropy term for exploration.

The global model aggregates updates from all workers asynchronously, promoting knowledge sharing across different environmental conditions and worker experiences.

\textbf{Individual Worker Training:} In this decentralized approach, each worker develops its own specialized policy independently. Workers train separate neural networks without sharing parameters, allowing for environment-specific adaptation and specialized behavior development.

\subsection{Environment Configuration}

The experimental framework utilizes a custom UAV environment with the following key parameters:
\begin{itemize}
\item Maximum computation units: 200
\item Maximum computation units for cloud: 1000
\item Maximum epoch size: 100
\item Maximum queue size: 20
\item Agent velocities: 50 units
\end{itemize}

Five distinct environmental configurations are evaluated, each representing different operational scenarios with varying computational demands, network conditions, and resource availability patterns.

\subsection{Performance Metrics}
Our evaluation framework includes the following key metrics:
\begin{itemize}
\item Episode-level total rewards
\item Convergence characteristics across training episodes
\item Statistical significance of performance differences
\item Cross-environment generalization performance
\end{itemize}

\begin{algorithm*}[ht!]
\caption{Training phase of ATLAS using A3C with RNN}
\begin{algorithmic}[1]
    \State Initialize global actor-critic network $\theta$ with recurrent architecture
    \State Initialize shared optimizer and cloud resource state $\mathcal{R}_{\text{cloud}}$
    \For{each worker $w \in \{1, \ldots, W\}$ \textbf{in parallel}}
        \State $\theta_w \gets \theta$ \Comment{Copy global parameters}
        \For{episode $e = 1$ to $E_{\max}$}
            \State Initialize state $s_0$ and RNN hidden state $h_0 \gets \mathbf{0}$
            \While{episode not terminated}
                \State \textbf{// Collect T-step rollout}
                \For{$t = 0$ to $T-1$}
                    \State Compute $\pi(\cdot | s_t, h_t; \theta_w)$ and $V(s_t, h_t; \theta_w)$
                    \State Sample $a_t \sim \pi(\cdot | s_t, h_t; \theta_w)$
                    \State Execute $a_t$, observe $r_t$ and $s_{t+1}$
                    \State Update $h_{t+1}$ via GRU; if done, $h_{t+1} \gets \mathbf{0}$
                \EndFor
                \State \textbf{// Compute returns and advantages}
                \State Bootstrap $R \gets V(s_T, h_T; \theta_w)$ if not done, else $R \gets 0$
                \For{$t = T-1$ down to $0$}
                    \State $R \gets r_t + \gamma R$; $A_t \gets R - V(s_t, h_t; \theta_w)$
                \EndFor
                \State \textbf{// Compute loss}
                \State $\mathcal{L} \gets -\sum_t \log \pi(a_t | s_t, h_t; \theta_w) A_t + \beta_v \sum_t (R_t - V_t)^2 - \beta_h \mathcal{H}(\pi)$
                \State \textbf{// Asynchronous update}
                \State Compute gradients $\nabla_{\theta_w} \mathcal{L}$ and clip
                \State Copy gradients to global: $\nabla_{\theta} \gets \nabla_{\theta_w}$
                \State Update global: $\theta \gets \theta - \alpha \nabla_{\theta}$ (SharedAdam)
                \State Synchronize local: $\theta_w \gets \theta$
            \EndWhile
        \EndFor
    \EndFor
    \State \textbf{return} Trained global network $\theta$
\end{algorithmic}
\label{alg:atlas}
\end{algorithm*}




% Experimental Setup
\section{Experimental Setup}
\label{sec:experimental_setup}

\subsection{Implementation Details}
% Implementation details from params.py

\subsection{Environment Configurations}
Five distinct environmental configurations were evaluated, each representing different operational scenarios with varying computational demands and network conditions.

\subsection{Training Parameters}
% Training parameters from the codebase

% Results and Analysis
\section{Results and Analysis}
\label{sec:results}

\subsection{Performance Comparison}
% Analysis based on environment_comparison results

\subsection{Episode-level Analysis}
% Analysis based on episode_curves results

\subsection{Distribution Analysis}
% Analysis based on distribution_analysis results

\subsection{Cross-Environment Performance}
% Analysis based on performance_heatmap results

\subsection{Statistical Significance}
% Statistical analysis results

% Discussion
\section{Discussion}
\label{sec:discussion}

% Discussion of results, implications, limitations

% Conclusion
\section{Conclusion}
\label{sec:conclusion}

% Conclusion and future work

% References
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
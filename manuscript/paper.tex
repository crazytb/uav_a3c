% LTeX: enabled=false

\documentclass[journal]{IEEEtran}
% \documentclass[lettersize,journal]{IEEEtran}

% Fix font issues
% \renewcommand{\rmdefault}{cmr}
% \renewcommand{\sfdefault}{cmss}
% \renewcommand{\ttdefault}{cmtt}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{graphicx}
\usepackage{array}
\usepackage{textcomp}
\usepackage{bigstrut}
\usepackage{numprint}
\usepackage{multirow}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{makecell}
\usepackage[dvipsnames]{xcolor}
\usepackage[multiple]{footmisc}
\usepackage{subcaption}
\usepackage{siunitx}

\newfloat{algorithm}{t}{lop}
\newcommand{\St}{{\mathchoice{}{}{\scriptscriptstyle}{}S}}
\newcommand{\Ac}{{\mathchoice{}{}{\scriptscriptstyle}{}A}}

% Begin document
\begin{document}

\title{ATLAS: Adaptive Task Offloading System in Multi-UAV-assisted Multi-Access Edge Computing}

\author{Taewon Song and Taeyoon Kim
\thanks{T. Song is with the Department of Internet of Things, College of SW Convergence, Soonchunhyang University, 22 Soonchunhyang-ro, Shinchang-myeon, Asan-si, Chungcheongnam-do, 31538, Korea (e-mail: twsong@sch.ac.kr) and T. Kim is with ... (e-mail: 2000kty@dankook.ac.kr), Corresponding author: T. Kim.}% <-this % stops a space

\thanks{This work was supported in part by ..., and in part by ..., and in part by ...}
% \thanks{Manuscript received XXX, XX, 2015; revised XXX, XX, 2015.}
}

% \markboth{IEEE Transactions on Vehicular Technology,~Vol.~XX, No.~XX, XXX~2015}
{}
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}

\maketitle

\begin{abstract}
Multi-UAV task offloading in edge computing environments requires coordinated decision-making under dynamic network conditions and limited computational resources. This paper presents an A3C-based multi-UAV task offloading system that leverages parameter sharing among distributed workers to achieve superior generalization performance across diverse operational conditions. We formulate the problem as a partially observable Markov decision process where UAV agents select optimal offloading actions, specifically, local processing, MEC offloading, or task discard, based on computational state, queue status, and network conditions. The proposed system employs a recurrent actor-critic architecture with layer normalization to handle sequential dependencies and stabilize asynchronous training across multiple workers. Experimental evaluation demonstrates that the A3C-based approach with parameter sharing significantly outperforms independent learning strategies, achieving enhanced mean performance, improved stability, and superior worst-case robustness. Through systematic ablation studies, we validate our architectural design choices and identify critical system parameters that ensure A3C's advantages. The results provide practical guidelines for deploying distributed reinforcement learning in multi-UAV edge computing systems, demonstrating when coordinated learning approaches provide genuine benefits over independent policies.
\end{abstract}


\begin{IEEEkeywords}
UAV, Task Offloading, A3C, Deep Reinforcement Learning, Multi-Agent Systems, Multi-Access Edge Computing
\end{IEEEkeywords}


\IEEEpeerreviewmaketitle

% Introduction
\section{Introduction}
\label{sec:introduction}

% General problem area
The proliferation of Unmanned Aerial Vehicles (UAVs) in applications ranging from surveillance to emergency response has created unprecedented opportunities for distributed edge computing. Multi-UAV systems must make real-time decisions about computational task offloading while operating under stringent energy constraints, dynamic network conditions, and limited onboard resources. As UAV deployments scale to swarms of tens or hundreds of agents, the challenge of coordinating task distribution across heterogeneous edge computing infrastructure becomes increasingly critical for system performance and mission success.

% Specific problem being addressed
While existing approaches have explored various task offloading strategies~\cite{seid2021multi,wang2023stackelberg,guo2024multi,hao2024joint,li2024robust}, a fundamental gap remains in understanding how to effectively coordinate learning among distributed UAV agents. Traditional centralized optimization methods struggle with the dynamic, partially observable nature of multi-UAV environments, while independent learning approaches fail to leverage collective experience across the swarm. The key challenge lies in designing a distributed reinforcement learning system that can share knowledge effectively among heterogeneous workers operating in diverse conditions, maintain stable performance across varying network topologies and resource availability, and generalize to previously unseen operational scenarios without requiring complete retraining.

% Example
Consider a disaster response scenario where multiple UAVs must coordinate to process sensor data, execute path planning, and transmit critical information to ground stations. Each UAV is equipped with a local mobile edge computing (MEC) server and faces a continuous stream of computational tasks. For each incoming task, the UAV must decide whether to (1) process it locally using its onboard computing resources (consuming battery energy but avoiding communication delays), (2) offload it to cloud servers on the core network through wireless backhaul links (leveraging superior computational capacity but incurring transmission costs, network latency, and potential communication failures), or (3) discard the task when neither local nor cloud processing is feasible (sacrificing task completion to conserve critical resources). The optimal decision depends on the current system state observed by each UAV, which includes local computational state, queue state, cloud server state, local computation capacity, and the wireless channel conditions. Moreover, environmental heterogeneity means that UAVs operating in different regions encounter vastly different network conditions and resource availability, making coordinated learning through parameter sharing essential for achieving robust system-wide performance.

% Thesis statement
In this paper, we present an A3C-based multi-UAV task offloading system that leverages parameter sharing among distributed workers to achieve superior generalization performance across diverse operational conditions. Our system employs a recurrent actor-critic architecture with layer normalization to handle sequential decision dependencies and stabilize asynchronous training across multiple heterogeneous workers.

% Approach
We formulate the multi-UAV task offloading problem as a partially observable Markov decision process (POMDP) where each UAV agent selects actions based on the observations. The proposed architecture integrates recurrent neural networks to capture temporal dependencies in wireless channel conditions and resource availability on the cloud size, which might be hidden on the UAV side, while layer normalization ensures stable gradient updates across asynchronously training workers. Through systematic experimentation across varying resource constraints, network velocities, and exploration parameters, we validate our design choices and identify the operational regimes where distributed learning provides genuine advantages over independent policies.

% Contributions
The main contributions of this paper are as follows:
\begin{itemize}
    \item We design and implement an A3C-based multi-UAV task offloading system with a carefully architected recurrent actor-critic network that achieves superior generalization performance through parameter sharing among distributed workers.
    \item We conduct comprehensive ablation studies across architectural components, hyperparameters, and environmental factors to validate our design choices and establish practical deployment guidelines.
    % \item We demonstrate that A3C's performance advantage stems from its algorithmic design (parameter sharing and asynchronous updates) rather than architectural sophistication, providing fundamental insights into when and why distributed learning outperforms independent approaches.
    \item We identify critical operational regimes---including resource availability, exploration requirements, and environmental dynamics---that determine whether A3C provides genuine benefits, offering practitioners clear criteria for algorithm selection in multi-UAV deployments.
\end{itemize}

% Paper structure
The remainder of this paper is organized as follows. Section~\ref{sec:related_work} reviews related work in UAV task offloading and multi-agent reinforcement learning. Section~\ref{sec:POMDP} formulates the problem as a POMDP and describes our system architecture. Section~\ref{sec:experimental_setup} details the experimental setup and evaluation methodology. Section~\ref{sec:results} presents performance evaluation results and ablation study findings. Section~\ref{sec:discussion} discusses implications and limitations, and Section~\ref{sec:conclusion} concludes the paper.

\section{Related Works}
\label{sec:related_work}

The intersection of UAV-assisted mobile edge computing and deep reinforcement learning has emerged as a critical research area, with various approaches addressing the challenges of distributed task offloading optimization. This section reviews existing work across three key themes: reinforcement learning methods for task offloading, multi-agent coordination and parameter sharing strategies, and architectural components for handling partial observability in sequential decision-making systems.

\subsection{Reinforcement Learning for UAV Task Offloading}

Deep reinforcement learning has been extensively applied to UAV task offloading problems due to its ability to handle complex, dynamic environments without requiring explicit models of system dynamics.

\cite{seid2021multi},
\cite{zhao2022multiagent},
\cite{song2024drqn},
\cite{hao2024joint}, 
\cite{li2024robust},
\cite{guo2025cognitive},


% Value-based methods (DQN, DRQN)
% TODO: Add references on DQN-based task offloading
% TODO: Add references on DRQN for partial observability

% Actor-critic methods
% TODO: Add references on actor-critic approaches
% TODO: Add references on policy gradient methods

% Research gap for this subsection
While these approaches demonstrate the effectiveness of deep RL for task offloading, they primarily focus on single-agent scenarios or do not systematically compare centralized parameter sharing versus independent learning strategies. Moreover, limited attention has been given to A3C algorithms specifically designed for multi-UAV coordination with parameter sharing. \textbf{Gap 1:} Existing work does not systematically compare A3C's global parameter sharing against independent learning strategies, nor does it identify the operational regimes where distributed learning provides genuine advantages.

\subsection{Distributed Learning and Parameter Sharing}

Multi-agent reinforcement learning for edge computing systems requires careful consideration of how knowledge is shared among distributed agents.

\cite{zhuo2019federated},
\cite{wang2020federated},
\cite{tehrani2021federated},
\cite{yu2021when},
\cite{moon2022federated},
\cite{zhao2024federated},

% A3C and asynchronous methods
% TODO: Add original A3C paper and variants
% TODO: Add A3C applications in different domains

% Centralized vs Decentralized training
% TODO: Add references on CTDE (Centralized Training Decentralized Execution)
% TODO: Add references on federated learning approaches

% Parameter sharing strategies
% TODO: Add references on parameter sharing benefits
% TODO: Add references on multi-agent coordination

% Research gap for this subsection
While various distributed learning paradigms have been explored, existing work lacks systematic comparison between A3C with global parameter sharing and fully independent worker training in the context of UAV task offloading. \textbf{Gap 2:} Current research provides limited guidance on when distributed parameter sharing provides genuine advantages over independent learning. The operational regimes---including resource availability, exploration requirements, and environmental dynamics---that determine algorithm effectiveness remain unclear, particularly across heterogeneous operational environments.

\subsection{Architecture Components in Deep Reinforcement Learning}

The design of neural network architectures significantly impacts the performance and stability of deep RL systems, particularly in partially observable environments.

% Recurrent architectures for RL
% TODO: Add references on RNN/LSTM/GRU in RL
% TODO: Add references on handling partial observability

% Normalization techniques
% TODO: Add references on layer normalization in deep RL
% TODO: Add references on batch normalization variants

% Ablation studies in RL
% TODO: Add references on systematic architecture ablation studies
% TODO: Add references on component analysis in deep RL

% Research gap for this subsection
While individual architectural components have been studied in isolation, comprehensive ablation studies that systematically validate design choices across multiple dimensions remain limited. \textbf{Gap 3:} Existing studies lack comprehensive ablation analysis that validates architectural design choices (RNN, layer normalization) in the context of multi-agent asynchronous training across architectural components, hyperparameters, and environmental factors. The interaction effects between recurrent components and normalization techniques in multi-agent asynchronous training have not been thoroughly investigated, particularly regarding their impact on training stability and generalization performance.

Addressing these three research gaps, this paper presents a carefully designed A3C-based multi-UAV task offloading system with comprehensive ablation studies that provide both system design contributions and fundamental insights into when and why distributed learning outperforms independent approaches.

% POMDP Formulation
\section{POMDP Formulation}
\label{sec:POMDP}

\subsection{Problem Formulation}
\label{subsec:problem}

We formulate the multi-UAV task offloading optimization problem as a Partially Observable Markov Decision Process (POMDP), where each UAV agent operates as an intelligent decision maker in a dynamic edge computing environment. The POMDP framework enables UAV agents to make sequential decisions for incoming computational tasks while adapting to changing environmental conditions and resource availability. Furthermore, the POMDP formulation captures the inherent uncertainty in UAV operations, such as fluctuating network conditions and the computation capability of the cloud servers for task offloading.

Formally, the ATLAS framework models the task offloading problem as a tuple $\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $\mathcal{P}$ is the state transition function, $\mathcal{R}$ is the reward function, and $\gamma \in [0,1]$ is the discount factor. Each UAV agent can be in a state $s \in \mathcal{S}$, then selects an action $a \in \mathcal{A}$ according to its policy $\pi(a\mid s)$, and receives a reward $r_t = \mathcal{R}(s, a)$ while transitioning to the next state $s' \sim \mathcal{P}(s'\mid s, a)$.

The primary objective is to learn an optimal policy $\pi^*$ that maximizes the expected cumulative discounted reward:
\begin{equation}
\pi^* = \arg\max_\pi \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r_t \mid \pi\right].
\end{equation}
This formulation enables UAV agents to balance multiple competing objectives including energy efficiency, processing latency minimization, and task completion success rates in dynamic edge computing scenarios.

\subsection{State Space}
\label{subsec:state}

We formulate the multi-UAV task offloading problem as a partially observable Markov decision process (POMDP), where each UAV agent makes decisions based on locally observable information while the underlying environment dynamics are governed by additional exogenous factors that are not directly observable.

We define the environment state space $\mathcal{S}$ as
\begin{equation}
\label{eq:S}
\mathcal{S} = \mathcal{O} \times \mathcal{Z}
\end{equation}
where $\mathcal{O}$ denotes the set of state components observable by individual UAV agents, and $\mathcal{Z}$ represents latent environment state components that are not included in the agents’ local observations. Importantly, these hidden components are part of the environment state and are introduced to preserve the Markov property, rather than being latent variables inferred through belief updates. 
% In this work, we define $\mathcal{S}_{hid}$ as follows:
% \begin{equation}
% \label{eq:S_hid}
% \mathcal{S}_{hid} \triangleq Z = S_{cloud} \times S_{channel}
% \end{equation}
% where $Z$ captures exogenous cloud and wireless channel conditions that influence state transitions and rewards but are not directly observable from the persepective of individual UAV agents.


\subsubsection{Observable State Components}

The observable state space $\mathcal{O}$ consists of system information directly measurable by each UAV agent and is defined as
\begin{equation}
\label{eq:S_obs}
\mathcal{O} = S_{local} \times S_{context} \times S_{mec}.
\end{equation}


\textbf{Local Device State (${S_{local}}$):} This component characterizes the UAV's local computational resources, task queue, and processing feedback:
\begin{equation}
{S_{local}} = {C_l} \times {E_r} \times {C_q} \times {T_q} \times {I_l} \times {I_o}
\end{equation}
where ${C_l}$ represents a set of available computation units, ${E_r}$ denotes a set of remaining epochs during one episode, ${C_q}$ and ${T_q}$ capture sets of queue computation units and processing times, and ${I_l}, {I_o}$ provide binary success indicators for local and offload operations. Above-mentioned values are normalized within $[0, 1]$ to ensure stable neural network training and effective learning across different operational scales.


\textbf{Environmental Context (${S_{context}}$):} This encompasses dynamic environmental conditions directly measurable by the UAV:
\begin{equation}
{S_{context}} = {V} \times {C}
\end{equation}
% where $\mathbf{V}$ represents normalized velocity context, and $\mathbf{C}$ indicates normalized local computation capacity context. We set the minimum and the maximum velocities to \SI{30}{km/h} and \SI{100}{km/h}~\cite{3GPP_TS_22.125}, respectively, and $\mathbf{V}$ is normalized according to these marginal velocity values.
where ${V}$ represents normalized velocity context, and ${C}$ indicates normalized local computation capacity context. We set the minimum and the maximum velocities to \SI{30}{km/h} and \SI{100}{km/h}~\cite{3GPP_TS_22.125}, respectively, and the computation capacity ranges from 0 to 200 units. Both ${V}$ and ${C}$ are normalized to $[0, 1]$ according to their respective marginal values.


\textbf{MEC Server State (${S_{mec}}$):} This component captures the observable state of nearby, or physically attached mobile edge computing servers through feedback mechanisms:
\begin{equation}
{S_{mec}} = {C_m} \times {T_m}
\end{equation}
where ${C_m} = \prod_{k \in K} {C_{m,k}}$ represents ongoing computation units for each task $k \in K$, and ${T_m} = \prod_{k \in K} {T_{m,k}}$ denotes Corresponding processing times for each task $k \in K$. UAV agents can observe MEC state through acknowledgement messages and status updates from edge infrastructure.

\subsubsection{Hidden State Components}

The hidden state space $\mathcal{Z}$ captures environment state components that are not directly observable by individual UAV agents, primarily due to aggregation effects and limited sensing capabilities, while still influencing system dynamics and rewards.

\textbf{Cloud Server State (${S_{cloud}}$):} This component models the remote cloud infrastructure availability in the core network, which is hidden from direct UAV observation because of the aggregated nature of cloud resources serving numerous MEC servers simultaneously:
\begin{equation}
{S_{{cloud}}} = {C_c} \times {T_c}
\end{equation}
where ${C_c} = \prod_{k \in K} {C_{c,k}}$ represents ongoing computation units for each task $k \in K$, and ${T_c} = \prod_{k \in K} {T_{c,k}}$ denotes processing times for each task $k \in K$. The shared and distributed architecture of cloud infrastructure, coupled with the large number of attached MEC nodes, makes direct state monitoring infeasible from individual UAV perspectives.

\textbf{Channel State (${S_{channel}}$):} This captures the wireless communication quality between UAV and infrastructure as
\begin{equation}
{S_{{channel}}} = {Q}
\end{equation}
where ${Q} \in \{0, 1\}$ denotes binary channel quality indicators, with 0 and 1 representing bad and good channel states, respectively. The channel state follows a finite-state Markov channel (FSMC) formulation, adopting a two-state representation derived from the Gilbert–Elliott model~\cite{gilbert1960capacity,elliott1963estimates} for Rayleigh fading channels~\cite{zhang1999finite}.

Although the hidden state components $\mathcal{Z}$ are not included in the UAVs’ local observations, they are explicitly incorporated into the environment state to ensure Markovian dynamics. UAV agents execute their policies based solely on observable states in $\mathcal{O}$, resulting in a POMDP formulation. The recurrent neural network architecture employed in this work is designed to mitigate the effects of partial observability at the agent level by exploiting temporal correlations in observable state sequences, rather than explicitly reconstructing the hidden environment state.

% \subsubsection{Complete State Representation}

% The complete state vector at time $t$ integrates both observable and hidden components:
% \begin{equation}
% s = (\underbrace{C_l^{(t)}, E_r^{(t)}, C_q^{(t)}, T_q^{(t)}, I_l^{(t)}, I_o^{(t)}, V^{(t)}, C^{(t)}, C_m^{(t)}, T_m^{(t)}}_{\text{Observable: } \mathcal{O}}, \underbrace{C_c^{(t)}, T_c^{(t)}, Q^{(t)}}_{\text{Hidden: } \mathcal{H}})
% \end{equation}

% All continuous state components are normalized to $[0, 1]$ to ensure stable neural network training and enable effective knowledge transfer across different environmental configurations. The recurrent neural network architecture enables the agent to maintain internal memory of past observations and action outcomes, allowing implicit inference of hidden state components ($\mathcal{H}$) through temporal patterns in observable measurements ($\mathcal{O}$).


\subsection{Action Space}

The action space $\mathcal{A}$ in the ATLAS framework defines the discrete set of decisions available to each UAV agent when processing incoming computational tasks. We formulate the action space as a finite discrete set:

\begin{equation}
\mathcal{A} = \{0, 1, 2\}
\end{equation}
where where 0, 1, and 2 stand for each defined action, respectively. A specific procedure will be stated as follows.

\begin{itemize}
\item \textbf{Local Processing ($a = 0$):} When this action is selected, the UAV processes the computational task using its associating MEC server. The local processing action triggers immediate computation on the UAV, consuming local computational resources but avoiding transmission delays and potential network failures.
\item \textbf{Offloading ($a = 1$):} This action involves transmitting the computational task to available cloud servers for remote processing. The offloading action incurs transmission energy costs and may introduces network-dependent failures, but potentially reduces local computational load and enables processing of resource-intensive tasks.
\item \textbf{Task Discard ($a = 2$):} When system conditions are unfavorable for both local processing and offloading, because of insufficient local resources, saturated server queues, or poor channel conditions that prevent successful task completion, the agent may choose to discard the task. Task discard minimizes immediate resource consumption but results in task failure penalties in the reward function.
\end{itemize}

\subsection{State Transition Function}

The state transition function $\mathcal{P}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$ characterizes the probabilistic evolution of the environment state in response to the agent’s actions.
Following the state definition in Section~\ref{subsec:problem}, the environment state is given by $s = (o, z)$
where $o \in \mathcal{O}$ denotes the observation state available to UAV agents, and $z \in \mathcal{Z}$ represents hidden exogenous environmental states. We also define $s'$ notation to represent the state of the next time epoch, e.g., $s' = (o', z')$.
% where $\mathbf{c}_m, \mathbf{t}_m, \in \mathbb{R}^K$ and $\mathbf{c}_c, \mathbf{t}_c \in \mathbb{R}^K$ represent local-side MEC and cloud server-side queue state vectors, respectively. 
% Each component of the state vector is an element of a predefined domain in~\ref{subsec:state},
% e.g., $c_l \in \mathbf{C}_l$, and $c_l'$ denotes its value in the next state.

Although components of $z$, i.e., cloud information and channel quality, evolve on time scales that are not synchronized with the UAV decision process, the state transition function is defined with respect to decision epochs rather than physical time.
Specifically, the value of each exogenous component is sampled at each decision epoch and incorporated into the environment state, ensuring that the transition $P(s'\mid  s, a)$ is well-defined and Markovian with respect to the decision-aligned state.

The complete transition function can be decomposed as
\begin{equation}
    P(s'\mid s, a) = P(o'\mid o, z, a) \cdot P(z' \mid  z),
\end{equation}
where the observable and exogenous components evolve according to distinct mechanisms. This decomposition reflects the fact that UAV actions directly influence task execution, queue dynamics, and local resource utilization, while the evolution of cloud and channel states is governed by external processes independent of individual UAV decisions.

% where $\mathcal{O}' \in \mathcal{O}$ and $s_{hid}' \in \mathcal{S}_{hid}$, respectively, making sure that the dynamics of exogenous states are independent from the agent's action. The endogenous components evolve according to controllable system dynamics influenced by the agent's actions, while the exogenous components evolve independently, modeling external task arrivals and environmental conditions. This decomposition explicitly separates controllable resource allocation from uncontrollable workload variations, enabling the agent to learn robust policies under stochastic task arrivals.

% where $\mathcal{O}' \in \mathcal{O}$ represents the observable state components directly accessible to UAV agents, and external factors, such as channel quality $Q$ and cloud information, $C_c$ and $T_c$ influence state transitions without being part of the state representation. Observable state components include both action-dependent transitions and action-independent transitions. This formulation explicitly captures the partial observability arising from hidden wireless channel conditions and asynchronous multi-agent cloud resource competition, enabling agents to learn robust policies through implicit belief state inference via recurrent neural networks.


\subsubsection{Observable State Transitions}

The state transitions of $P(o'\mid o, z, a)$ are governed by the agent's action selection and are fully observable by the UAV. These include resource consumption, queue management, and action outcome indicators. In contrast to hidden state transitions, observable components evolve deterministically given the action and current resource availability. $P(o'\mid o, z, a)$ can be represented as follows:

\begin{equation}
\begin{split}
&P(o' \mid  o, z, a) \\
&= P(c_l' \mid  c_l, c_q, t_q, c_m, t_m, a) \\
&\times P(e_r' \mid  e_r) \\
&\times P(c_q' \mid \cdot) \\
&\times P(t_q' \mid \cdot) \\
&\times P(i_l' \mid  i_l, c_l, c_q, c_m, a) \\
&\times P(i_o' \mid  i_o, c_q, q, c_c, a) \\
&\times P(c_m' \mid  c_l, c_q, t_q, c_m, t_m, a) \\
&\times P(t_m' \mid  c_l, c_q, t_q, c_m, t_m, a),
\end{split}
\end{equation}
where $c_l \in C_l$, $e_r \in E_r$, $c_q \in C_q$, $t_q \in T_q$, $i_l \in I_l$, $i_o \in I_o$, $c_m \in C_m$, and $t_m \in T_m$ denote the realizations of the corresponding state variables, and their primed counterparts represent the values at the next decision epoch.


\textbf{Local Computation Units ($c_l$):} Available local computation units evolve through resource consumption upon successful local processing and recovery upon MEC task completion.
Define the local processing feasibility indicator:
\begin{equation}
\phi_l \triangleq \mathbf{1}[c_l \geq c_q]\cdot\mathbf{1}\!\left[\exists\, k \in \{1,\dots,K\}: c_{m,k} = 0\right]\cdot\mathbf{1}[c_q > 0],
\end{equation}
where $\mathbf{1}[\cdot]$ denotes the indicator function, yielding 1 when the enclosed condition holds and 0 otherwise. This indicator means that $\phi_l = 1$ indicates that local processing is feasible: sufficient local resources are available, the MEC queue has capacity, and a pending task exists.

Let $\kappa_m = \min\{k: c_{m,k} = 0\}$ denote the first available MEC queue slot.
The intermediate MEC state after task enqueue is
\begin{equation}
\label{eq:tilde}
\begin{aligned}
\tilde{c}_{m,k} &= c_{m,k} + c_q\cdot\mathbf{1}[a{=}0]\cdot\phi_l\cdot\mathbf{1}[k{=}\kappa_m], \\
\tilde{t}_{m,k} &= t_{m,k} + t_q\cdot\mathbf{1}[a{=}0]\cdot\phi_l\cdot\mathbf{1}[k{=}\kappa_m].
\end{aligned}
\end{equation}
Then the transition of $c_l$ is given by
% \begin{equation}
% c_l' = c_l - c_q\cdot\mathbf{1}[a{=}0]\cdot\phi_l + \sum_{k=1}^{K}\tilde{c}_{m,k}\cdot\mathbf{1}[\tilde{t}_{m,k}=1],
% \end{equation}
\begin{equation}
\begin{aligned}
& P(c_l' \mid c_l, c_q, t_q, c_m, t_m, a) \\
& =
\mathbf{1}\!\left[
c_l' =
c_l
- c_q\cdot\mathbf{1}[a{=}0]\cdot\phi_l
+ \sum_{k=1}^{K}\tilde{c}_{m,k}\cdot\mathbf{1}[\tilde{t}_{m,k}=1]
\right].
\end{aligned}
\end{equation}

The first negative term captures resource consumption when a task is successfully scheduled for local processing, while the summation captures resource recovery when MEC-scheduled tasks complete their processing.

\textbf{Remaining Epochs ($e_r$):} The remaining epoch counter decrements deterministically at each decision epoch, independent of the agent's action:
% \begin{equation}
% P(e_r' \mid e_r) =
% \begin{cases}
% 1, & e_r' = e_r - 1, \\
% 0, & \text{otherwise}.
% \end{cases}
% \end{equation}
\begin{equation}
P(e_r' \mid e_r) = \mathbf{1}[e_r' = e_r - 1]
\end{equation}


\textbf{Queue Task Arrivals ($c_q, t_q$):} A new computational task arrives at each decision epoch with computation units and processing time drawn independently from discrete uniform distributions:
% \begin{equation}
% \begin{split}
% c_q' \sim \text{Uniform}(1, \texttt{MAX\_COMP\_UNITS}), \\
% t_q' \sim \text{Uniform}(1, \texttt{MAX\_PROC\_TIMES}).
% \end{split}
% \end{equation}
\begin{equation}
\begin{split}
P(c_q' \mid \cdot) &= \frac{1}{C_{\max}}, 
\quad \forall c_q' \in \{1, \dots, C_{\max}\}, \\
P(t_q' \mid \cdot) &= \frac{1}{T_{\max}}, 
\quad \forall t_q' \in \{1, \dots, T_{\max}\}.
\end{split}
\end{equation}
where $C_{\max}$ and $T_{\max}$ denote the maximum number of computation units 
and the maximum processing time, ans are predefined as 200 and 50, respectively. These arrivals are mutually independent and independent of all other state variables, modeling the unpredictable workload in UAV task offloading scenarios.

\textbf{Success Indicators ($i_l, i_o$):} Each indicator is updated only when the corresponding action is taken and retains its previous value otherwise.
The offload feasibility additionally depends on hidden state components $q \in \mathcal{Z}$ and ${c}_c \in \mathcal{Z}$.
Define the offload feasibility indicator:
\begin{equation}
\phi_o \triangleq \mathbf{1}\!\left[\exists\, k: c_{c,k} = 0\right]\cdot\mathbf{1}[c_q > 0]\cdot\mathbf{1}[Q = 1]\cdot\mathbf{1}[c_c^{\mathrm{avail}} \geq c_q],
\end{equation}
where $c_c^{\mathrm{avail}}$ denotes the remaining shared cloud computation capacity across all UAV agents.
Then, the indicator transitions are as follows:
% \begin{equation}
% i_l' = \begin{cases}\phi_l & \text{if } a = 0,\\ i_l & \text{otherwise,}\end{cases}
% \qquad
% i_o' = \begin{cases}\phi_o & \text{if } a = 1,\\ i_o & \text{otherwise.}\end{cases}
% \end{equation}
\begin{equation}
\begin{split}
P(i_l' \mid  i_l, c_l, c_q, c_m, a)
&= \mathbf{1}\!\left[
i_l' = \phi_l\cdot\mathbf{1}[a{=}0]
      + i_l\cdot\mathbf{1}[a{\neq}0]
\right], \\
P(i_o' \mid  i_o, c_q, q, c_c, a)
&= \mathbf{1}\!\left[
i_o' = \phi_o\cdot\mathbf{1}[a{=}1]
      + i_o\cdot\mathbf{1}[a{\neq}1]
\right].
\end{split}
\end{equation}


\textbf{MEC Server State (${c}_m, {t}_m$):} The MEC queue undergoes a three-phase update per decision epoch: enqueue, completion removal, and processing-time decrement.
Using the intermediate queue state $(\tilde{c}_{m,k}, \tilde{t}_{m,k})$ defined in~\eqref{eq:tilde},
tasks satisfying $\tilde{t}_{m,k}=1$ are identified as completed; their computation units contribute to local resource recovery as described in the $c_l'$ transition.
The final MEC state after completion removal and one-epoch decrement is
% \begin{equation}
% \begin{aligned}
% c_{m,k}' &= \tilde{c}_{m,k}\;\cdot\;\mathbf{1}[\tilde{t}_{m,k}\neq 1], \\
% t_{m,k}' &= \max\!\bigl(\tilde{t}_{m,k}-1,\;0\bigr),
% \end{aligned}
% \end{equation}
\begin{equation}
\begin{aligned}
& P(c_{m,k}' \mid c_l, c_q, t_q, c_m, t_m, a) \\
& = \mathbf{1}\!\left[
c_{m,k}' = \tilde{c}_{m,k}\cdot\mathbf{1}[\tilde{t}_{m,k}\neq 1]
\right],\\
& P(t_{m,k}' \mid c_l, c_q, t_q, c_m, t_m, a) \\
& = \mathbf{1}\!\left[
t_{m,k}' = \max\!\bigl(\tilde{t}_{m,k}-1,\;0\bigr)
\right],
\end{aligned}
\end{equation}
% \begin{equation}
% P(t_{m,k}' \mid s,a)=
% \begin{cases}
% 1, & t_{m,k}'=\max(\tilde{t}_{m,k}-1,0),\\
% 0, & \text{otherwise}.
% \end{cases}
% \end{equation}
for each $k \in \{1,\dots,K\}$, after which active tasks are compacted to the leading positions of the queue vector.

\textbf{Environmental Context ($v, c$):} 
Each UAV agent may have a distinct velocity and maximum computation capacity, reflecting heterogeneous platform characteristics. These parameters are sampled once at episode initialization and remain constant throughout the episode:
\begin{equation}
P(v' \mid v) = \mathbf{1}[v' = v], \quad 
P(c' \mid c) = \mathbf{1}[c' = c].
\end{equation}


\subsubsection{Hidden State Transitions}

The hidden state space $\mathcal{Z}=S_{\text{cloud}}\times S_{\text{channel}}$
captures exogenous environment dynamics that are not directly observable by individual UAV agents.
While these components influence state transitions and rewards, their evolution is not synchronized
with the agent-level decision epochs.

Among the hidden components, the channel state $q\in Q$ evolves as an autonomous
finite-state Markov process. The cloud state $c_c \in C_c$ and $t_c \in T_c$ follows queueing dynamics
analogous to those of MEC servers; however, unlike MEC states, cloud queue information is not directly
observable due to aggregation and shared access across multiple UAV agents.
Moreover, the evolution of $c_c$ and $t_c$ is affected by successful offloading actions from other UAVs,
rendering its dynamics asynchronous with respect to the local decision process of any individual agent.

Accordingly, the hidden-state transition at each decision epoch is modeled as a snapshot of these
exogenous processes, and is given by
\begin{equation}
P(z' \mid z)
= P(q' \mid q)\cdot P(c_c',t_c' \mid c_c,t_c),
\end{equation}
where the cloud-state transition is treated as an exogenous stochastic process independent of the
current agent's action.


\textbf{Channel State ($q$):} The binary channel quality evolves as a two-state finite-state Markov channel (FSMC) based on the Gilbert--Elliott model~\cite{gilbert1960capacity,elliott1963estimates}, with transition rates governed by the Doppler effect~\cite{zhang1999finite}. 

The Doppler frequency is given by
\begin{equation}
f_d = \frac{v \cdot f_0}{c_{\text{light}}},
\end{equation}
where $f_0 = 5.9\,\text{GHz}$ is the carrier frequency and $c_{\text{light}}$ is the speed of light. The transition probabilities are parameterized by the normalized Doppler spread $\bar{f}_d = f_d \cdot T_p$, where $T_p$ denotes the decision epoch duration:
% \begin{equation}
% \begin{aligned}
% P(q'{=}1 \mid q{=}0) &= \frac{\bar{f}_d\,\sqrt{2\pi\,\gamma_{\text{thr}}/\bar{\gamma}}}{\exp\!\bigl(\gamma_{\text{thr}}/\bar{\gamma}\bigr) - 1}, \\[4pt]
% P(q'{=}0 \mid q{=}1) &= \bar{f}_d\,\sqrt{2\pi\,\gamma_{\text{thr}}/\bar{\gamma}},
% \end{aligned}
% \end{equation}
\begin{equation}
P(q' \mid q) =
\begin{cases}
p_{01}, & q=0,\; q'=1,\\
1-p_{01}, & q=0,\; q'=0,\\
p_{10}, & q=1,\; q'=0,\\
1-p_{10}, & q=1,\; q'=1,
\end{cases}
\end{equation}
where each transition probabilities are as
\begin{equation}
\begin{aligned}
p_{01} &\triangleq 
\frac{\bar{f}_d\,\sqrt{2\pi\,\gamma_{\text{thr}}/\bar{\gamma}}}
{\exp\!\bigl(\gamma_{\text{thr}}/\bar{\gamma}\bigr)-1},\\[4pt]
p_{10} &\triangleq 
\bar{f}_d\,\sqrt{2\pi\,\gamma_{\text{thr}}/\bar{\gamma}} .
\end{aligned}
\end{equation}
where $\gamma_{\text{thr}}$ and $\bar{\gamma}$ denote the SNR threshold and average SNR, respectively, and the complementary probabilities are $P(q'{=}0\mid q{=}0) = 1 - P(q'{=}1\mid q{=}0)$ and $P(q'{=}1\mid q{=}1) = 1 - P(q'{=}0\mid q{=}1)$.
Both transition rates increase with velocity through $\bar{f}_d$, reflecting increased channel variability at higher UAV speeds.

\textbf{Cloud State ($c_c,t_c$):}
The cloud state captures the aggregated task queue dynamics in the core network, which are not directly observable by individual UAV agents. Unlike MEC server states, the cloud queue evolves asynchronously due to background traffic and successful offloading actions from multiple UAVs, rendering its dynamics exogenous from the perspective of any single agent.

To model temporal correlations in cloud workload, the cloud state $(c_c,t_c)$ is represented as a finite-state Markov process, where $c_c$ and $t_c$ denote the normalized cloud processing capacity and service latency levels, respectively. At each decision epoch, the agent observes a snapshot of this process through its effect on task completion outcomes.

% The cloud-state transition is given by
% \begin{equation}
% P(c_c',t_c' \mid c_c,t_c)
% = \Pi_{\text{cloud}}\!\left[(c_c,t_c)\rightarrow(c_c',t_c')\right],
% \end{equation}
% where $\Pi_{\text{cloud}}$ is a transition matrix capturing background workload fluctuations and
% multi-UAV coupling effects.



% \textbf{Cloud Queue State ($\mathbf{C}_c, \mathbf{T}_c$):} The cloud queue evolves similarly to the MEC queue---tasks are enqueued upon successful offloading and dequeued upon completion. However, two factors make this state hidden from individual agents:

% \begin{enumerate}
%     \item \textbf{Observability:} Cloud infrastructure state is not directly accessible to individual UAV agents due to communication constraints and the aggregated nature of cloud resources serving multiple edge nodes.
    
%     \item \textbf{Multi-Agent Asynchrony:} In the distributed A3C framework, multiple UAV agents compete for shared cloud capacity. When agent $i$ operates at its local time epoch $t_i$, other agents $j \neq i$ may simultaneously execute offloading actions at their respective epochs $t_j$. Consequently, the cloud state transition cannot be expressed as:
%     \begin{equation}
%     \Pr[\mathbf{C}_c', \mathbf{T}_c' \mid  \mathbf{C}_c, \mathbf{T}_c, a_i] \quad \text{(not well-defined)}
%     \end{equation}
%     since it additionally depends on concurrent actions from all agents:
%     \begin{equation}
%     \mathbf{C}_c'(\tau) = f(\mathbf{C}_c(\tau), a_1(\tau), a_2(\tau), \ldots, a_W(\tau))
%     \end{equation}
%     where $\tau$ denotes global wall-clock time and $W$ is the number of parallel workers.
% \end{enumerate}

% From the perspective of agent $i$, cloud availability appears stochastic despite being deterministic at the global level. Each agent queries cloud capacity atomically during action execution via the shared \texttt{NetworkState} and observes only the binary outcome $i_o \in \{0, 1\}$, enabling implicit inference of cloud congestion through recurrent processing of success/failure patterns.


% The complete state transition function combines the observable and hidden components:
% \begin{equation}
% P(s' \mid  s, a) = P(o' \mid  o, z, a) \cdot P(z' \mid  z).
% \end{equation}


% The discrete action formulation enables straightforward policy optimization through categorical probability distributions, allowing the neural network to output action probabilities $\pi(a\mid s)$ for effective exploration and exploitation during training.

% \subsection{State Transition Function}

% The state transition function $\mathcal{P}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$ defines the probability distribution over next states given the current state and action. In the ATLAS framework, state transitions are governed by the dynamics of the UAV environment, task processing outcomes, and system resource evolution.

% We denote two arbitrary states, the current state $s$ and the next state $s'$ in $\mathcal{S}$ as
% \begin{equation}
%     s = \{c_l, e_r, c_q, t_q, i_l, i_o, v, c, c_m, t_m, c_c, t_c, q\}
% \end{equation}
% and
% \begin{equation}
%     s' = \{c_l', e_r', c_q', t_q', i_l', i_o', v', c', c_m', t_m', c_c', t_c', q'\}
% \end{equation}
% where all components belong to their respective state spaces as defined in the state space formulation.

% % The transition probability can be expressed as:
% % \begin{equation}
% % \mathcal{P}(s'\mid s, a) = P(s' = (c', q', e', v')\mid s = (c, q, e, v), a)
% % \end{equation}

% \subsection{State Transition Function}

% The state transition function $\mathcal{P}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$ defines the probability distribution over next states given the current state and action. In the ATLAS framework, state transitions are governed by the dynamics of the UAV environment, task processing outcomes, and system resource evolution.

% We denote two arbitrary states, the current state $s$ and the next state $s'$ in $\mathcal{S}$ as
% \begin{equation}
%     s = \{c_l, e_r, c_q, t_q, i_l, i_o, \mathbf{c}_m, \mathbf{t}_m, v, c, q, \mathbf{c}_c, \mathbf{t}_c\}
% \end{equation}
% and
% \begin{equation}
%     s' = \{c_l', e_r', c_q', t_q', i_l', i_o', \mathbf{c}_m', \mathbf{t}_m', v', c', q', \mathbf{c}_c', \mathbf{t}_c'\}
% \end{equation}
% where $\mathbf{c}_m, \mathbf{t}_m, \mathbf{c}_c, \mathbf{t}_c \in \mathbb{R}^K$ are $K$-dimensional vectors representing MEC and cloud server queue states, and all other components are scalars. Note that channel quality $q$ and cloud states $\mathbf{c}_c, \mathbf{t}_c$ are hidden (not directly observable), while the remaining components constitute the observable state space.

% The state transition function can be decomposed as:
% \begin{equation}
% \begin{split}
% \Pr[s' \mid  s, a] &= \Pr[c_l' \mid  c_l, c_q, a] \\
% &\times \Pr[e_r' \mid  e_r] \\
% &\times \Pr[c_q' \mid  c_q] \\
% &\times \Pr[t_q' \mid  t_q] \\
% &\times \Pr[i_l' \mid  c_l, c_q, \mathbf{c}_m, a] \\
% &\times \Pr[i_o' \mid  \mathbf{c}_c, c_q, q, a] \\
% &\times \prod_{k=1}^{K} \Pr[c_{m,k}' \mid  c_{m,k}, t_{m,k}, a] \\
% &\times \prod_{k=1}^{K} \Pr[t_{m,k}' \mid  t_{m,k}] \\
% &\times \Pr[v'] \times \Pr[c'] \\
% &\times \Pr[q' \mid  q] \\
% &\times \prod_{k=1}^{K} \Pr[c_{c,k}' \mid  c_{c,k}, t_{c,k}, a] \\
% &\times \prod_{k=1}^{K} \Pr[t_{c,k}' \mid  t_{c,k}]
% \end{split}
% \end{equation}
% where the first nine terms represent observable state transitions and the last three terms capture hidden state dynamics.



% The state transition function is the probability of reaching state $s'$ from state $s$ when the agent takes action $a \in \{0, 1, 2\}$ (LOCAL, OFFLOAD, DISCARD). For clarity, we explain each state component's transition dynamics.

% The state transition function can be expressed as:

% \begin{equation}
%     \begin{split}
%         \Pr[s' \mid  s, a] &= \Pr[C_l' \mid  C_l, C_q, a] \\
%         &\times \Pr[E_r' \mid  E_r] \\
%         &\times \Pr[C_q' \mid  C_q] \\
%         &\times \Pr[T_q' \mid  T_q] \\
%         &\times \Pr[I_l' \mid  C_l, C_q, C_m, a] \\
%         &\times \Pr[I_o' \mid  C_c, C_q, Q, a] \\
%         &\times \prod_{k=1}^{K} \Pr[C_{m,k}' \mid  C_{m,k}, T_{m,k}, a] \\
%         &\times \prod_{k=1}^{K} \Pr[T_{m,k}' \mid  T_{m,k}] \\
%         &\times \Pr[V'] \\
%         &\times \Pr[C']
%     \end{split}
% \end{equation}

% The state transition dynamics are influenced by several key factors:

% \textbf{Computational Resource Evolution:} The computational state transitions depend on task processing outcomes and resource consumption:
% \begin{equation}
% c'_{avail} = \begin{cases}
% \max(0, c_{avail} - \Delta c_{local}) & \text{if } a = 0 \\
% c_{avail} & \text{if } a = 1 \\
% c_{avail} & \text{if } a = 2
% \end{cases}
% \end{equation}

% where $\Delta c_{local}$ represents the normalized computational resources consumed for local processing.

% \textbf{Queue State Dynamics:} The queue state evolves based on task arrivals, processing actions, and completion events:
% \begin{equation}
% q'_{units} = f_{queue}(q_{units}, a, \text{new\_tasks}, \text{completed\_tasks})
% \end{equation}

% Queue updates incorporate the stochastic nature of task arrivals and the deterministic effects of processing actions.

% \textbf{Environment State Changes:} MEC server states evolve based on system load and external factors:
% \begin{equation}
% e'_{mec\_units} \sim P_{MEC}(e_{mec\_units}\mid e_{mec\_units}, \text{system\_load})
% \end{equation}

% The MEC state transitions follow a semi-Markov process reflecting the dynamic nature of edge computing infrastructure.

% \textbf{Vehicle Mobility:} UAV contextual states evolve according to mobility patterns and mission requirements:
% \begin{equation}
% v'_{velocity} = v_{velocity} + \epsilon_{velocity}
% \end{equation}

% where $\epsilon_{velocity}$ represents bounded random variations in UAV velocity based on environmental conditions and flight dynamics.

% The complete transition function captures the stochastic nature of the UAV environment while maintaining computational tractability for reinforcement learning optimization.

\subsection{Reward Function}

The reward function $\mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ quantifies the total computation successfully completed at each decision epoch. No reward is assigned at the moment of action selection; reward is received only when previously scheduled tasks finish processing at either the MEC server or the cloud. This deferred-reward structure requires the agent to implicitly associate its current scheduling decisions with future task completions.

A task is identified as completed when its remaining processing time reaches unity in the intermediate queue state. The reward is the total computation units recovered from all completed tasks across both processing locations, scaled by a constant $\rho$:
\begin{equation}
\begin{aligned}
&\mathcal{R}(s, a) \\
&= \rho \left(
  \underbrace{\sum_{k=1}^{K} \tilde{c}_{m,k} \cdot \mathbf{1}[\tilde{t}_{m,k} = 1]}_{R_{\mathrm{MEC}}}
  + \underbrace{\sum_{k=1}^{K} \tilde{c}_{c,k} \cdot \mathbf{1}[\tilde{t}_{c,k} = 1]}_{R_{\mathrm{cloud}}}
\right),
\end{aligned}
\end{equation}
where $(\tilde{c}_{m,k}, \tilde{t}_{m,k})$ is the intermediate MEC queue state defined in the observable state transitions in~\eqref{eq:tilde}. The cloud intermediate state $(\tilde{c}_{c,k}, \tilde{t}_{c,k})$ follows the same enqueue structure with action $a{=}1$ and feasibility indicator $\phi_o$ in place of $a{=}0$ and $\phi_l$:
\begin{equation}
\begin{aligned}
\tilde{c}_{c,k} &= c_{c,k} + c_q\cdot\mathbf{1}[a{=}1]\cdot\phi_o\cdot\mathbf{1}[k{=}\kappa_c], \\
\tilde{t}_{c,k} &= t_{c,k} + t_q\cdot\mathbf{1}[a{=}1]\cdot\phi_o\cdot\mathbf{1}[k{=}\kappa_c],
\end{aligned}
\end{equation}
where $\kappa_c = \min\{k: c_{c,k} = 0\}$ denotes the first available cloud queue slot.

The MEC term $R_{\mathrm{MEC}}$ captures completions at the agent-controlled server, while the cloud term $R_{\mathrm{cloud}}$ captures completions at the shared cloud infrastructure. Both terms contribute identically to the reward, reflecting that the optimization objective is to maximize total computation throughput regardless of processing location. The scaling factor $\rho$ controls the reward magnitude relative to the policy gradient updates and is treated as a hyperparameter.

\section{Deep Reinforcement Learning Algorithm with A3C Architecture}

The ATLAS framework addresses the UAV task offloading problem as a POMDP in which agents cannot directly observe channel quality or cloud resource availability. To handle this partial observability, we employ a recurrent actor-critic network with GRU-based hidden state propagation. 
% We compare two training paradigms—A3C with asynchronous parameter sharing across multiple workers, and individual training where each worker develops an independent policy—to evaluate the benefit of collective exploration and knowledge transfer in dynamic multi-agent environments.

\subsection{A3C Architecture}

The agent employs a recurrent actor-critic network. At each decision epoch, the network receives the current observation $\mathbf{o} \in \mathbb{R}^{d}$ and the hidden state $\mathbf{h} \in \mathbb{R}^{H}$ carried from the previous epoch. Features are first extracted through an embedding layer with layer normalization:
\begin{equation}
\mathbf{z} = \mathrm{ReLU}\bigl(\mathrm{LayerNorm}(\mathbf{W}_{\mathrm{emb}}\,\mathbf{o} + \mathbf{b}_{\mathrm{emb}})\bigr).
\end{equation}
The embedded vector and the previous hidden state are fed into a single-layer GRU, whose output is normalized:
\begin{equation}
\mathbf{h}' = \mathrm{GRU}(\mathbf{z},\, \mathbf{h}), \quad
\tilde{\mathbf{h}}' = \mathrm{LayerNorm}(\mathbf{h}').
\end{equation}
Two linear heads produce the action distribution and state-value estimate from $\tilde{\mathbf{h}}'$:
\begin{equation}
\begin{aligned}
\pi(\cdot \mid o;\theta) &= \mathrm{Categorical}(\mathbf{W}_\pi\,\tilde{\mathbf{h}}' + \mathbf{b}_\pi), \\
V(o;\theta) &= \mathbf{w}_v^\top \tilde{\mathbf{h}}' + b_v.
\end{aligned}
\end{equation}

All linear layers are initialized with orthogonal weights and the hidden state $\mathbf{h}$ is reset to $\mathbf{0}$ at each episode boundary. The hidden dimension is $H = 128$ throughout. A feedforward variant that replaces the GRU and layer normalization with a two-layer ReLU network is additionally evaluated in the ablation study.

\subsection{Learning Algorithm}

\textbf{A3C Global Training:} $W$ workers run in parallel, each assigned a distinct agent velocity from a uniformly spaced grid. All workers share a single global model stored in shared memory. At each update, a worker (i) copies the current global parameters, (ii) collects a $T$-step rollout, (iii) computes gradients on its local copy, and (iv) transfers those gradients to the global model for a single SharedAdam optimizer step. The worker then re-synchronizes with the updated global parameters.

\textbf{Individual Training:} Each of the $W$ workers maintains its own independent model updated with a standard Adam optimizer. No parameters are shared across workers. An episode-level barrier synchronizes all workers at episode boundaries, ensuring that the shared cloud resource state evolves identically under both paradigms for a controlled comparison.

Both paradigms minimize the following combined loss over a $T$-step rollout:
\begin{equation}
\begin{aligned}
\mathcal{L} &= \underbrace{-\frac{1}{T}\sum_{t=0}^{T-1}
  \hat{A}_t \log \pi(a_t \mid o_t;\theta)}_{\mathcal{L}_{\mathrm{policy}}}
  + \;\beta_v \underbrace{\frac{1}{T}\sum_{t=0}^{T-1}
  \bigl(V(o_t;\theta) - G_t\bigr)^2}_{\mathcal{L}_{\mathrm{value}}} \\
  &\quad - \;\beta_H \underbrace{\frac{1}{T}\sum_{t=0}^{T-1}
  H\bigl(\pi(\cdot \mid o_t;\theta)\bigr)}_{\text{entropy bonus}},
\end{aligned}
\end{equation}
where $G_t = \sum_{\tau=t}^{T-1}\gamma^{\tau-t}r_\tau + \gamma^{T-t}V(o_T;\theta)$ is the bootstrapped $T$-step return (setting $V(o_T;\theta)=0$ when the episode terminates within the rollout), and $\hat{A}_t = G_t - V(o_t;\theta)$ is the advantage estimate, normalized to zero mean and unit variance over the rollout. Gradients are clipped to a maximum $\ell_2$-norm of $g_{\max}$.

\subsection{Environment Configuration}

The experimental framework utilizes a custom UAV environment with the following key parameters:
\begin{itemize}
\item Maximum computation units: 200
\item Maximum computation units for cloud: 1000
\item Maximum epoch size: 100
\item Maximum queue size: 20
\item Agent velocities: 50 units
\end{itemize}

Five distinct environmental configurations are evaluated, each representing different operational scenarios with varying computational demands, network conditions, and resource availability patterns.

\subsection{Performance Metrics}
Our evaluation framework includes the following key metrics:
\begin{itemize}
\item Episode-level total rewards
\item Convergence characteristics across training episodes
\item Statistical significance of performance differences
\item Cross-environment generalization performance
\end{itemize}

\begin{algorithm*}[ht!]
\caption{Training phase of ATLAS using A3C with RNN}
\begin{algorithmic}[1]
    \State Initialize global actor-critic network $\theta$ with recurrent architecture
    \State Initialize shared optimizer and cloud resource state $\mathcal{R}_{\text{cloud}}$
    \For{each worker $w \in \{1, \ldots, W\}$ \textbf{in parallel}}
        \State $\theta_w \gets \theta$ \Comment{Copy global parameters}
        \For{episode $e = 1$ to $E_{\max}$}
            \State Initialize observation $o_0$ and RNN hidden state $h_0 \gets \mathbf{0}$
            \While{episode not terminated}
                \State \textbf{// Collect T-step rollout}
                \For{$t = 0$ to $T-1$}
                    \State Compute $\pi(\cdot \mid  o_t, h_t; \theta_w)$ and $V(o_t, h_t; \theta_w)$
                    \State Sample $a \sim \pi(\cdot \mid  o_t, h_t; \theta_w)$
                    \State Execute $a$, observe $r_t$ and $o'$
                    \State Update $h'$ via GRU; if done, $h' \gets \mathbf{0}$
                \EndFor
                \State \textbf{// Compute returns and advantages}
                \State Bootstrap $R \gets V(o_T, h_T; \theta_w)$ if not done, else $R \gets 0$
                \For{$t = T-1$ down to $0$}
                    \State $R \gets r_t + \gamma R$; $A_t \gets R - V(o_t, h_t; \theta_w)$
                \EndFor
                \State \textbf{// Compute loss}
                \State $\mathcal{L} \gets -\sum_t \log \pi(a \mid  o_t, h_t; \theta_w) A_t + \beta_v \sum_t (R_t - V_t)^2 - \beta_h \mathcal{H}(\pi)$
                \State \textbf{// Asynchronous update}
                \State Compute gradients $\nabla_{\theta_w} \mathcal{L}$ and clip
                \State Copy gradients to global: $\nabla_{\theta} \gets \nabla_{\theta_w}$
                \State Update global: $\theta \gets \theta - \alpha \nabla_{\theta}$ (SharedAdam)
                \State Synchronize local: $\theta_w \gets \theta$
            \EndWhile
        \EndFor
    \EndFor
    \State \textbf{return} Trained global network $\theta$
\end{algorithmic}
\label{alg:atlas}
\end{algorithm*}




% Experimental Setup
\section{Experimental Setup}
\label{sec:experimental_setup}

\subsection{Implementation Details}
% Implementation details from params.py

\subsection{Environment Configurations}
Five distinct environmental configurations were evaluated, each representing different operational scenarios with varying computational demands and network conditions.

\subsection{Training Parameters}
% Training parameters from the codebase

% Results and Analysis
\section{Results and Analysis}
\label{sec:results}

\subsection{Performance Comparison}
% Analysis based on environment_comparison results

\subsection{Episode-level Analysis}
% Analysis based on episode_curves results

\subsection{Distribution Analysis}
% Analysis based on distribution_analysis results

\subsection{Cross-Environment Performance}
% Analysis based on performance_heatmap results

\subsection{Statistical Significance}
% Statistical analysis results

% Discussion
\section{Discussion}
\label{sec:discussion}

% Discussion of results, implications, limitations

% Conclusion
\section{Conclusion}
\label{sec:conclusion}

% Conclusion and future work

% References
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
# Why A3C Benefits MORE from Layer Normalization than Individual

## í•µì‹¬ ì§ˆë¬¸

**Individualì´ training stabilityì—ì„œ ë” í° ê°œì„ (+91.1%)ì„ ë³´ì˜€ëŠ”ë°, ì™œ generalizationì—ì„œëŠ” A3Cê°€ í›¨ì”¬ ë” ì¢‹ì€ ê²°ê³¼(+251%)ë¥¼ ë³´ì˜€ì„ê¹Œ?**

| Metric | A3C | Individual |
|--------|-----|------------|
| **Training Stability** (Value Loss ê°ì†Œ) | +61.5% | **+91.1%** âœ“ |
| **Generalization** (Test ì„±ëŠ¥ í–¥ìƒ) | **+251%** âœ“âœ“âœ“ | -13.9% âœ— |

â†’ **ì—­ì„¤**: Individualì´ ë” ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí–ˆì§€ë§Œ, A3Cê°€ ë” ì˜ ì¼ë°˜í™”í•œë‹¤!

---

## 5ê°€ì§€ ê°€ì„¤ ê²€ì¦ ê²°ê³¼

### 1. Value Function Quality (ê°€ì¥ ì¤‘ìš”!) ğŸ¯

**ì¸¡ì • ì§€í‘œ**: Value Loss / Reward Ratio (ë‚®ì„ìˆ˜ë¡ ì •í™•í•œ value function)

```
A3C With LN:    0.484 (Value Loss=30.4, Reward=62.7)
A3C Without LN: 1.378 (Value Loss=78.9, Reward=57.2)
â†’ LN reduces ratio by 64.9%
```

**í•µì‹¬ í†µì°°**:
- A3C + LN â†’ Value functionì´ **í›¨ì”¬ ë” ì •í™•**
- ì •í™•í•œ Value function â†’ Policy gradientì˜ ë¶„ì‚° ê°ì†Œ â†’ ë” ì¢‹ì€ ì¼ë°˜í™”
- Without LN: Value Loss(78.9)ê°€ Reward(57.2)ë³´ë‹¤ í¬ë‹¤! (ë¹„ìœ¨ 1.378)
  - ì´ëŠ” value functionì´ rewardë¥¼ ì œëŒ€ë¡œ ì˜ˆì¸¡í•˜ì§€ ëª»í•¨ì„ ì˜ë¯¸

**ì™œ A3Cì—ì„œ ì´ íš¨ê³¼ê°€ ë” í´ê¹Œ?**
- A3CëŠ” 5ê°œ workerì˜ gradientë¥¼ **ì§‘ê³„(aggregate)**
- LNì´ ê° workerì˜ activationì„ ì •ê·œí™” â†’ gradient ì‹ í˜¸ê°€ ë” ì¼ê´€ì 
- Individualì€ ë‹¨ì¼ worker â†’ gradient ì§‘ê³„ì˜ ì´ì  ì—†ìŒ

---

### 2. Learning Speedì™€ Convergence Quality

**Reward ì„ê³„ê°’ ë„ë‹¬ ì†ë„** (episode ìˆ˜):

| Threshold | A3C With LN | A3C Without LN |
|-----------|-------------|----------------|
| Reward > 50 | Episode 0 | Episode 2 |
| Reward > 70 | Episode 13 | Episode 3 |
| Reward > 90 | **Episode 61** | Episode 20 |

**Final Performance (Last 1000 episodes)**:
- With LN: **72.36** (max=136.75)
- Without LN: 57.49 (max=122.00)

**ê´€ì°°**:
- ì´ˆë°˜ì—ëŠ” Without LNì´ ë” ë¹ ë¥´ê²Œ Reward 70ì— ë„ë‹¬ (Ep 3 vs 13)
- í•˜ì§€ë§Œ ìµœì¢… ì„±ëŠ¥ì€ With LNì´ **25.9% ë” ë†’ìŒ**
- Without LNì€ Reward 90ì„ ë¹¨ë¦¬ ë„˜ì§€ë§Œ ê·¸ ì´ìƒ í–¥ìƒì´ ì–´ë ¤ì›€
- **With LN**: ëŠë¦¬ì§€ë§Œ ë” ë†’ì€ ìµœì ì ì— ë„ë‹¬

**í•´ì„**:
- Without LN: ë¹ ë¥¸ ìˆ˜ë ´ì´ì§€ë§Œ **êµ­ì†Œ ìµœì ì (local optimum)**ì— ë¹ ì§
- With LN: ëŠë¦° ì´ˆê¸° í•™ìŠµì´ì§€ë§Œ **ë” ì¢‹ì€ ì „ì—­ ìµœì ì (global optimum)** ë°œê²¬
- ì´ê²ƒì´ generalization ì°¨ì´ë¥¼ ì„¤ëª…!

---

### 3. Exploration vs Exploitation Balance

**Policy Loss ë¶„ì„** (entropyì˜ proxy):

```
A3C Policy Loss (averaged over training):
  With LN:    -0.0075
  Without LN: -0.0015
  Difference: -0.0060
```

**Policy Loss Evolution (Early â†’ Late)**:
- With LN: 0.0016 â†’ -0.0137 (ë³€í™”: -0.0153)
- Without LN: -0.0018 â†’ -0.0030 (ë³€í™”: -0.0012)

**í•´ì„**:
- **With LN**: Policy lossê°€ ë” í¬ê²Œ ë³€í™” â†’ **ë” ë§ì€ exploration**
- Without LN: Policy lossê°€ ê±°ì˜ ë³€í•˜ì§€ ì•ŠìŒ â†’ ë¹ ë¥´ê²Œ exploitationìœ¼ë¡œ ì „í™˜
- ë” ë§ì€ exploration â†’ ë‹¤ì–‘í•œ í™˜ê²½ì— ëŒ€í•œ robustí•œ policy í•™ìŠµ

---

### 4. Value Loss Variance Evolution

**Training ì´ˆë°˜ vs í›„ë°˜ì˜ Value Loss í‘œì¤€í¸ì°¨**:

```
A3C Value Loss Std Evolution (Early 1500 eps â†’ Late 1500 eps):
  With LN:    48.1 â†’ 42.4 (-11.7%)
  Without LN: 129.1 â†’ 43.3 (-66.4%)
```

**ê´€ì°°**:
- Without LN: ì´ˆë°˜ì— ë§¤ìš° ë¶ˆì•ˆì • (Ïƒ=129.1) â†’ í›„ë°˜ì— ì•ˆì •í™” (Ïƒ=43.3)
- With LN: ì´ˆë°˜ë¶€í„° ì•ˆì •ì  (Ïƒ=48.1) â†’ í›„ë°˜ì—ë„ ìœ ì‚¬ (Ïƒ=42.4)

**í•µì‹¬ í†µì°°**:
- Without LNì˜ ì´ˆê¸° ë¶ˆì•ˆì •ì„± â†’ **íƒìƒ‰ì˜ í­ì´ ì¢ìŒ**
- ë¹ ë¥´ê²Œ ì•ˆì •í™”ë˜ì§€ë§Œ íƒìƒ‰ì´ ë¶€ì¡±í–ˆê¸° ë•Œë¬¸ì— êµ­ì†Œ ìµœì ì ì— ìˆ˜ë ´
- With LN: ì´ˆë°˜ë¶€í„° ì•ˆì •ì  â†’ ì¶©ë¶„í•œ íƒìƒ‰ ê°€ëŠ¥ â†’ ë” ì¢‹ì€ ìµœì ì  ë°œê²¬

---

### 5. Individualì˜ Over-Stabilization Problem

**Individual Worker 0 - ë¶„ì‚° ë¹„êµ**:

| Metric | With LN | Without LN | LN íš¨ê³¼ |
|--------|---------|------------|---------|
| **Value Loss Std** | 46.14 | 187.09 | **-75.3%** âœ“ |
| **Policy Loss Std** | 0.0868 | 0.0568 | **+52.7%** âœ— |
| **Final Reward** | 49.32 | 51.54 | -4.3% âœ— |
| **Final Reward Std** | 13.55 | 17.35 | **-21.9%** |

**í•µì‹¬ ë°œê²¬**:

1. **Value LossëŠ” í¬ê²Œ ê°ì†Œ** (75.3%) â†’ í•™ìŠµì´ ë§¤ìš° ì•ˆì •ì 
2. **í•˜ì§€ë§Œ Policy Loss ë¶„ì‚°ì€ ì˜¤íˆë ¤ ì¦ê°€** (52.7%) â†’ í˜¼ë€ìŠ¤ëŸ¬ìš´ ì‹ í˜¸
3. **Final RewardëŠ” Without LNì´ ë” ë†’ìŒ** (51.54 vs 49.32)
4. **Reward ë¶„ì‚°ì´ ê°ì†Œ** (21.9%) â†’ âš ï¸ **ê³¼ì í•©(overfitting)ì˜ ì§•í›„!**

**ì™œ Individualì€ LNìœ¼ë¡œ overfittingë˜ë‚˜?**

```
Individual ëª¨ë¸ì˜ íŠ¹ì„±:
1. ë‹¨ì¼ worker â†’ ì œí•œëœ íƒìƒ‰
2. ì œí•œëœ capacity (íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ì ìŒ)
3. LNì´ activationì„ ê°•í•˜ê²Œ ì œì•½ â†’ capacityê°€ ë”ìš± ê°ì†Œ

ê²°ê³¼:
â†’ Training í™˜ê²½ì—ëŠ” ì˜ ë§ì§€ë§Œ (ë‚®ì€ variance)
â†’ ìƒˆë¡œìš´ í™˜ê²½ì—ëŠ” ì¼ë°˜í™” ëª»í•¨ (Extra -33.2%)
```

**Without LNì—ì„œì˜ "implicit regularization"**:
- Value Loss explosions â†’ **ê°•ì œì ì¸ íƒìƒ‰(forced exploration)**
- ë†’ì€ reward variance â†’ ë‹¤ì–‘í•œ ê²½í—˜ ìˆ˜ì§‘
- ê²°ê³¼: ì¼ë¶€ worker(W0, W3, W4)ê°€ ì¢‹ì€ ì¼ë°˜í™” ë‹¬ì„±

---

## í†µí•© ë©”ì»¤ë‹ˆì¦˜: ì™œ A3Cê°€ ë” ì´ë“ì„ ë³´ë‚˜?

### A3Cì˜ ì„±ê³µ ìš”ì¸

```
Multi-Worker Gradient Aggregation + Layer Normalization

1. ê° workerê°€ ë‹¤ë¥¸ trajectory ê²½í—˜
   â†“
2. LNì´ ê° workerì˜ activation ì •ê·œí™”
   â†“
3. Gradient signalì´ ì¼ê´€ì„± ìˆê²Œ ì§‘ê³„
   â†“
4. Value functionì˜ ì •í™•ë„ í–¥ìƒ (ratio 0.484)
   â†“
5. ì •í™•í•œ policy gradient â†’ ë” ì¢‹ì€ ìµœì ì 
   â†“
6. ì¶©ë¶„í•œ exploration (policy loss -0.0153 ë³€í™”)
   â†“
7. ìš°ìˆ˜í•œ generalization (+251%)
```

**í•µì‹¬**: A3Cì˜ **multi-worker architecture**ê°€ LNê³¼ **ì‹œë„ˆì§€**ë¥¼ ì¼ìœ¼í‚´

### Individualì˜ ì‹¤íŒ¨ ìš”ì¸

```
Single Worker + Layer Normalization

1. ë‹¨ì¼ worker â†’ ì œí•œëœ íƒìƒ‰
   â†“
2. LNì´ activation ê°•í•˜ê²Œ ì œì•½
   â†“
3. Capacity ê°ì†Œ (ì´ë¯¸ ì‘ì€ë° ë” ì œì•½)
   â†“
4. Value functionì€ ì•ˆì •ì ì´ì§€ë§Œ ë¶€ì •í™• (training envì—ë§Œ ë§ì¶¤)
   â†“
5. Reward variance ê°ì†Œ (21.9%) â†’ ê³¼ì í•©
   â†“
6. ìƒˆë¡œìš´ í™˜ê²½ì— ì¼ë°˜í™” ì‹¤íŒ¨ (Extra -33.2%)
```

**í•µì‹¬**: Individualì˜ **single-worker architecture**ê°€ LNê³¼ **ì¶©ëŒ**

---

## ì •ëŸ‰ì  ì¦ê±° ìš”ì•½

### Value Function Quality (ê°€ì¥ ê°•ë ¥í•œ ì¦ê±°)

| Model | Value Loss | Reward | Ratio | Generalization |
|-------|------------|--------|-------|----------------|
| **A3C + LN** | 30.4 | 62.7 | **0.484** | +251% âœ“âœ“âœ“ |
| A3C - LN | 78.9 | 57.2 | 1.378 | baseline |
| **Individual + LN** | 46.1 | 49.3 | 0.935 | -13.9% âœ— |
| Individual - LN | 187.1 | 51.5 | 3.632 | baseline |

**í•µì‹¬ ìƒê´€ê´€ê³„**:
- **ë‚®ì€ ratio â†’ ì¢‹ì€ generalization** (A3C + LN: 0.484, +251%)
- **ë†’ì€ ratio â†’ ë‚˜ìœ generalization** (Individual - LN: 3.632)

BUT: Individual + LNì˜ ê²½ìš° ratioëŠ” ê°œì„ (0.935)ë˜ì—ˆì§€ë§Œ generalizationì€ ì•…í™”!
â†’ **Capacity constraint** ë•Œë¬¸ì— training envì—ë§Œ ê³¼ì í•©

---

## ê²°ë¡ 

### ì™œ A3Cê°€ Individualë³´ë‹¤ LNì—ì„œ ë” í° ì´ë“ì„ ë³´ëŠ”ê°€?

**3ê°€ì§€ í•µì‹¬ ì´ìœ **:

1. **Multi-Worker Synergy** ğŸ”¥
   - 5ê°œ workerì˜ gradient ì§‘ê³„ + LNì˜ ì •ê·œí™”
   - ê° workerì˜ ì¼ê´€ì„± ìˆëŠ” gradient signal
   - Individualì€ ì´ ì´ì ì´ ì „í˜€ ì—†ìŒ

2. **Value Function Quality** ğŸ¯
   - A3C + LN: Ratio 0.484 (ë§¤ìš° ì •í™•)
   - A3C - LN: Ratio 1.378 (ë¶€ì •í™•)
   - 64.9% ê°œì„  â†’ 251% generalization í–¥ìƒ
   - Individualì€ ratio ê°œì„ ì—ë„ capacity ë¶€ì¡±ìœ¼ë¡œ overfitting

3. **Exploration vs Exploitation Balance** ğŸ”
   - A3C + LN: ì¶©ë¶„í•œ exploration (policy loss -0.0153 ë³€í™”)
   - A3C - LN: ë¹ ë¥¸ exploitation (policy loss -0.0012 ë³€í™”)
   - Individual + LN: Over-stabilization â†’ íƒìƒ‰ ë¶€ì¡± â†’ ê³¼ì í•©

---

## ì‹¤ìš©ì  í•¨ì˜

### âœ“ A3Cì— LN ì‚¬ìš©ì„ ê°•ë ¥íˆ ê¶Œì¥

- Training stability: +61.5%
- Generalization: +251%
- Value function quality: +64.9%
- **ëª¨ë“  ì§€í‘œì—ì„œ ê°œì„ **

### âœ— Individualì— LN ì‚¬ìš© ê¶Œì¥í•˜ì§€ ì•ŠìŒ

- Training stability: +91.1% (ì¢‹ì•„ ë³´ì´ì§€ë§Œ...)
- Generalization: **-13.9%** (íŠ¹íˆ Extra -33.2%)
- Overfitting ìœ„í—˜ (reward variance -21.9%)

### Individualì„ ìœ„í•œ ëŒ€ì•ˆ

1. **Gradient Clipping** - Value loss explosionë§Œ ë°©ì§€
2. **Adaptive Learning Rate** - ë¶ˆì•ˆì •í•  ë•Œë§Œ learning rate ê°ì†Œ
3. **Value Loss Clipping** - ì •ê·œí™” ì—†ì´ explosionë§Œ ë°©ì§€
4. **Ensemble** - ì—¬ëŸ¬ Individual ëª¨ë¸ ì¡°í•© (A3C íš¨ê³¼ ëª¨ë°©)

---

## íŒŒì¼ ë° ì‹œê°í™”

- **ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸**: [analyze_why_a3c_benefits_more.py](analyze_why_a3c_benefits_more.py)
- **ì‹œê°í™”**: `why_a3c_benefits_more_from_ln.png` (9ê°œ subplot)
  - Row 1: A3C Value Loss ë¹„êµ
  - Row 2: A3C Reward ë¹„êµ
  - Row 3: A3C vs Individual Generalization ë¹„êµ

---

**ìµœì¢… ë‹µë³€**:

A3Cê°€ Individualë³´ë‹¤ LNì—ì„œ ë” í° ì´ë“ì„ ë³´ëŠ” ì´ìœ ëŠ” **multi-worker gradient aggregationê³¼ Layer Normalizationì˜ ì‹œë„ˆì§€ íš¨ê³¼** ë•Œë¬¸ì…ë‹ˆë‹¤.

LNì€ ê° workerì˜ activationì„ ì •ê·œí™”í•˜ì—¬ gradient signalì„ ì¼ê´€ì„± ìˆê²Œ ë§Œë“¤ê³ , ì´ê²ƒì´ ì§‘ê³„ë  ë•Œ **ë” ì •í™•í•œ value function**ì„ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤ (ratio 0.484 vs 1.378).

ë°˜ë©´ Individualì€ single workerì´ê¸° ë•Œë¬¸ì— ì´ëŸ° ì‹œë„ˆì§€ê°€ ì—†ê³ , ì˜¤íˆë ¤ LNì˜ ì œì•½ì´ **capacityë¥¼ ê°ì†Œ**ì‹œì¼œ training environmentì—ë§Œ ê³¼ì í•©ë©ë‹ˆë‹¤.

**â†’ Architecture matters! ê°™ì€ ê¸°ë²•ì´ë¼ë„ ì•„í‚¤í…ì²˜ì— ë”°ë¼ ì •ë°˜ëŒ€ì˜ íš¨ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.**

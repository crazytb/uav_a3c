# UAV Task Offloading - Neural Network Architecture

## ğŸ“Š ì „ì²´ êµ¬ì¡° ê°œìš”

í˜„ì¬ ì‹œë®¬ë ˆì´ì…˜ì€ **RecurrentActorCritic** ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

```
Input (State: 48-dim)
    â†“
Feature Extraction Layer
    â†“
Recurrent Layer (GRU)
    â†“
Actor-Critic Heads
    â†“
Output (Policy Logits: 3-dim, Value: 1-dim)
```

---

## ğŸ”¢ ì…ë ¥ êµ¬ì¡° (State Space: 48 dimensions)

### State Composition

| Component | Dimension | Description | Normalization |
|-----------|-----------|-------------|---------------|
| **Queue Information** (40-dim) | | | |
| â”” `mec_comp_units` | 20 | MEC íì˜ ê° íƒœìŠ¤í¬ ê³„ì‚°ëŸ‰ | [0, 1] (Ã· max_comp_units) |
| â”” `mec_proc_times` | 20 | MEC íì˜ ê° íƒœìŠ¤í¬ ì²˜ë¦¬ì‹œê°„ | [0, 1] (Ã· max_proc_times) |
| **Context Features** (2-dim) | | | |
| â”” `ctx_vel` | 1 | Agent ì†ë„ (generalization ë³€ìˆ˜) | [0, 1] normalized |
| â”” `ctx_comp` | 1 | Max computation units (generalization ë³€ìˆ˜) | [0, 1] normalized |
| **Current Task Flags** (2-dim) | | | |
| â”” `local_success` | 1 | ë¡œì»¬ ì²˜ë¦¬ ì„±ê³µ ì—¬ë¶€ | {0, 1} discrete |
| â”” `offload_success` | 1 | ì˜¤í”„ë¡œë“œ ì„±ê³µ ì—¬ë¶€ | {0, 1} discrete |
| **Scalar Features** (4-dim) | | | |
| â”” `available_computation_units` | 1 | í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ê³„ì‚° ìœ ë‹› | [0, 1] normalized |
| â”” `remain_epochs` | 1 | ë‚¨ì€ ì—í¬í¬ ìˆ˜ | [0, 1] normalized |
| â”” `queue_comp_units` | 1 | í˜„ì¬ íƒœìŠ¤í¬ ê³„ì‚°ëŸ‰ | [0, 1] normalized |
| â”” `queue_proc_times` | 1 | í˜„ì¬ íƒœìŠ¤í¬ ì²˜ë¦¬ì‹œê°„ | [0, 1] normalized |
| **Total** | **48** | | |

### Heterogeneous Input Structure (ì¤‘ìš”!)

ì…ë ¥ ìƒíƒœëŠ” **ì´ì§ˆì (heterogeneous)** êµ¬ì¡°:
- **ì—°ì†í˜•(Continuous)**: 44ê°œ (í ì •ë³´ 40 + ì»¨í…ìŠ¤íŠ¸ 2 + ìŠ¤ì¹¼ë¼ 2)
- **ì´ì‚°í˜•(Discrete)**: 2ê°œ (ì„±ê³µ í”Œë˜ê·¸)
- **ë²”ìœ„(Scale)**: ëª¨ë‘ [0, 1]ë¡œ ì •ê·œí™”ë˜ì–´ ìˆì§€ë§Œ, ë¶„í¬ê°€ ë‹¤ë¦„

ì´ ì´ì§ˆì„±ì´ **Layer Normalization ì—†ì´ëŠ” Value Loss explosion**ì„ ì¼ìœ¼í‚¤ëŠ” ì£¼ìš” ì›ì¸!

---

## ğŸ§  ì‹ ê²½ë§ ì•„í‚¤í…ì²˜: RecurrentActorCritic

### ì „ì²´ êµ¬ì¡°

```python
RecurrentActorCritic(
    state_dim=48,
    action_dim=3,
    hidden_dim=128,
    num_layers=1,
    use_layer_norm=True/False  # ì‹¤í—˜ ë³€ìˆ˜
)
```

### Layer-by-Layer êµ¬ì¡°

#### 1. Feature Extraction Layer

```
Input: (B, 48)
    â†“
Linear(48 â†’ 128)
    â†“
[LayerNorm(128)]  â† Optional (use_layer_norm=True)
    â†“
ReLU
    â†“
Output: (B, 128)
```

**ìˆ˜ì‹**:
```
z = Linear(x)           # (B, 48) â†’ (B, 128)
z = LayerNorm(z)        # Optional: Î¼=0, Ïƒ=1 per sample
z = ReLU(z)             # Activation
```

**íŒŒë¼ë¯¸í„° ìˆ˜**:
- Linear: 48 Ã— 128 + 128 = **6,272**
- LayerNorm (if enabled): 128 + 128 = **256** (Î³, Î²)

#### 2. Recurrent Layer (GRU)

```
Input: (B, 1, 128)  + Hidden: (1, B, 128)
    â†“
GRU(128 â†’ 128, num_layers=1, batch_first=True)
    â†“
Output: (B, 128)  + Next Hidden: (1, B, 128)
    â†“
[LayerNorm(128)]  â† Optional (use_layer_norm=True)
    â†“
```

**GRU ë‚´ë¶€ êµ¬ì¡°** (ë‹¨ì¼ ë ˆì´ì–´):
```
r_t = Ïƒ(W_ir @ x_t + b_ir + W_hr @ h_{t-1} + b_hr)  # Reset gate
z_t = Ïƒ(W_iz @ x_t + b_iz + W_hz @ h_{t-1} + b_hz)  # Update gate
n_t = tanh(W_in @ x_t + b_in + r_t âŠ™ (W_hn @ h_{t-1} + b_hn))  # New gate
h_t = (1 - z_t) âŠ™ n_t + z_t âŠ™ h_{t-1}  # Hidden state
```

**íŒŒë¼ë¯¸í„° ìˆ˜**:
- GRU: 3 Ã— (128Ã—128 + 128Ã—128 + 128 + 128) = 3 Ã— (16,384 + 16,384 + 256) = **98,688**
- LayerNorm (if enabled): 128 + 128 = **256**

#### 3. Actor-Critic Heads

##### Policy Head (Actor)

```
Input: (B, 128)
    â†“
Linear(128 â†’ 3)
    â†“
[+ Action Mask]  â† ìœ íš¨í•˜ì§€ ì•Šì€ actionì— -inf
    â†“
Output: Logits (B, 3)
```

**íŒŒë¼ë¯¸í„° ìˆ˜**: 128 Ã— 3 + 3 = **387**

##### Value Head (Critic)

```
Input: (B, 128)
    â†“
Linear(128 â†’ 1)
    â†“
Output: Value (B, 1)
```

**íŒŒë¼ë¯¸í„° ìˆ˜**: 128 Ã— 1 + 1 = **129**

---

## ğŸ“ˆ Total Parameter Count

### Without Layer Normalization

```
Feature Layer:    6,272
GRU:             98,688
Policy Head:        387
Value Head:         129
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:          105,476
```

### With Layer Normalization

```
Feature Layer:    6,272
  LayerNorm:        256  â† Added
GRU:             98,688
  LayerNorm:        256  â† Added
Policy Head:        387
Value Head:         129
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:          106,988

Difference:        +512 parameters (0.5% increase)
```

**ê²€ì¦** (ì‹¤ì œ ì¸¡ì •):
- With LN: **106,372** (ì•½ê°„ì˜ ì°¨ì´ëŠ” PyTorch ë‚´ë¶€ êµ¬í˜„ ì°¨ì´)
- Without LN: **105,860**

---

## ğŸ”„ Forward Pass (Step-by-Step)

### 1-Step Forward (Environment Interaction)

```python
def step(x, hx, action_mask=None):
    """
    x: (B, 48)        - Current state
    hx: (1, B, 128)   - Previous hidden state
    """
    # 1. Feature Extraction
    z = self.feature(x)              # (B, 48) â†’ (B, 128)
    z = self.ln_feature(z)           # (B, 128) â†’ (B, 128) [Optional]
    z = F.relu(z)                    # ReLU activation

    # 2. Recurrent Processing
    z, next_hx = self.rnn(z.unsqueeze(1), hx)  # (B, 1, 128), (1, B, 128)
    z = z[:, 0, :]                   # (B, 128)
    z = self.ln_rnn(z)               # (B, 128) â†’ (B, 128) [Optional]

    # 3. Actor-Critic Heads
    logits = self.policy(z)          # (B, 128) â†’ (B, 3)
    if action_mask is not None:
        logits = logits + (action_mask + 1e-45).log()  # Mask invalid actions
    value = self.value(z)            # (B, 128) â†’ (B, 1)

    return logits, value, next_hx
```

### T-Step Forward (Training with BPTT)

```python
def rollout(x_seq, hx, done_seq=None):
    """
    x_seq: (B, T, 48)     - Sequence of states
    hx: (1, B, 128)       - Initial hidden state
    done_seq: (B, T)      - Episode termination flags
    """
    for t in range(T):
        logits_t, value_t, hx = self.step(x_seq[:, t, :], hx)

        # Reset hidden state if episode done
        if done_seq is not None:
            done_t = done_seq[:, t].float().view(1, B, 1)
            hx = hx * (1.0 - done_t)

    return logits_seq, values_seq, hx
```

---

## ğŸ¯ Action Space

```python
action_space = Discrete(3)

Actions:
  0: LOCAL     - Process task locally
  1: OFFLOAD   - Offload to MEC server
  2: DISCARD   - Discard the task
```

### Action Masking

ìœ íš¨í•˜ì§€ ì•Šì€ actionì„ ë§ˆìŠ¤í‚¹í•˜ì—¬ policy logitsì—ì„œ ì œê±°:
```python
logits = logits + (action_mask + 1e-45).log()
# action_mask[i] = 0 â†’ logits[i] = -inf â†’ P(action=i) = 0
```

---

## ğŸ”§ Hyperparameters

### Network Configuration

| Parameter | Value | Description |
|-----------|-------|-------------|
| `state_dim` | 48 | Input state dimension |
| `action_dim` | 3 | Number of actions (LOCAL/OFFLOAD/DISCARD) |
| `hidden_dim` | 128 | GRU hidden dimension |
| `num_layers` | 1 | Number of GRU layers |
| `use_layer_norm` | True/False | Enable Layer Normalization |

### Training Configuration

| Parameter | Value | Description |
|-----------|-------|-------------|
| `gamma` | 0.99 | Discount factor |
| `entropy_coef` | 0.05 | Entropy regularization coefficient |
| `value_loss_coef` | 0.25 | Value loss weight |
| `lr` | 1e-4 | Learning rate (Adam) |
| `max_grad_norm` | 2.0 | Gradient clipping threshold |

### A3C Configuration

| Parameter | Value | Description |
|-----------|-------|-------------|
| `n_workers` | 5 | Number of parallel workers |
| `target_episode_count` | 2000 | Episodes per worker |
| `device` | CPU | Training device |

---

## ğŸ§ª Layer Normalization Details

### Mathematical Formulation

For a feature vector **z** of dimension D:

```
Î¼ = (1/D) Î£ z_i                    # Mean
ÏƒÂ² = (1/D) Î£ (z_i - Î¼)Â²          # Variance
z_norm = (z - Î¼) / sqrt(ÏƒÂ² + Îµ)  # Normalize
output = Î³ âŠ™ z_norm + Î²           # Scale and shift
```

**Learnable parameters**:
- Î³ (gamma): Scale parameter (initialized to 1.0)
- Î² (beta): Shift parameter (initialized to 0.0)

### Application Points in RecurrentActorCritic

1. **After Feature Extraction** (`ln_feature`)
   ```
   Linear(48â†’128) â†’ LayerNorm â†’ ReLU
   ```
   - **Why**: Input heterogeneity (40-dim queue + 2-dim context + 2-dim flags + 4-dim scalars)
   - **Effect**: Normalizes before ReLU, preventing dead neurons

2. **After RNN Output** (`ln_rnn`)
   ```
   GRU â†’ LayerNorm
   ```
   - **Why**: RNN hidden state accumulation can cause value explosion
   - **Effect**: Stabilizes value function learning

---

## ğŸ“Š Comparison: With vs Without Layer Normalization

### Architectural Difference

```
WITHOUT Layer Normalization:
Input(48) â†’ Linear(128) â†’ ReLU â†’ GRU(128) â†’ Policy(3)
                                           â†’ Value(1)

WITH Layer Normalization:
Input(48) â†’ Linear(128) â†’ LN â†’ ReLU â†’ GRU(128) â†’ LN â†’ Policy(3)
                                                      â†’ Value(1)
```

### Impact on Training

| Metric | Without LN | With LN | Change |
|--------|------------|---------|--------|
| **A3C Value Loss** | 78.9 | 30.4 | **-61.5%** |
| **A3C Generalization** | Baseline | +251% | **+251%** |
| **Individual Value Loss** | 187.1 | 46.1 | **-75.3%** |
| **Individual Generalization** | Baseline | -13.9% | **-13.9%** |

**Key Insight**: Same architecture, opposite effects depending on A3C vs Individual!

---

## ğŸ“ Weight Initialization

### Orthogonal Initialization

```python
def _init_weights(m):
    if isinstance(m, nn.Linear):
        nn.init.orthogonal_(m.weight, gain=1.0)
        nn.init.zeros_(m.bias)
```

**Why Orthogonal?**
- Preserves gradient norm during backpropagation
- Prevents gradient vanishing/explosion in deep networks
- Standard for RL applications (especially A3C)

### LayerNorm Initialization

```python
# Default PyTorch initialization (no custom init needed)
LayerNorm.weight (Î³) = 1.0
LayerNorm.bias (Î²) = 0.0
```

---

## ğŸ”„ Information Flow

### Training Phase

```
Environment â†’ State(48)
    â†“
RecurrentActorCritic
    â†“
Policy Logits(3) + Value(1)
    â†“
Sample Action â†’ Execute
    â†“
Collect (s, a, r, s', done)
    â†“
Compute Advantage = r + Î³V(s') - V(s)
    â†“
Policy Loss = -log Ï€(a|s) Ã— Advantage
Value Loss = (r + Î³V(s') - V(s))Â²
Entropy Loss = -Î£ Ï€ log Ï€
    â†“
Total Loss = Policy + 0.25Ã—Value + 0.05Ã—Entropy
    â†“
Backprop + Adam Optimizer
    â†“
[A3C]: Aggregate gradients from 5 workers
    â†“
Update Global Model
```

### Evaluation Phase

```
Environment â†’ State(48)
    â†“
RecurrentActorCritic (no_grad)
    â†“
Policy Logits(3)
    â†“
Greedy Action = argmax(logits)  or  Sample from softmax
    â†“
Execute Action
    â†“
Collect Reward
```

---

## ğŸ“ Key Design Choices

### 1. Why GRU instead of LSTM?

- **Fewer parameters**: GRU has 3 gates vs LSTM's 4 gates
- **Faster training**: Simpler computation
- **Similar performance**: For RL tasks, GRU often matches LSTM
- **Better for A3C**: Faster gradient computation for multi-worker training

### 2. Why hidden_dim=128?

- **Balance**: Not too large (overfitting), not too small (underfitting)
- **Standard**: Common choice for RL tasks
- **Computation**: Reasonable for CPU training
- **Generalization**: Sufficient capacity for 48-dim input

### 3. Why num_layers=1?

- **Simplicity**: Single GRU layer avoids gradient issues
- **Sufficient**: Task complexity doesn't require deep recurrence
- **Training speed**: Faster convergence with shallow RNN
- **A3C requirement**: Faster forward/backward for multi-worker

### 4. Why Action Masking?

- **Validity**: Prevents invalid actions (e.g., offload when queue full)
- **Efficiency**: Focuses exploration on valid actions
- **Safety**: Ensures environment doesn't receive invalid commands

---

## ğŸ” Debugging and Inspection

### Check Model Parameters

```python
# Total parameters
total_params = sum(p.numel() for p in model.parameters())
print(f"Total parameters: {total_params:,}")

# Check LayerNorm presence
has_ln = any('ln_' in name for name, _ in model.named_modules())
print(f"Has LayerNorm: {has_ln}")

# Layer-wise parameters
for name, param in model.named_parameters():
    print(f"{name}: {param.shape} ({param.numel():,} params)")
```

### Forward Pass Shape Check

```python
batch_size = 4
state_dim = 48
model = RecurrentActorCritic(state_dim, action_dim=3, hidden_dim=128)

x = torch.randn(batch_size, state_dim)
hx = model.init_hidden(batch_size)

logits, value, next_hx = model.step(x, hx)

print(f"Input: {x.shape}")           # (4, 48)
print(f"Logits: {logits.shape}")     # (4, 3)
print(f"Value: {value.shape}")       # (4, 1)
print(f"Hidden: {next_hx.shape}")    # (1, 4, 128)
```

---

## ğŸ“š References

- **A3C Paper**: Mnih et al. (2016) "Asynchronous Methods for Deep Reinforcement Learning"
- **Layer Normalization**: Ba et al. (2016) "Layer Normalization"
- **GRU**: Cho et al. (2014) "Learning Phrase Representations using RNN Encoder-Decoder"
- **Actor-Critic**: Sutton & Barto (2018) "Reinforcement Learning: An Introduction"

---

**File**: [networks.py](drl_framework/networks.py)
**Configuration**: [params.py](drl_framework/params.py)
**Environment**: [custom_env.py](drl_framework/custom_env.py)

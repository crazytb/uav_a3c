\documentclass[journal]{IEEEtran}
% \documentclass[lettersize,journal]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{graphicx}
\usepackage{array}
\usepackage{textcomp}
\usepackage{bigstrut}
\usepackage{numprint}
\usepackage{multirow}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{makecell}
\usepackage[dvipsnames]{xcolor}
\usepackage[multiple]{footmisc}
\usepackage{subcaption}

\newfloat{algorithm}{t}{lop}
\newcommand{\St}{{\mathchoice{}{}{\scriptscriptstyle}{}S}}
\newcommand{\Ac}{{\mathchoice{}{}{\scriptscriptstyle}{}A}}



\begin{document}

\title{Deep Reinforcement Learning Based \\ Age-of-Information-Aware Low-Power Active Queue Management for IoT Sensor Networks}

\author{Taewon Song and Yeunwoong Kyung
\thanks{T. Song is with the Department of Internet of Things, SCH \mbox{MediaLabs}, Soonchunhyang University, 22 Soonchunhyang-ro, Shinchang-myeon, Asan-si, Chungcheongnam-do, 31538, Korea (e-mail: twsong@sch.ac.kr) and Y. Kyung is with the Division of Information \& Communication Engineering, Kongju National University, Chunan-si, Chungcheongnam-do, 31080, Korea (e-mail: ywkyung@kongju.ac.kr), Corresponding author: Y. Kyung.}% <-this % stops a space

\thanks{This work was supported in part by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2022R1F1A1076069), and in part by ``Regional Innovation Strategy (RIS)'' through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (MOE) (2021RIS-004), and in part by the Soonchunhyang University Research Fund.}
% \thanks{Manuscript received XXX, XX, 2015; revised XXX, XX, 2015.}
}

% \markboth{IEEE Transactions on Vehicular Technology,~Vol.~XX, No.~XX, XXX~2015}
{}
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}

\maketitle


\begin{abstract}
As the number of Internet of Things (IoT) sensors increases and their deployment becomes denser, power management for IoT sensor networks becomes more important. In most IoT sensor networks, one cluster head (CH) collects data from a large number of sensors and forwards them to backbone networks. Thus, managing CH's queue condition is crucial in order to extend the network's lifespan or satisfy quality of service (QoS) requirements. 
Meanwhile, reducing age of information (AoI), a metric describing how fresh information is, has become one of the most important metrics, from simple data such as temperature and humidity to more complex data that must be timely, such as vehicle information and road dynamics. However, it is shown that AoI may heavily fluctuate depending on the medium access control protocol. In this paper, we propose a deep reinforcement learning-based AoI-aware low-power active queue management for IoT sensor networks. To this end, we formulate a Markov decision process model in which the CH can select one of the actions including forward, flush, or leave buffered data from associated cluster member nodes.
Extensive simulations show that compared with traditional queue management methods, our queue management method can reduce the power consumption of CH while trying not to exceed the AoI value threshold, thereby enabling IoT sensor networks to be stable while ensuring satisfactory QoS.
\end{abstract}


\begin{IEEEkeywords}
Wireless sensor networks, active queue management, deep reinforcement learning, deep-Q network
\end{IEEEkeywords}


\IEEEpeerreviewmaketitle



\section{Introduction}
\label{sec:introduction}

A wireless sensor network (WSN) in Internet-of-things (IoT) is a wireless network used to monitor a large number of sensors without infrastructure. It's commonly employed to oversee physical and environmental conditions of various systems~\cite{Lazarescu2013}. Wireless IoT sensors are convenient, easy to use, and can monitor conditions that were previously difficult to monitor. With the increasing number of IoT devices and systems, wireless IoT sensors are becoming more important for ensuring efficient and effective operation.

One of the major challenges of WSNs is that the devices are usually battery-powered and have limited energy resources. Indeed, in the era of ``big data'', a massive amount of data is putting a huge burden on the devices, making low power consumption one of its most significant performance metrics. Therefore, energy efficiency is an important issue in WSN design.

Besides energy efficiency, another important challenge for WSNs is to guarantee quality of service (QoS), which includes latency, throughput, reliability, and so on. Among these features, we are focusing on age-of-information (AoI), which is the time elapsed from the creation of the most recently generated data until it is received by the destination. Among the large number of applications that use WSN-based IoT networks, especially monitoring-related applications, it can be important to know how up-to-date the data is from the time it was generated. Therefore, AoI is an appropriate performance metric to measure the performance of WSN applied to the IoT environment.

Maintaining low AoI values while minimizing battery consumption is a difficult task. For instance, frequent transmissions may ensure low AoI, but this approach can lead to a higher number of transmissions and retransmissions caused by collisions, which can significantly drain the battery. Consider, for example, a smart IoT sensor system deployed in a city infrastructure. These sensors gather data on various environmental factors, such as temperature, air quality, and noise levels, and communicate this information to a central hub. The hub then processes the data to provide real-time insights for city management, like environmental monitoring. Keeping the AoI low is crucial to ensure timely and relevant data, but it is equally important to minimize the energy consumption of the IoT sensors to prolong their battery life. Designing a WSN that can balance a low AoI with energy efficiency requires careful consideration of these trade-offs.

In this paper, we propose DeepAAQM, a deep reinforcement learning-based algorithm for low-power active queue management in IoT WSNs. With DeepAAQM, the agent in a cluster head (CH) can decide to forward, flush, or leave collected data based on timestamps that represent the moment the frame was created on the connected CM. We formulate a Markov decision process (MDP) model and use deep-Q networks (DQN)~\cite{mnih2013playing} algorithm which is one of the well-known value-based algorithms that updates parameters using action-state value function to obtain an optimal policy. We simulate DeepAAQM on top of two well-known channel access methods, slotted ALOHA and CSMA/CA, and verify the proposed technique through extensive simulations. Performance analysis shows that DeepAAQM significantly reduces power consumption while satisfying an arbitrary peak AoI level.

To sum up, the contributions of this paper are threefold: 
\begin{itemize}
    \item We propose DeepAAQM, a deep reinforcement learning-based algorithm for low-power active queue management in IoT WSNs that can maintain an AoI value while minimizing energy consumption with the optimized policy acting at the head of the queue.
    \item We formulate an MDP model and obtain an optimal policy for DeepAAQM by using the DQN algorithm.
    \item We conduct simulations on top of slotted ALOHA and CSMA/CA to verify the proposed algorithm, showing that DeepAAQM significantly reduces power consumption while maintaining a competent AoI value level.
\end{itemize}

The remainder of this paper is organized as follows. Section~\ref{sec:related} offers a concise overview of relevant studies as the initial step. We next describe the system model of DeepAAQM in Section~\ref{sec:systemmodel}. The problem is formulated in Section~\ref{sec:deeplesson}. We next describe a learning-based algorithm for DeepAAQM in Section~\ref{sec:drl}. Performance evaluation results and conclusion are given in Sections~\ref{sec:performance} and~\ref{sec:conclusion}, respectively.

\section{Related Works}
\label{sec:related}


\subsection{Energy-efficient cluster-based communication protocol}
\label{subsec:protocol}
Routing protocols can be divided into two types: Flat routing and cluster-based routing. Flat routing protocols have all sensor nodes performing the same role and functions in the network. On the other hand, in cluster-based routing, the network is divided into clusters and some nodes are selected as special nodes based on certain criteria. Since DeepAAQM is based on cluster-based routing, the introduction of related works will be limited to cluster-based routing protocols.

There has been a lot of research on energy-efficient cluster-based communication protocols, which can extend battery life and ensure devices are always operational and ready to transmit data, which is essential for many IoT applications. LEACH~\cite{Heinzelman2000} is a cluster-based protocol that evenly distributes power consumption among sensors by randomly rotating the role of CH. Fuzzy logic-based clustering methods, such as those in~\cite{Gupta2005} and~\cite{Dwivedi2021}, build on and enhance LEACH. Q-LEACH~\cite{Manzoor2013} partitions the network to enable wide coverage and enhances stability period, network lifetime, and throughput. Haque \textit{et al.} proposed a hybrid approach~\cite{Haque2022} that enhances the lifespan of WSNs by jointly considering network topology maintenance and construction algorithms. O-LEACH~\cite{Senthil2022} minimizes energy usage by covering the entire network with a minimum number of orphaned nodes. However, in order to implement these approaches, the routing table must be dynamically modified and CH candidate nodes should have reception capability, which can be a hurdle to widely adapting IoT networks.

Existing research has mostly focused on routing optimization, while dynamic power management has received comparatively little attention. Queue management protocol based on queue threshold technique proposed by Maheswar~\textit{et al.} in~\cite{Maheswar2018} can reduce the number of transmissions to improve network lifetime. Reinforcement learning for dynamic power management to ensure QoS for diverse applications is adopted by Hosahalli \textit{et. al.} in~\cite{Hosahalli2020}. Al Mahdi \textit{et al.} proposed an energy-efficient queue management scheme in~\cite{Alwasef2021energy}. The proposed algorithm saves buffer space and thus can decrease power consumption at CHs. However, these approaches require many parameters and they may require high operation capabilities for sensor nodes.


\subsection{AoI analysis}
\label{subsec:aoi}
Besides energy-efficient communication protocols, analyzing the AoI has become crucial in WSNs for IoT. AoI measures the timeliness of information, which is critical in monitoring systems. This subsection reviews studies on AoI analysis in WSNs.

Kaul \textit{et al.}~\cite{Kaul2012} defined a metric known as the age of information (AoI) that described how `fresh' collected data was. Specifically, if the most recent update at the CH carries a time stamp $U(t)$ at time $t$, then the AoI at the time $\Delta(t) = t - U(t)$. Therefore, AoI is a suitable metric to comprehensively describe the freshness of information that cannot be expressed only with the throughput and the delay. In addition, in~\cite{Sun2016}, the authors argued that `zero-wait' policy, i.e., work-conserving scheduling, did not always minimize the average age. In~\cite{Yates2021}, the authors introduced variations of AoI and mathematically analyzed them. Among the variations, we focus on peak AoI which is the maximum AoI within a particular duration as it can reflect the minimum freshness of information~\cite{diao2022}.

\subsection{Active queue management}
Active queue management (AQM) is an intelligent packet-dropping technique that has been widely studied in the field of networking. It is designed to prevent network congestion by proactively dropping packets before the queue becomes full. Therefore, AQM is crucial for WSNs to balance low age-of-information (AoI) values with low power consumption. 

Random early drop (RED)~\cite{floyd1993random} is one of the most extensively studied algorithms for AQM. RED tracks queue size with two thresholds, min and max. If the queue size is below the min threshold, no packet is discarded. If the queue size is between the min and max thresholds, packets can be discarded with a probability calculated using the max probability, average queue size, and min threshold. If the average queue size exceeds the max threshold, all packets are discarded. Stabilized RED (SRED)~\cite{Ott1999} is a representative variant algorithm based on RED. The SRED algorithm stabilizes queue size by estimating the number of active flows and instantaneous queue size, and computing packet drop probabilities based on this estimation. Controlled delay (CoDel)~\cite{Nichols2012} is a relatively recently developed AQM technique published as RFC8289~\cite{rfc8289} to address the bufferbloat issue, leading to reduced latency and jitter in packet-switched networks due to excessive packet buffering.

In a programmable network, such as software-defined network (SDN), the control plane is separated from the hardware and implemented in software. Integrating active queue management into SDN~\cite{palma2014queuepusher,gu2015controlled} improves network performance by reducing delays and boosting throughput, while keeping the data plane simple and using the global network view of its control plane. Further, a low-latency communication technology in SDN is reviewed in~\cite{yan2021survey}, especially focusing on traffic management and congestion control. Packet rank-aware AQM (PR-AQM) is introduced in~\cite{li2023packet}, a system designed to execute AQM on a programmable data plane. This is achieved by real-time capture and analysis of packet rank distribution and the level of queue congestion. Although these studies underscore SDN's potential in queue management, our algorithm differs by addressing the AoI, which is a crucial metric for services using WSNs.

As machine learning has proven its practical applicability through multiple use cases in fields ranging from robotics to process automation, it is also being adopted and studied to optimize AQM. Reinforcement learning-based AQM (RL-AQM)~\cite{Alwahab2021}, based on reinforcement learning is proposed to manage the network resources and keep the queue delay low at the same time. Deep Q-network-based AQM (DQN-AQ)~\cite{kim2021deep} is proposed based on the deep reinforcement learning (DRL) technique and a scaling factor to achieve the trade-off between queuing delay and throughput is introduced. Meanwhile, the algorithms mentioned above decide whether to admit or discard the packet when it enters the queue. Due to the very dynamic nature of the wireless environment, these policies may have large fluctuations in AoI.

\section{System Model}
\label{sec:systemmodel}

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figure/song1.eps}
\caption{Wireless sensor network architecture.}
\label{fig:wsn}
\end{figure}
Suppose we have a WSN consisting of several cluster members (CMs), cluster heads (CHs) with which some CMs are associated, and a sink node with which all CHs are associated. Fig.~\ref{fig:wsn} illustrates a typical type of WSN. The CMs can either be monitoring IoT sensors, home appliances, platooning vehicles, or unmanned aerial vehicles (UAVs), according to their applications. One CH and several CMs form an area, which is called a cluster. Each CH performs the role of forwarding data monitored by and transmitted from its associated CMs. The sink node can typically be a base station (BS) for cellular networks or an access point (AP) for wireless local area networks. The CH is responsible for forwarding data from the sensors to a sink node. In order for the AoI value not to exceed a certain threshold, for example, it may be possible for the CH to transmit the data collected from the CMs in a fast cycle, but this may cause a serious amount of power consumption. Therefore, we need to minimize the amount of power consumed while guaranteeing to maintain an appropriate AoI value level as much as possible.

We consider a WSN network, as shown in Fig.~\ref{fig:wsn}, composed of one sink node, several wireless CHs, and lots of wireless CMs. Each wireless CM collects sensing data and transmits them to corresponding CH\footnote{The role of CH can be rotated among CMs, but in this paper, it is assumed that the role of CH is assigned to a specific CM during the simulation period.}. Each CH forwards the buffered sensing data to the sink node. 
It is assumed that the CH is within transmission range of the sink node and the CMs are within range of the associating CH.
Transmission channels of clusters are not overlapped and thus each agent in each CH can operate independently. The transmissions occur through wireless medium via a certain wireless medium access protocol; in this literature, we consider slotted ALOHA and CSMA/CA without RTS/CTS exchange. 
During the medium access, time synchronization among the CH and the CMs may be needed. This could be achieved by additional GPS system~\cite{baek2013beacon} or introduced time synchronization algorithm~\cite{polonelli2019slotted}. However, since time synchronization technique is beyond the scope of this paper, this will not be discussed further.

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figure/song2.eps}
\caption{The timing diagram for DeepAAQM.}
\label{fig:timing}
\end{figure*}

A specific system model, especially the timing diagram is elaborated as follows, assuming one cluster to make it easy to illustrate. Fig.~\ref{fig:timing} shows a timing diagram for DeepAAQM. In this situation, a sink node periodically disseminates a beacon frame that may contain various fields\footnote{The name of the beacon frame and the fields contained in the beacon frame may differ depending on the communication protocol, so general terms are used in this paper and are not limited to specific names.}. 
During one beacon interval, which is one \textit{episode}, CMs compete with each other to acquire wireless channel to forward monitored data that might be temperature, velocity, or location depending on their applications.
Once a CM gets a channel access opportunity, it transmits a frame containing the monitored data and the timestamp that data was observed as well.
A \textit{decision epoch} is defined as a frame duration or slightly longer than the duration for frame processing time\footnote{It is assumed that the data rates of the frames from CMs are constant in this paper.}. At each decision epoch, an agent in a CH decides an \textit{action} among action sets including forward, flush, and leave. 
When the beacon interval ends, the sink node will broadcast the beacon frame including a calculated \textit{return}, that is the accumulated sum of the \textit{rewards}, through DeepAAQM. Once the CH receives the beacon frame, the policy network learns using the return values of one episode included in the beacon frame the CH receives.


\section{DRL-based Active Queue Management}
\label{sec:deeplesson}

The MDP model is an appropriate mathematical decision-making framework. In this section, we present an MDP model for queue control operation and derive the optimal policy. In addition, one sink node, one CH, and many CMs associated with the CH are assumed hereafter. 

\subsection{State Space}
\label{subsec:state}

We define the state space of a finite set $\mathbf{S}$ as follows:
\begin{equation}
    \mathbf{S} = \mathbf{C}
    \times \mathbf{T}
    \times \mathbf{\Delta}^{AoI}
    \times \mathbf{\Delta}^{TS}
    \times \mathbf{L},
\label{eq:S}
\end{equation}
where $\mathbf{C}$ is the set of channel conditions, $\mathbf{T}$ is the current time normalized by beacon interval duration, $\mathbf{\Delta}^{AoI}$ and $\mathbf{\Delta}^{TS}$ are arrays of the sets of normalized current AoI value and timestamp at the transmission moment for each CM normalized by beacon interval duration, and $\mathbf{L}$ is a relative location from the head of the queue normalized by queue length.

In this paper, we utilize a classical two-state Gilbert-Elliot model to represent wireless channel~\cite{Zhang1999,Boujemaa2005}. There are two states; a good state and a bad state. Based on Rayleigh fading model, we consider the present state to be a good state when signal-to-noise ratio (SNR) is larger than the threshold value. Otherwise, the channel is said to be in a bad state. With the assumption mentioned above, $\mathbf{C}$, the set of channel conditions is represented by
\begin{equation}
    \mathbf{C} = \{0, 1\},
\label{eq:C}
\end{equation}
where $c(\in \mathbf{C})=0$ represents a good state and $c=1$ be a bad state. 

The current time space normalized by beacon interval $\mathbf{T}$ can be represented as follows:
\begin{equation}
    \mathbf{T} = \{t \mid 0 \leq t \leq 1 \}.
\label{eq:t}
\end{equation}

Assuming there are $N$ CMs associated with the CH, $\mathbf{\Delta}^{AoI}$ can be represented as follows:
\begin{equation}
    \mathbf{\Delta}^{AoI} = \prod_{i=1}^{N} \mathbf{\Delta}^{AoI}_{i},
\label{eq:delta1}
\end{equation}
where $\mathbf{\Delta}^{AoI}_{i}$ stands for the set of normalized current AoI value for CM $i$. Through the normalization process with respect to beacon duration, $\mathbf{\Delta}^{AoI}_{i}$ is within $[0, 1]$.

The remaining state spaces, $\mathbf{\Delta}^{TS}$ and $\mathbf{L}$ are defined through information on frames entered into the queue. Similar to the definition of $\mathbf{\Delta}^{AoI}$, $\mathbf{\Delta}^{TS}$ and $\mathbf{L}$ can be represented as follows:
\begin{equation}
    \mathbf{\Delta}^{TS} = \prod_{i=1}^{N} \mathbf{\Delta}^{TS}_{i},
\label{eq:delta2}
\end{equation}
and
\begin{equation}
    \mathbf{L} = \prod_{i=1}^{N} \mathbf{L}_i.
\label{eq:l}
\end{equation}
Here, $\mathbf{L}_i$ is the relative location for the frame transmitted by CM $i$ which is the closest to the head of queue, and $\mathbf{\Delta}^{TS}_{i}$ is the normalized timestamp of the corresponding frame. 

\subsection{Action Space}
\label{subsec:action}
Based on the state information in Section~\ref{subsec:state}, the CH chooses the action $a$ configured to aggregate and forward buffered frames, flush the queue, or leave the queue based on the MDP model. To do this, we define the action state as follows:
\begin{equation}
    \mathbf{A} = \{ 0, 1, 2\},
\label{eq:A}
\end{equation}
where $0$, $1$, and $2$ stand for each defined action, respectively. A specific procedure based on Fig.~\ref{fig:timing} will be stated as follows.
\begin{itemize}
    \item $a=0$ (Aggregate and forward frames): If this action is taken, one or more frames that are in head-of-line of the queue are aggregated and forwarded to the sink node and then removed\footnote{The aggregation of frames may vary depending on the applied transmission protocol, but typically, in WLAN standards after IEEE 802.11n, the aggregate MAC service data unit (A-MSDU) or aggregate MAC protocol data unit (A-MSDU) method can be used.}\footnote{Since we assume frame durations from CM to CH and from CH to sink node are 30 $\mu s$ and 270 $\mu s$, respectively. Hence the CH can aggregate up to $9$ stored frames in this paper.}. Since most of the time the CH may transmit frames, we assume that the CH has two or more transceivers to receive frames from the associated CMs while transmitting the aggregated frame to the sink node. The number of frames that can be aggregated is determined by the length of the frame duration transmitted by the CM and the length of the decision epoch.
    \item $a=1$ (Flush the queue): If the CM decides to flush out the queue, all frames stored in the queue are discarded. When there is little space left in the queue while the frames in the queue are `stale', that is to say, a lot of time has passed since the recorded timestamp, this action may be selected. 
    \item $a=2$ (Leave the queue): The CM may select leave, which means the CM just listens to the medium without forwarding or discarding a frame in the queue.
\end{itemize}



\subsection{State Transition Function}
We indicate two arbitrary states, the current state $s$ and the next state $s'$ in $\mathbf{S}$ as 
\begin{equation}
    s=\{c, t, \delta^{AoI}, \delta^{TS}, l \}
\end{equation}
    and 
\begin{equation}
    s'=\{c', t', \delta^{AoI}{}', \delta^{TS}{}', l' \},
\end{equation}
    where $\delta^{AoI}$, $\delta^{AoI}{}'$, $\delta^{TS}$, $\delta^{TS}{}'$, $l$ and $l'$ are $N$ length-size vectors, and an arbitrary action in $\mathbf{A}$ as $a \in \{0,1,2\}$. The components of the state, that are $c, t, \delta^{AoI}, \delta^{TS}$ and $l$, belong to $\mathbf{C}, \mathbf{T}, \mathbf{\Delta}^{AoI}, \mathbf{\Delta}^{TS}$ and $\mathbf{L}$, as stated in~\eqref{eq:S}, respectively.


The state transition function is the probability of reaching the state $s'$ from the state $s$ when the system does the action $a$. For ease of understanding, each entry consisting of state $s$ is explained in detail.

The next channel state $c'$ only depends on its current channel state $c$ and the next time $t'$ also does. Meanwhile, since the other components consist of the number of CMs. The next normalized current AoI value $\delta^{AoI}_{i}{}'$ depends on the current AoI values $\delta^{AoI}_{i}$, the timestamp when the frame sent by CM $i$ was created $\delta^{TS}_{i}$, the channel quality $c$, current time $t$, as well as the performed action $a$. On the other hand, the timestamp of CM $i$'s frame $\delta^{TS}_{i}{}'$. Next frame location of CM $i$, $l_i'$ will be change according to its current location $l_i$ and the action, such as $a=0$ or $a=1$.

To sum up, the state transition function can be expressed as follows:
\begin{equation}
    \begin{split}
        \Pr \left[ {s'\mid s,a} \right] & = {\Pr \left[ {c'\mid c} \right]} \\
        & \times \Pr \left[ {t'\mid t} \right] \\
        & \times \prod_{i=1}^{N} \Pr \left[ {\delta^{AoI}_{i}{}'\mid c, t, {\delta^{TS}_{i}}, l_i, a} \right] \\
        & \times \prod_{i=1}^{N} \Pr \left[ {\delta^{TS}_{i}{}'\mid {\delta^{TS}_{i}}, a} \right] \\
        & \times \prod_{i=1}^{N} \Pr \left[ {l_i'\mid {l_i}, a} \right].
    \end{split}
\label{eq:transition1}
\end{equation}

Looking at each element in~(\ref{eq:transition1}) one by one, we can define state transition probability regarding the channel state as shown in~\cite{Boujemaa2005} as follows:

\begin{equation}
\Pr \left[ {c' \mid c } \right] = \left\{ {
    \begin{array}{*{20}{l}}
        {\displaystyle \frac{f_D T_p \sqrt{2\pi\rho}}{e^\rho-1},} & {\textrm{if} \  c'=1, \ c=0 }, \\
        {\displaystyle 1-{\Pr \left[ {c'=1\mid c=0} \right]},} & {\textrm{if} \  c'=0, \ c=0}, \\
        {\displaystyle f_D T_p \sqrt{2\pi\rho},} & {\textrm{if} \  c'=0,\ c=1 }, \\
        {\displaystyle 1-{\Pr \left[ {c'=0\mid c=1} \right]},} & {\textrm{if} \  c'=1,\ c=1 }, \\ 
    \end{array}} \right.
\label{eq:tran_c}
\end{equation}
where $c=0$ and $c=1$ represent a good state and a bad state, respectively, as mentioned in~\eqref{eq:C}, $T_p$ is the state transition duration, which is set to 300 $\mu s$ and is the same as the duration of decision epoch, $\rho$ is the threshold level normalized to the average SNR that is represented as $\Gamma/\Bar{\gamma}$, $f_D$ is Doppler frequency, defined as $(v / v_c) f_0$, where $v$ is the mobility speed, $v_c$ is the speed of light, and $f_0$ is the center of the frequency band.

As time elapses, regardless of the other circumstances, we can define state transition probability for time $t$ as follows:
\begin{equation}
    t' = t + \frac{T_p}{T_{b}}.
\label{eq:tran_t}
\end{equation}
where the normalized current times $t, t'$ belong to $[0, 1]$ and $T_{b}$ is the beacon duration, which is the length of one episode and is set to 100 $ms$. When $t'$ becomes 1, an episode ends and the next episode begins.

Next, we define current AoI value state transition probability. Basically, current AoI value of CM $i$ increases proportionally to the current time unless the frame transmitted by the CM $i$ is successfully forwarded to the sink node. Hence, $\delta^{AoI}_{i}$ is stated as
\begin{equation}
    \delta^{AoI}_{i}{}' = \left\{ {
    \begin{array}{*{20}{l}}
        {\displaystyle t-\delta^{TS}_{i}+\frac{T_p}{T_{b}},} & {\displaystyle \textrm{if} \ c=0, \ l_i \leq \frac{t_s}{t_l}, \ a=0}, \\ 
        {\displaystyle \delta^{AoI}_{i} + \frac{T_p}{T_{b}} ,} & {\textrm{otherwise}}.
    \end{array}} \right.
\label{eq:tran_curr}
\end{equation}
where $t_s$ and $t_l$ are durations for frame from CM to CH, and from CH to sink node, respectively. In other words, current AoI value of the CM $i$ is updated if buffered frame transmitted by the CM $i$ in the CM's queue is successfully forwarded to the sink node.


\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figure/song3.eps}
\caption{$\delta^{TS}_{i}$ and $l_{i}$ during two adjacent epochs.}
\label{fig:delta}
\end{figure}

When it comes to the information on frames in the queue, there are two sub-states; $\delta^{TS}_{i}$ and ${l_i}$. A CH maintains a table containing CM's ID and corresponding CM's normalized timestamp value, $\delta^{TS}_{i}$ and its relative location, $l_i$ and it is updated every decision epoch. $\delta^{TS}_{i}$ is the value recorded when the frame at the head of the queue is created. If there isn't any frame from CM $i$, $\delta^{TS}_{i}$ equals to 1. In a similar way, $l_i$ lies within $[0, 1)$ in the direction from head to tail of the queue. Otherwise, if there isn't any frame from CM $i$, $l_i$ is equal to 1.

For ease of understanding, we briefly illustrate an example trend of $\delta^{TS}_{i}$ and $l_i$ in Fig.~\ref{fig:delta} showing two adjacent decision epochs. The values of $\delta^{TS}_{i}$ and $l_i$ are only relevant for the frame transmitted by CM $i$ which is the closest to the head of the queue. Accordingly, $\delta_{TS, 1}$ is 0.15 and $l_1$ is 0, which means the frame from CM 0 is located at the very head of the queue. When two frames are aggregated and forwarded to the sink node, since the frame from CM 0 is still positioned at the beginning, $l_1'$ is 0. However, the value of $\delta_{TS, 1}'$ would change according to the timestamp of the new frame. In addition, $\delta_{TS, 4}$, $l_4$, $\delta_{TS, 3}'$, and $l_3'$ are all 1 because the corresponding frame does not exist.


\subsection{Reward Function}
\label{subsec:reward}
To define the reward function, we consider the peak AoI values among CMs and their energy consumption. Since we are assuming fixed decision epoch duration, we replace the consumed energy with consumed power hereafter.

In particular, the reward when action $a$ is selected on state $s$ is as follows:
\begin{equation}
R(s, a) = c_{pow} \cdot R^{pow}(s, a) + c_{aoi} \cdot R^{aoi}(s, a),
\label{eq:reward}
\end{equation}
where $R^{pow}(s, a)$ and $R^{aoi}(s, a)$ are power consumed by CH, and the maximum peak AoI value among CMs, respectively, upon the state $s$ and the action $a$, and $c_{pow}$ and $c_{aoi}$ stands for balance parameters between energy consumption and the maximum peak AoI values.

Subsequently, the power consumed by CM is represented as
\begin{equation}
\begin{split}
    R^{pow}(s, a) = \left\{ {
    \begin{array}{*{20}{l}}
        {-P_{tx},} & {\textrm{if} \quad a = 0,} \\ 
        0 & {\textrm{otherwise}. } \\ 
    \end{array}} \right.
\label{eq:power}
\end{split}
\end{equation}
In~\eqref{eq:power}, $P_{tx}$ can be any parameter, however in this paper, we set $P_{tx}$ as 0.280, according to common power model parameters introduced for IEEE 802.11ax~\cite{14/0980}. To evaluate the value of the consumed power, we omit the unit, use the number itself, and normalize it by using parameter $c_{pow}$.

Next, the maximum peak AoI value is represented as 
\begin{equation}
    R^{aoi}(s, a) = -\sum_{\forall i} \left(\delta_i^{TS}-\frac{T_{target}}{T_b} \right)^{+},
\label{eq:peakaoi}
\end{equation}
where $t_k$ is the time of $k$-th decision epoch and $x^{+}$ stands for the ramp function which is defined as follows:
\begin{equation}
\begin{split}
    x^{+} = \left\{ {
    \begin{array}{*{20}{l}}
        {x,} & {\textrm{if} \quad x \geq 0,} \\ 
        {0,} & {\textrm{otherwise}. } \\ 
    \end{array}} \right.
\label{eq:ramp}
\end{split}
\end{equation}
A specific peak AoI value may be needed depending on the requirements of different applications. With the equation~\eqref{eq:reward}, DeepAAQM tries to satisfy these AoI service requirements while minimizing consumed power.


\section{Deep reinforcement learning algorithm}
\label{sec:drl}

\begin{figure*}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{figure/song4.eps}
\caption{DeepAAQM architecture with a learning agent and network components.}
\label{fig:architecture}
\end{figure*}


DRL has demonstrated the ability to learn and make optimal decisions in dynamic and uncertain network environments through trial and error, particularly when large state spaces complicate optimization problems. Therefore, we propose using the DRL algorithm to solve the MDP model shown in Sec.~\ref{sec:deeplesson}.

We adopt a DRL model based on the DQN algorithm with experience replay\cite{lin1992reinforcement}. When it comes to AoI values within a beacon interval, collected transitions may be correlated and have non-stationary distributions. The experience replay mechanism randomly selects previous transitions, resulting in a more balanced training distribution that accounts for a variety of past behaviors. This section, therefore, describes how DRL applies to action decision problems for the CH.

We depict the DeepAAQM architecture accompanied with network components in Fig.~\ref{fig:architecture}. Once a CM gets an opportunity to access the wireless medium, it transmits a frame including a timestamp. The agent then queues the received frame into CH's queue by first-in first-out policy. The agent also manages a table, named AoI table, to record received frames' transmitter IDs and recorded AoI. One room of the queue and one row of the AoI table have one-to-one mapping. The agent keeps track of the queue and AoI table and reorganizes them in accordance with the indexes of CMs. They can then be regarded as entries of a state, $\delta^{AoI}_{i}(t)$ and $\delta^{TS}_{i}(t)$, and the state comprises them across CMs. When it comes to the neural network, the input layer consists of $2N+1$ nodes, as defined in~\eqref{eq:S}. We adopted two hidden layers with $N+2$ nodes each, including activation functions of ReLU. At the end of the output layer, the action can be determined through the softmax function. The structure of the policy network has been empirically optimized through several simulations. Once the policy is calculated by the policy network, the agent, i.e., the CH, determines the action for the frame of the head-of-line. Next, a reward can be calculated by means of the chosen action and it is used to update the policy network. The overall algorithm proceeds with the revised neural network parameters in sequence. Algorithm~\ref{alg:alg1} demonstrates how to utilize the DRL algorithm to solve the CH queue management problem.

\begin{algorithm}[ht!]
\caption{Training phase of DeepAAQM using DQN with experience replay}
\begin{algorithmic}[1]
    \State Initialize prediction network $Q$ with parameter $\theta$
    \State Initialize target network $\hat{Q}$ with parameter $\hat{\theta}$
    \State Initialize replay memory $D$
    \For {each episodes}
        \State Initialize the state of an agent
        \While{episode is \textbf{not} done}
            \State With probability $\epsilon$, select a random action $a$. Otherwise, select $a$ that maximizes $Q(s, a)$
            \State Agent takes an action $a$
            \State Agent observes reward $r$ and next state $s'$
            \State Store transition $(s, a, r, s')$ in the memory $D$
            \If{$D$ is full}
                \State Sample $N$ transitions from $D$
                \For{$n$th transition in the samples}
                    \State $\displaystyle y_i = r_i + \gamma \max_{a' \in A} \hat{Q}(s_i',a')$
                \EndFor
                \State Calculate loss $\displaystyle \mathcal{L}=\frac{1}{N} \sum_{i=1}^{N}\left(y_i - Q(s_i,a_i)\right)^2$
                \State Update $\theta$ in $Q$ by minimizing the loss $\mathcal{L}$ with stochastic gradient descent
                \State Soft parameter update, $\hat{\theta} = \tau \theta + (1-\tau) \hat{\theta}$
            \EndIf
        \EndWhile
        \State \textbf{return} $\theta$
    \EndFor
\end{algorithmic}
\label{alg:alg1}
\end{algorithm}


\section{Performance Analysis Results}
\label{sec:performance}

For performance analysis, we conduct extensive simulations with Python 3.10 along with gymnasium~\cite{gym} which is a standard API for reinforcement learning, NumPy~\cite{numpy} which is a Python package for scientific computing, and PyTorch~\cite{torch} which is an open source machine learning framework. 

We compared DeepAAQM with conventional active queue management methods, which are 1) stabilized random early drop (SRED)~\cite{Ott1999} and
2) controlled delay (CoDel)~\cite{Nichols2012}.
Specifically, SRED algorithm pre-emptively discards frames with a load-dependent probability, as introduced in~\cite{Ott1999}. Frame drop probability $p_{sred}(q)$ is set as follows:
\begin{equation}
    p_{sred}(q) = \left\{ {
    \begin{array}{*{20}{l}}
        \displaystyle{0}, & {\textrm{if} \quad 0 \leq q < \frac{1}{6},} \\
        \displaystyle{\frac{p_{max}}{4}}, & {\textrm{if} \quad \frac{1}{6} \leq q < \frac{1}{3},} \\
        \displaystyle{p_{max}}, & {\textrm{if} \quad \frac{1}{3} \leq q < 1,} \\
    \end{array}} \right.
\label{eq:sred}
\end{equation}
where $q$ is the ratio of occupied queue amount to maximum queue amount and $p_{max}$ is set to 0.15. 

In CoDel, we set parameters in conformity with \url{https://queue.acm.org/appendices/codel.html}, the pseudocode in~\cite{Nichols2012}, and with RFC8289~\cite{rfc8289}. CoDel uses two key variables: \textit{target} and \textit{interval} and they are set to 100 ms and 5 ms, respectively. To explain in detail, CoDel measures packet delay each \textit{interval}. If delay is below \textit{target}, no packets are dropped. If delay exceeds the target, however, CoDel drops a packet and adjusts \textit{interval} based on the inverse square root of consecutive drop-mode intervals, with a typical sequence being $100$, $100/\sqrt{2}$, $100/\sqrt{3}$. When delay falls below \textit{target}, CoDel exits drop mode, stops dropping packets, and resets the \textit{interval}.


In this analysis, we cover two well-used wireless medium access protocols; slotted ALOHA and CSMA/CA. For slotted ALOHA, we set the medium access probability as $1/N$, which is known to be an optimal access probability. For CSMA/CA, we use a conventional IEEE 802.11 distributed coordination function (DCF) that employs CSMA/CA with a binary exponential backoff algorithm. Hyperparameters for DRL and network/device parameters are listed in Table~\ref{tab:parameter}. Underlined values in the table are used in the default option when unmentioned in simulations.

\begin{table}
\caption{Summary of notations}
\label{table}
\setlength{\tabcolsep}{3pt}
\centering
\begin{tabular}{c|c}
\hline
Description & Value \\
\hline
\hline
\multicolumn{2}{c}{Hyperparameters} \\
\hline
Number of hidden layers & 2 \\
Optimizer & Adam~\cite{adam} \\ 
Learning rate, $\alpha$ & 0.001 \\
Loss function & Smooth L1 loss~\cite{girshick2015fast} \\
Threshold for $\epsilon$-greedy & 0.1 \\
Activation function & ReLU \\
Discount rate, $\gamma$ & 1 \\
Replay memory batch size, $N$ & 128 \\
Soft parameter update weight, $\tau$ & 0.005 \\
\hline
\hline
\multicolumn{2}{c}{Network and device parameters} \\
\hline
Beacon interval, $T_{b}$ & 100 msec \\
Decision epoch, $T_{p}$ & 300 $\mu$sec \\
Frame duration from CM to CH, $t_s$ & 30 $\mu$sec \\
Frame duration from CH to sink node, $t_l$ & 270 $\mu$sec \\
Simulation duration & 10 sec (100 episodes) \\
Number of CMs & 5, 10, 20 \\
CMs' velocity, $v$ & 10 km/sec \\
Target AoI value, $T_{target}$ & 20 msec \\
Queue size in CH & 20 \\
Aggregation limit, $t_l/t_s$ & 9 \\
Medium access protocol & Slotted ALOHA, CSMA/CA \\
\hline
\end{tabular}
\label{tab:parameter}
\end{table}

\subsection{Convergency examination of DeepAAQM}
\label{subsec:conv}

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figure/song5.eps}
\caption{Trends of DeepAAQM’s moving averages of the returns for various numbers of nodes and access algorithms. The values are unweighted running averages of the previous 100 episodes.}
\label{fig:return}
\end{figure}

Before analyzing the simulation results, we first want to ensure that the DeepAAQM algorithm is stable by examining whether the return values converge as the episode progresses. Fig.~\ref{fig:return} shows the trend of the algorithm's return values in various environments. Although there are some differences, it can be observed that the return values generally converge from approximately the 200th episode. Also, considering that the return value converges reliably, it is believed that there is no catastrophic forgetting~\cite{mccloskey1989catastrophic} on DeepAAQM both for slotted ALOHA and CSMA/CA.

On the other hand, as the number of CMs increases, competition for occupying shared wireless medium intensifies. This increases the probability of collisions and thus lowers the probability of a certain CM occupying the channel. Of course, as the number of CMs increases, the probability of individual CMs accessing the media also decreases. Therefore, the return value when the number of CMs is 20 can be expected to be lower than that when the number of CMs is 5 or 10, because the CMs are likely to have large AoI values, and this can be seen in the figure. Additionally, due to the collision avoidance feature of CSMA/CA, we can observe that the return value of CSMA/CA is much higher than that of Slotted ALOHA, especially when the number of CMs is large.



\subsection{AoI comparison}
\label{subsec:aoicomp}

\begin{figure}
    \centering
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/song6-1.eps}
        \caption{}
        \label{fig:cdfaoi1}
        % (a)
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/song6-2.eps}
        \caption{}
        \label{fig:cdfaoi2}
        % (b)
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/song6-3.eps}
        \caption{}
        \label{fig:cdfaoi3}
        % (c)
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/song6-4.eps}
        \caption{}
        \label{fig:cdfaoi4}
        % (d)
    \end{subfigure}
    \caption{Cumulative distribution function of AoI values for DeepAAQM and the comparison algorithms in (a) slotted ALOHA with 5 CMs (b) slotted ALOHA with 10 CMs (c) CSMA/CA with 5 CMs (d) CSMA/CA with 10 CMs.}
    \label{fig:aoicomp}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/song7-1.eps}
        \caption{}
        \label{fig:sawaoi1}
        % (a)
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/song7-2.eps}
        \caption{}
        \label{fig:sawaoi2}
        % (b)
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/song7-3.eps}
        \caption{}
        \label{fig:sawaoi3}
        % (c)
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/song7-4.eps}
        \caption{}
        \label{fig:sawaoi4}
        % (d)
    \end{subfigure}
    \caption{Trends of AoI values of the identical CM for DeepAAQM and comparison algorithms in (a) slotted ALOHA with 5 CMs (b) slotted ALOHA with 10 CMs (c) CSMA with 5 CMs (d) CSMA with 10 CMs }
    \label{fig:sawaoi}
\end{figure}

In this subsection, the analysis of AoI values over DeepAAQM and the comparison algorithms through Figs.~\ref{fig:aoicomp} and~\ref{fig:sawaoi}. First, we show CDFs of collected AoI values over 100 individual episodes for each management algorithm in Fig.~\ref{fig:aoicomp}. The experiment is conducted under the numbers of CMs of 5 and 10. $T_{target}$ is 20~ms and is marked with a red dotted vertical line in the figure.

Through Figs.~\ref{fig:aoicomp}(a) to~\ref{fig:aoicomp}(d), it can be found that DeepAAQM outperforms compared algorithms in terms of minimizing AoI value. Let $p_{out}$ be the ratio of AoI values exceeding $T_{target}$, it is shown that DeepAAQM reduces $p_{out}$ by 83.7\%, 49.0\%, 39.9\%, and 33.0\%, compared to other algorithms in slotted ALOHA with 5 CMs, slotted ALOHA with 10 CMs, CSMA/CA with 5 CMs, and CSMA/CA with 10 CMs, respectively. In addition, the AoI values of CoDel and SRED are almost similar. From these results, we can observe that the reward function using $T_{target}$ was appropriately designed and that the queue management mechanism is working well as intended by this reward function. We can also see that DeepAAQM shows greater performance improvement when using slotted ALOHA than CSMA/CA, and when using a small number of CMs rather than a large number of CMs. 
What is noteworthy in these figures is that in the situation of CSMA/CA with 10 CMs in Fig.~\ref{fig:cdfaoi2}, the performances of all cases decrease significantly, although DeepAAQM still significantly outperforms the comparisons. Due to the nature of the binary exponential backoff used in CSMA/CA, some CMs will likely have very large contention windows, and there is a possibility that some of these devices will rapidly increase the AoI value. The use of DeepAAQM in this situation can speed up frame transmission from these devices with large backoff values through quick resolution of the head of the queue.

Through Figs.~\ref{fig:sawaoi}(a) to~\ref{fig:sawaoi}(d), we observe trends of the AoI values of the queue management algorithms for the same CM in the same episode. $T_{target}$ is depicted as a red dotted horizontal line. Intuitively, the trends of 10 CMs have larger peak AoI values than those of 5 CMs. Additionally, in Slotted ALOHA, each CM has the same channel access probability, so the peak AoI value is small, and the sawtooth graph is expressed in a finer form compared to CSMA/CA, accordingly.

\subsection{Consumed power comparison}
\label{subsec:power}


\begin{figure}
    \centering
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/song8-1.eps}
        \caption{}
        \label{fig:power1}
        % (a)
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/song8-2.eps}
        \caption{}
        \label{fig:power2}
        % (b)
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/song8-3.eps}
        \caption{}
        \label{fig:power3}
        % (c)
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/song8-4.eps}
        \caption{}
        \label{fig:power4}
        % (d)
    \end{subfigure}
    \caption{Cumulative distribution function of AoIs for DeepAAQM and comparison algorithms in (a) slotted ALOHA with 5 CMs (b) slotted ALOHA with 10 CMs (c) CSMA with 5 CMs (d) CSMA with 10 CMs }
    \label{fig:power}
\end{figure}

From Figs.~\ref{fig:power}(a) to~\ref{fig:power}(d), power consumptions of DeepAAQM and the comparative algorithms are presented. The consumed power values of each episode are averaged over one episode. Throughout the figures, SRED appears to consistently consume a lot of energy while others do not. The reason for this result can be analyzed as follows. SRED adaptively sets the frame drop probability based on the ratio of occupied frames in a queue, as seen in~\eqref{eq:sred}. However, media access algorithms such as slotted ALOHA and CSMA/CA preemptively perform the functionality of the queue management themselves. That is, as the number of nodes increases, a collision occurs and the channel becomes idle in slotted ALOHA, and a collision avoidance function through binary exponential backoff operates in CSMA/CA. Therefore, in this simulation environment, the ratio of occupied frames in the queue does not increase in slotted ALOHA or CSMA/CA. In environments with very small queue lengths, the frame drop probability of SRED is expected to vary.

What is notable is that DeepAAQM consumes more power than CoDel, as seen in Fig.~\ref{fig:power2}. When a large number of CMs compete for channel access through CSMA/CA, the queue is likely to be empty due to extreme contention. So in the CoDel algorithm, it is more likely to be in idle. On the other hand, DeepAAQM attempts to empty the queue as much as possible even when there are a small amount of packets in the queue in order to maximize the reward, as seen in~\eqref{eq:peakaoi}, so it is observed that power consumption increased. This can also be supported by the CDF figure of AoI in Fig.~\ref{fig:sawaoi2}.


\subsection{Summary}
\label{subsec:summary}

\begin{table*}
\caption{Representative performance values for 100 test episodes}
\centering
\begin{tabular}{c|ccc|ccc}
\hline
\hline
\multirow{2}{*}{No of CMs = 5} & \multicolumn{3}{c|}{slotted ALOHA} & 
\multicolumn{3}{c}{CSMA/CA} \\
                            & DeepAAQM     & CoDel    & SRED    & DeepAAQM   & CoDel  & SRED  \\
\hline
Peak AoI (ms)               & 42.12        & 63.81    & 59.94   & 90.67      & 90.67  & 92.82 \\
Average mean AoI (ms)       &  6.49        &  9.72    &  9.51   &  5.31      &  6.42  &  6.40 \\
Average consumed power (mW) & 86.20        & 109.63   & 276.93  & 45.56      & 213.06 & 276.92\\
\hline
\hline
\multirow{2}{*}{No of CMs = 10} & \multicolumn{3}{c|}{slotted ALOHA} & \multicolumn{3}{c}{CSMA/CA} \\
                            & DeepAAQM     & CoDel    & SRED    & DeepAAQM   & CoDel  & SRED  \\
\hline
Peak AoI (ms)               & 70.23        & 99.90    & 99.90   & 99.90      & 99.90  & 99.90 \\
Average mean AoI (ms)       & 12.23        & 18.72    & 18.34   & 10.69      & 13.67  & 13.68 \\
Average consumed power (mW) & 124.88       & 102.79   & 276.92  & 95.35      & 207.35 & 276.92\\
\hline
\hline
\end{tabular}
\label{tab:results}
\end{table*}

In Table~\ref{tab:results}, we show peak AoI, average AoI, and average consumed power calculated from all CMs after 1,000 episodes of training. In this scenario, it is shown that DeepAAQM can reduce peak AoI and average mean AoI up to 33.99\% and 34.67\%, respectively. The average power consumption can be reduced by up to 83.55\% compared to SRED and 78.62\% compared to CoDel, and in some cases, about 21.49\% more power is consumed to reduce the AoI value. According to the results, it was shown that DeepAAQM can satisfy AoI values. DeepAAQM is particularly suitable for services where guaranteeing AoI bounds is crucial, such as disaster emergency communication systems, especially when they have energy limitations.


\section{Conclusion}
\label{sec:conclusion}
In this paper, we proposed DeepAAQM, an active queue management algorithm for CH that adaptively selects an action for a head-of-line frame in IoT sensor networks that requires target AoI value and low power consumption. 
To find out the optimal policy to maximize the reward consisting of the power consumption and the peak AoI value, we formulated a Markov decision process that considers the current channel condition and the current AoI value. Performance analysis demonstrated that DeepAAQM could outperform other comparative algorithms, specifically, maintaining the target AoI value with low power consumption. We then investigate the convergency of DeepAAQM, the trend of AoI, and consumed power. It was shown that DeepAAQM can be adopted in various conditions through intensive simulations. 


\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\bibliographystyle{IEEEtran}
\bibliography{deepaaqm}

% \begin{thebibliography}{1}
% % You can use other form of bib file by changing here... 

% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

% \end{thebibliography}



% \begin{IEEEbiography}
% [{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{taewon.jpg}}]{Taewon Song} received the B.S. and Ph.D. degrees in electrical engineering from Korea University, Seoul, South Korea, in 2010 and 2017, respectively. In 2020, he joined Soonchunhyang University, where he is currently a Research Assistant Professor at the SCH Convergence Science Institute. From 2017 to 2020, he was a Senior Researcher at the Advanced Standard R\&D Laboratory, LG Electronics, where he worked on standardization for wake-up radio and next-generation WLANs. He was a Visiting Student supported by the BK21+ project with the University of Florida from February 2015 to April 2015. From January 2012 to February 2012, he was a Visiting Researcher supported by the National Research Foundation of Korea at the National Institute of Information and Communications Technology. His current research interests include AI-empowered networking, low-power wake-up radio, and the wireless medium access control protocol of next-generation WLANs.
% \end{IEEEbiography}
% \begin{IEEEbiography}
% [{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./Yeunwoong_Kyung}}]{Yeunwoong Kyung} received the B.S. and Ph.D. degrees from Korea University, Seoul, Korea, in 2011 and 2016, respectively, both in School of Electrical Engineering. He was a staff engineer at advanced CP lab., Mobile Communications Business, Samsung Electronics. He is currently an Assistant Professor with the School of Computer Engineering, Hanshin University, Osan, South Korea. His current research interests include mobility management, mobile cloud computing, SDN/NFV, and IoT.
% \end{IEEEbiography}

% !!Update needed!!

% %It is not necessary to upload the biography when you submit your manuscript.


\end{document}



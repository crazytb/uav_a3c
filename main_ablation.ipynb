{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UAV A3C Model Ablation Analysis (Main Models Only)\n",
    "\n",
    "This notebook provides comprehensive ablation analysis for UAV A3C models from the main models directory including:\n",
    "1. Global vs Individual model comparison\n",
    "2. Worker variability analysis  \n",
    "3. Model structure and parameter analysis\n",
    "4. Weight similarity analysis\n",
    "\n",
    "**Note: This analysis focuses only on models in the main `models/` directory, excluding the `runs/` folder for faster processing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "PyTorch version: 2.2.1+cu121\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# Custom imports\n",
    "from drl_framework.networks import ActorCritic, RecurrentActorCritic\n",
    "from drl_framework.custom_env import CustomEnv\n",
    "from drl_framework.params import ENV_PARAMS, REWARD_PARAMS, device  # 🔧 Added REWARD_PARAMS\n",
    "from drl_framework.utils import flatten_dict_values\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded: {'main_models_dir': 'models', 'runs_dir': 'runs', 'target_timestamp': '20250815_031631', 'state_dim': 48, 'action_dim': 3, 'hidden_dim': 128, 'model_type': 'recurrent', 'n_workers': 5, 'evaluation_episodes': 100}\n",
      "Target models: 20250815_031631\n",
      "Architecture: RNN with state_dim=48, action_dim=3\n",
      "✅ Compatible with 3-action environment (LOCAL, OFFLOAD, DISCARD)\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    def __init__(self, target_timestamp=None):\n",
    "        self.main_models_dir = \"models\"\n",
    "        self.runs_dir = \"runs\"\n",
    "        # Set target timestamp for analysis (use latest compatible models)\n",
    "        self.target_timestamp = target_timestamp or \"20250815_031631\"\n",
    "        \n",
    "        # Model architecture parameters (updated for 20250923_143004 models)\n",
    "        self.state_dim = 48  # Updated for latest models\n",
    "        self.action_dim = 3  # Updated to support all 3 actions: LOCAL, OFFLOAD, DISCARD\n",
    "        self.hidden_dim = 128\n",
    "        self.model_type = 'recurrent'  # 20250923 models are RNN-based\n",
    "        \n",
    "        # Analysis parameters\n",
    "        self.n_workers = 5  # Number of individual workers in 20250923 runs\n",
    "        self.evaluation_episodes = 100\n",
    "        \n",
    "config = Config(\"20250815_031631\")\n",
    "print(f\"Configuration loaded: {vars(config)}\")\n",
    "print(f\"Target models: {config.target_timestamp}\")\n",
    "print(f\"Architecture: RNN with state_dim={config.state_dim}, action_dim={config.action_dim}\")\n",
    "print(\"✅ Compatible with 3-action environment (LOCAL, OFFLOAD, DISCARD)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Discovery and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Discover models from target timestamp\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m models_info \u001b[38;5;241m=\u001b[39m \u001b[43mdiscover_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_timestamp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Available Models from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mtarget_timestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGlobal models: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(models_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_models\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[34], line 15\u001b[0m, in \u001b[0;36mdiscover_models\u001b[0;34m(target_timestamp)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a3c_dir\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     14\u001b[0m     global_models_dir \u001b[38;5;241m=\u001b[39m a3c_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mglobal_models_dir\u001b[49m\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m pth_file \u001b[38;5;129;01min\u001b[39;00m global_models_dir\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     17\u001b[0m             models_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_models\u001b[39m\u001b[38;5;124m'\u001b[39m][pth_file\u001b[38;5;241m.\u001b[39mstem] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(pth_file)\n",
      "Cell \u001b[0;32mIn[34], line 15\u001b[0m, in \u001b[0;36mdiscover_models\u001b[0;34m(target_timestamp)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a3c_dir\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     14\u001b[0m     global_models_dir \u001b[38;5;241m=\u001b[39m a3c_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mglobal_models_dir\u001b[49m\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m pth_file \u001b[38;5;129;01min\u001b[39;00m global_models_dir\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     17\u001b[0m             models_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_models\u001b[39m\u001b[38;5;124m'\u001b[39m][pth_file\u001b[38;5;241m.\u001b[39mstem] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(pth_file)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1395\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1344\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-cert/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-cert/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def discover_models(target_timestamp=None):\n",
    "    \"\"\"Discover models from specific timestamp runs\"\"\"\n",
    "    if target_timestamp is None:\n",
    "        target_timestamp = config.target_timestamp\n",
    "    \n",
    "    models_info = {\n",
    "        'global_models': {},\n",
    "        'individual_models': {}\n",
    "    }\n",
    "    \n",
    "    # Look for A3C global models with target timestamp\n",
    "    a3c_dir = Path(config.runs_dir) / f\"a3c_{target_timestamp}\"\n",
    "    if a3c_dir.exists():\n",
    "        global_models_dir = a3c_dir / \"models\"\n",
    "        if global_models_dir.exists():\n",
    "            for pth_file in global_models_dir.glob(\"*.pth\"):\n",
    "                models_info['global_models'][pth_file.stem] = str(pth_file)\n",
    "    \n",
    "    # Look for individual worker models with target timestamp\n",
    "    individual_dir = Path(config.runs_dir) / f\"individual_{target_timestamp}\"\n",
    "    if individual_dir.exists():\n",
    "        individual_models_dir = individual_dir / \"models\"\n",
    "        if individual_models_dir.exists():\n",
    "            for pth_file in individual_models_dir.glob(\"*.pth\"):\n",
    "                models_info['individual_models'][pth_file.stem] = str(pth_file)\n",
    "    \n",
    "    return models_info\n",
    "\n",
    "def load_model(model_path, model_type=None):\n",
    "    \"\"\"Load a model from path (supports both Standard and RNN models)\"\"\"\n",
    "    if model_type is None:\n",
    "        model_type = config.model_type\n",
    "        \n",
    "    try:\n",
    "        # Determine model architecture\n",
    "        if model_type == 'recurrent':\n",
    "            model = RecurrentActorCritic(\n",
    "                state_dim=config.state_dim,\n",
    "                action_dim=config.action_dim,\n",
    "                hidden_dim=config.hidden_dim\n",
    "            )\n",
    "        else:\n",
    "            model = ActorCritic(\n",
    "                state_dim=config.state_dim,\n",
    "                action_dim=config.action_dim,\n",
    "                hidden_dim=config.hidden_dim\n",
    "            )\n",
    "        \n",
    "        # Load state dict\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        \n",
    "        # Handle different checkpoint formats\n",
    "        if isinstance(checkpoint, dict):\n",
    "            if 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            elif 'state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['state_dict'])\n",
    "            else:\n",
    "                model.load_state_dict(checkpoint)\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "        \n",
    "        model.eval()\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Discover models from target timestamp\n",
    "models_info = discover_models(config.target_timestamp)\n",
    "print(f\"\\n=== Available Models from {config.target_timestamp} ===\")\n",
    "print(f\"Global models: {list(models_info['global_models'].keys())}\")\n",
    "print(f\"Individual models: {list(models_info['individual_models'].keys())}\")\n",
    "print(f\"Total models found: {len(models_info['global_models']) + len(models_info['individual_models'])}\")\n",
    "\n",
    "# Show model paths\n",
    "if models_info['global_models']:\n",
    "    print(f\"\\nGlobal model paths:\")\n",
    "    for name, path in models_info['global_models'].items():\n",
    "        print(f\"  {name}: {path}\")\n",
    "\n",
    "if models_info['individual_models']:\n",
    "    print(f\"\\nIndividual model paths (showing first 3):\")\n",
    "    for i, (name, path) in enumerate(list(models_info['individual_models'].items())[:3]):\n",
    "        print(f\"  {name}: {path}\")\n",
    "    if len(models_info['individual_models']) > 3:\n",
    "        print(f\"  ... and {len(models_info['individual_models']) - 3} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing environment reset...\n",
      "=== DEBUG: Observation Structure ===\n",
      "Type: <class 'dict'>\n",
      "  available_computation_units: <class 'float'> N/A = 1.0\n",
      "  remain_epochs: <class 'float'> N/A = 1.0\n",
      "  mec_comp_units: <class 'numpy.ndarray'> (20,) = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  mec_proc_times: <class 'numpy.ndarray'> (20,) = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  queue_comp_units: <class 'numpy.float64'> () = 0.37\n",
      "  queue_proc_times: <class 'numpy.float64'> () = 0.7\n",
      "  local_success: <class 'int'> N/A = 0\n",
      "  offload_success: <class 'int'> N/A = 0\n",
      "  ctx_vel: <class 'numpy.ndarray'> (1,) = [0.2857143]\n",
      "  ctx_comp: <class 'numpy.ndarray'> (1,) = [1.]\n",
      "Flattened observation shape: (48,)\n",
      "First few values: [1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\\nTesting environment step...\n",
      "Step successful - reward: -2.0, done: False\n",
      "\\nEnvironment initialized:\n",
      "  Base environment action space: 3 (LOCAL, OFFLOAD, DISCARD)\n",
      "  Models expect: state_dim=48, action_dim=3\n",
      "  Model type: RECURRENT\n",
      "  Reward params: ['ALPHA', 'BETA', 'GAMMA', 'REWARD_SCALE', 'FAILURE_PENALTY', 'ENERGY_COST_COEFF', 'CONGESTION_COST_COEFF']\n",
      "✅ Environment ready for evaluation!\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, env, episodes=100, deterministic=True):\n",
    "    \"\"\"Evaluate a model's performance (updated for 3 actions and RNN models)\"\"\"\n",
    "    total_rewards = []\n",
    "    episode_lengths = []\n",
    "    action_counts = {0: 0, 1: 0, 2: 0}  # LOCAL, OFFLOAD, DISCARD\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        done = False\n",
    "        \n",
    "        # Initialize hidden state for RNN models\n",
    "        if hasattr(model, 'init_hidden'):\n",
    "            hidden = model.init_hidden(1, device)\n",
    "        else:\n",
    "            hidden = None\n",
    "        \n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                if hidden is not None:\n",
    "                    # RNN model\n",
    "                    if hasattr(model, 'act'):\n",
    "                        action, _, _, hidden = model.act(state_tensor, hidden)\n",
    "                        action = action.item()\n",
    "                    else:\n",
    "                        logits, _, hidden = model.forward(state_tensor, hidden)\n",
    "                        if deterministic:\n",
    "                            action = torch.argmax(logits, dim=1).item()\n",
    "                        else:\n",
    "                            dist = torch.distributions.Categorical(logits=logits)\n",
    "                            action = dist.sample().item()\n",
    "                else:\n",
    "                    # Standard model\n",
    "                    logits, _ = model(state_tensor)\n",
    "                    if deterministic:\n",
    "                        action = torch.argmax(logits, dim=1).item()\n",
    "                    else:\n",
    "                        dist = torch.distributions.Categorical(logits=logits)\n",
    "                        action = dist.sample().item()\n",
    "            \n",
    "            # Ensure action is within valid range (0, 1, 2)\n",
    "            action = min(action, 2)\n",
    "            action_counts[action] = action_counts.get(action, 0) + 1\n",
    "            \n",
    "            state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "            \n",
    "            if episode_length > 1000:  # Prevent infinite loops\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "        episode_lengths.append(episode_length)\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': np.mean(total_rewards),\n",
    "        'std_reward': np.std(total_rewards),\n",
    "        'min_reward': np.min(total_rewards),\n",
    "        'max_reward': np.max(total_rewards),\n",
    "        'mean_length': np.mean(episode_lengths),\n",
    "        'std_length': np.std(episode_lengths),\n",
    "        'action_distribution': action_counts,\n",
    "        'all_rewards': total_rewards,\n",
    "        'all_lengths': episode_lengths\n",
    "    }\n",
    "\n",
    "# Create environment wrapper for RNN models with 48-dimensional state\n",
    "class RNNCompatibleEnv:\n",
    "    def __init__(self, base_env):\n",
    "        self.env = base_env\n",
    "        self.observation_space = base_env.observation_space\n",
    "        self.action_space = base_env.action_space\n",
    "    \n",
    "    def reset(self):\n",
    "        obs, info = self.env.reset()\n",
    "        # Debug: print observation structure (only once)\n",
    "        if not hasattr(self, '_debug_printed'):\n",
    "            print(\"=== DEBUG: Observation Structure ===\")\n",
    "            print(f\"Type: {type(obs)}\")\n",
    "            if isinstance(obs, dict):\n",
    "                for key, value in obs.items():\n",
    "                    print(f\"  {key}: {type(value)} {getattr(value, 'shape', 'N/A')} = {value}\")\n",
    "            else:\n",
    "                print(f\"Raw obs: {obs}\")\n",
    "            self._debug_printed = True\n",
    "        \n",
    "        flat_obs = self._flatten_obs(obs)\n",
    "        return flat_obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        flat_obs = self._flatten_obs(obs)\n",
    "        return flat_obs, reward, done, info\n",
    "    \n",
    "    def _flatten_obs(self, obs_dict):\n",
    "        \"\"\"Convert dict observation to flat array to match model's expected input (48-dim)\"\"\"\n",
    "        flat_obs = []\n",
    "        \n",
    "        try:\n",
    "            # Handle different observation formats\n",
    "            if isinstance(obs_dict, dict):\n",
    "                # Extract values in consistent order to match 48 dimensions\n",
    "                # Handle both array and scalar formats\n",
    "                def extract_value(key, index=0):\n",
    "                    value = obs_dict[key]\n",
    "                    if isinstance(value, (list, np.ndarray)) and len(value) > index:\n",
    "                        return value[index]\n",
    "                    elif isinstance(value, (int, float)):\n",
    "                        return float(value)\n",
    "                    else:\n",
    "                        return 0.0\n",
    "                \n",
    "                def extract_array(key):\n",
    "                    value = obs_dict[key]\n",
    "                    if isinstance(value, (list, np.ndarray)):\n",
    "                        return list(value)\n",
    "                    else:\n",
    "                        return [float(value)]\n",
    "                \n",
    "                # Build flat observation (targeting 48 dimensions)\n",
    "                flat_obs.append(extract_value(\"available_computation_units\"))  # 1\n",
    "                flat_obs.append(extract_value(\"remain_epochs\"))  # 1\n",
    "                flat_obs.extend(extract_array(\"mec_comp_units\"))  # 20 (max_queue_size)\n",
    "                flat_obs.extend(extract_array(\"mec_proc_times\"))  # 20\n",
    "                flat_obs.append(extract_value(\"queue_comp_units\"))  # 1\n",
    "                flat_obs.append(extract_value(\"queue_proc_times\"))  # 1\n",
    "                flat_obs.append(float(obs_dict.get(\"local_success\", 0)))  # 1\n",
    "                flat_obs.append(float(obs_dict.get(\"offload_success\", 0)))  # 1\n",
    "                flat_obs.extend(extract_array(\"ctx_vel\"))  # 1\n",
    "                flat_obs.extend(extract_array(\"ctx_comp\"))  # 1\n",
    "                \n",
    "            else:\n",
    "                # If obs is already a flat array, use it directly\n",
    "                flat_obs = list(obs_dict) if hasattr(obs_dict, '__iter__') else [float(obs_dict)]\n",
    "            \n",
    "            # Ensure exactly 48 dimensions\n",
    "            if len(flat_obs) < 48:\n",
    "                flat_obs.extend([0.0] * (48 - len(flat_obs)))\n",
    "            elif len(flat_obs) > 48:\n",
    "                flat_obs = flat_obs[:48]\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in _flatten_obs: {e}\")\n",
    "            print(f\"obs_dict type: {type(obs_dict)}\")\n",
    "            print(f\"obs_dict: {obs_dict}\")\n",
    "            # Fallback: create zero array\n",
    "            flat_obs = [0.0] * 48\n",
    "            \n",
    "        return np.array(flat_obs, dtype=np.float32)\n",
    "    \n",
    "    def get_valid_actions(self):\n",
    "        return self.env.get_valid_actions()\n",
    "\n",
    "# Initialize environment with proper reward parameters\n",
    "base_env = CustomEnv(\n",
    "    max_comp_units=ENV_PARAMS['max_comp_units'],\n",
    "    max_epoch_size=ENV_PARAMS['max_epoch_size'], \n",
    "    max_queue_size=ENV_PARAMS['max_queue_size'],\n",
    "    max_comp_units_for_cloud=ENV_PARAMS['max_comp_units_for_cloud'],\n",
    "    reward_weights=ENV_PARAMS['reward_weights'],\n",
    "    agent_velocities=ENV_PARAMS['agent_velocities'],\n",
    "    reward_params=REWARD_PARAMS  # 🔧 Added missing reward parameters\n",
    ")\n",
    "\n",
    "# Wrap environment for RNN model compatibility\n",
    "env = RNNCompatibleEnv(base_env)\n",
    "\n",
    "# Test environment reset to see observation structure\n",
    "print(\"Testing environment reset...\")\n",
    "test_obs = env.reset()\n",
    "print(f\"Flattened observation shape: {test_obs.shape}\")\n",
    "print(f\"First few values: {test_obs[:10]}\")\n",
    "\n",
    "# Test one step to verify reward calculation works\n",
    "print(\"\\\\nTesting environment step...\")\n",
    "try:\n",
    "    test_obs, test_reward, test_done, test_info = env.step(0)  # Try LOCAL action\n",
    "    print(f\"Step successful - reward: {test_reward}, done: {test_done}\")\n",
    "except Exception as e:\n",
    "    print(f\"Step failed: {e}\")\n",
    "\n",
    "print(f\"\\\\nEnvironment initialized:\")\n",
    "print(f\"  Base environment action space: {base_env.action_space.n} (LOCAL, OFFLOAD, DISCARD)\")\n",
    "print(f\"  Models expect: state_dim={config.state_dim}, action_dim={config.action_dim}\")\n",
    "print(f\"  Model type: {config.model_type.upper()}\")\n",
    "print(f\"  Reward params: {list(REWARD_PARAMS.keys())}\")\n",
    "print(\"✅ Environment ready for evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Global vs Individual Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading and Evaluating Models from 20250923_143004 ===\n",
      "❌ No global model found\n",
      "❌ No individual worker models found\n",
      "\n",
      "=== Summary ===\n",
      "Successfully loaded and evaluated:\n",
      "  Global models: 0\n",
      "  Individual worker models: 0\n",
      "  Total successful evaluations: 0\n",
      "❌ No models were successfully evaluated. Check model paths and compatibility.\n"
     ]
    }
   ],
   "source": [
    "from drl_framework.params import *\n",
    "\n",
    "# Load and evaluate models from target timestamp\n",
    "print(f\"=== Loading and Evaluating Models from {config.target_timestamp} ===\")\n",
    "main_results = {}\n",
    "\n",
    "# Load global model\n",
    "if 'global_final' in models_info['global_models']:\n",
    "    global_path = models_info['global_models']['global_final']\n",
    "    print(f\"Loading A3C global model from: {global_path}\")\n",
    "    global_model = load_model(global_path, config.model_type)\n",
    "    if global_model:\n",
    "        print(\"Evaluating A3C global model...\")\n",
    "        main_results['global'] = evaluate_model(global_model, env, config.evaluation_episodes)\n",
    "        print(f\"Global model - Mean reward: {main_results['global']['mean_reward']:.2f} ± {main_results['global']['std_reward']:.2f}\")\n",
    "    else:\n",
    "        print(\"❌ Failed to load global model\")\n",
    "else:\n",
    "    print(\"❌ No global model found\")\n",
    "\n",
    "# Load individual worker models\n",
    "individual_results = {}\n",
    "if models_info['individual_models']:\n",
    "    print(f\"\\nLoading {len(models_info['individual_models'])} individual worker models...\")\n",
    "    \n",
    "    for model_name, model_path in models_info['individual_models'].items():\n",
    "        if model_name.startswith('individual_worker_'):\n",
    "            worker_id = model_name.split('_')[-2]  # Extract worker ID\n",
    "            print(f\"Loading individual worker {worker_id} from: {model_path}\")\n",
    "            worker_model = load_model(model_path, config.model_type)\n",
    "            if worker_model:\n",
    "                print(f\"Evaluating worker {worker_id}...\")\n",
    "                individual_results[f'worker_{worker_id}'] = evaluate_model(worker_model, env, config.evaluation_episodes)\n",
    "                print(f\"Worker {worker_id} - Mean reward: {individual_results[f'worker_{worker_id}']['mean_reward']:.2f}\")\n",
    "            else:\n",
    "                print(f\"❌ Failed to load worker {worker_id}\")\n",
    "else:\n",
    "    print(\"❌ No individual worker models found\")\n",
    "\n",
    "print(f\"\\n=== Summary ===\")\n",
    "print(f\"Successfully loaded and evaluated:\")\n",
    "print(f\"  Global models: {len(main_results)}\")\n",
    "print(f\"  Individual worker models: {len(individual_results)}\")\n",
    "print(f\"  Total successful evaluations: {len(main_results) + len(individual_results)}\")\n",
    "\n",
    "if main_results or individual_results:\n",
    "    print(\"✅ Ready for ablation analysis!\")\n",
    "else:\n",
    "    print(\"❌ No models were successfully evaluated. Check model paths and compatibility.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to perform comparison - missing global or individual models\n",
      "Available results:\n",
      "  Global: ❌\n",
      "  Individual: ❌\n"
     ]
    }
   ],
   "source": [
    "# Statistical comparison between global and individual models (updated for 3 actions)\n",
    "if 'global' in main_results and individual_results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Reward comparison\n",
    "    global_rewards = main_results['global']['all_rewards']\n",
    "    individual_rewards_all = []\n",
    "    worker_labels = []\n",
    "    \n",
    "    for worker, results in individual_results.items():\n",
    "        individual_rewards_all.extend(results['all_rewards'])\n",
    "        worker_labels.extend([worker] * len(results['all_rewards']))\n",
    "    \n",
    "    # Box plot comparison\n",
    "    axes[0, 0].boxplot([global_rewards, individual_rewards_all], \n",
    "                       labels=['A3C Global', 'Individual Workers'])\n",
    "    axes[0, 0].set_title('Reward Distribution: A3C Global vs Individual Workers')\n",
    "    axes[0, 0].set_ylabel('Episode Reward')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Individual worker comparison\n",
    "    worker_rewards = [results['all_rewards'] for results in individual_results.values()]\n",
    "    worker_names = list(individual_results.keys())\n",
    "    \n",
    "    bp = axes[0, 1].boxplot(worker_rewards, labels=worker_names)\n",
    "    axes[0, 1].set_title('Reward Distribution Across Individual Workers')\n",
    "    axes[0, 1].set_ylabel('Episode Reward')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Action distribution comparison (updated for 3 actions)\n",
    "    action_names = ['LOCAL', 'OFFLOAD', 'DISCARD']\n",
    "    global_actions = [main_results['global']['action_distribution'].get(i, 0) for i in range(3)]\n",
    "    global_actions_norm = np.array(global_actions) / np.sum(global_actions) if np.sum(global_actions) > 0 else np.zeros(3)\n",
    "    \n",
    "    # Average individual action distribution\n",
    "    individual_actions = np.zeros(3)\n",
    "    for results in individual_results.values():\n",
    "        for i in range(3):\n",
    "            individual_actions[i] += results['action_distribution'].get(i, 0)\n",
    "    individual_actions_norm = individual_actions / np.sum(individual_actions) if np.sum(individual_actions) > 0 else np.zeros(3)\n",
    "    \n",
    "    x = np.arange(len(action_names))\n",
    "    width = 0.35\n",
    "    axes[1, 0].bar(x - width/2, global_actions_norm, width, label='A3C Global', alpha=0.8, color='red')\n",
    "    axes[1, 0].bar(x + width/2, individual_actions_norm, width, label='Individual Workers (Avg)', alpha=0.8, color='blue')\n",
    "    axes[1, 0].set_title('Action Distribution Comparison')\n",
    "    axes[1, 0].set_ylabel('Probability')\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels(action_names)\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for i, (g_val, i_val) in enumerate(zip(global_actions_norm, individual_actions_norm)):\n",
    "        if g_val > 0.01:  # Only show if > 1%\n",
    "            axes[1, 0].text(i - width/2, g_val + 0.01, f'{g_val:.1%}', ha='center', va='bottom', fontsize=8)\n",
    "        if i_val > 0.01:\n",
    "            axes[1, 0].text(i + width/2, i_val + 0.01, f'{i_val:.1%}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # 4. Performance summary table\n",
    "    summary_data = []\n",
    "    summary_data.append(['A3C Global', main_results['global']['mean_reward'], main_results['global']['std_reward']])\n",
    "    \n",
    "    for worker, results in individual_results.items():\n",
    "        summary_data.append([worker, results['mean_reward'], results['std_reward']])\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_data, columns=['Model', 'Mean Reward', 'Std Reward'])\n",
    "    axes[1, 1].axis('tight')\n",
    "    axes[1, 1].axis('off')\n",
    "    table = axes[1, 1].table(cellText=[[f\"{row[0]}\", f\"{row[1]:.2f}\", f\"{row[2]:.2f}\"] for row in summary_data],\n",
    "                            colLabels=['Model', 'Mean Reward', 'Std Reward'],\n",
    "                            cellLoc='center',\n",
    "                            loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.2, 1.5)\n",
    "    axes[1, 1].set_title(f'Performance Summary ({config.target_timestamp})')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical tests\n",
    "    print(\"\\\\n=== Statistical Analysis ===\")\n",
    "    global_mean = main_results['global']['mean_reward']\n",
    "    individual_means = [results['mean_reward'] for results in individual_results.values()]\n",
    "    individual_mean = np.mean(individual_means)\n",
    "    \n",
    "    print(f\"A3C Global model mean reward: {global_mean:.3f} ± {main_results['global']['std_reward']:.3f}\")\n",
    "    print(f\"Individual models mean reward: {individual_mean:.3f} ± {np.std(individual_means):.3f}\")\n",
    "    print(f\"Performance difference (Global - Individual): {global_mean - individual_mean:.3f}\")\n",
    "    \n",
    "    # Action distribution analysis\n",
    "    print(f\"\\\\n=== Action Distribution Analysis ===\")\n",
    "    print(\"A3C Global action preferences:\")\n",
    "    for i, (action, prob) in enumerate(zip(action_names, global_actions_norm)):\n",
    "        print(f\"  {action}: {prob:.1%}\")\n",
    "    \n",
    "    print(\"Individual workers average action preferences:\")\n",
    "    for i, (action, prob) in enumerate(zip(action_names, individual_actions_norm)):\n",
    "        print(f\"  {action}: {prob:.1%}\")\n",
    "    \n",
    "    # t-test between global and pooled individual rewards\n",
    "    t_stat, p_value = stats.ttest_ind(global_rewards, individual_rewards_all)\n",
    "    print(f\"\\\\nT-test (Global vs Individual): t={t_stat:.3f}, p={p_value:.3f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"✅ Significant difference detected between A3C global and individual models\")\n",
    "    else:\n",
    "        print(\"❌ No significant difference between A3C global and individual models\")\n",
    "    \n",
    "else:\n",
    "    print(\"Unable to perform comparison - missing global or individual models\")\n",
    "    print(\"Available results:\")\n",
    "    print(f\"  Global: {'✅' if 'global' in main_results else '❌'}\")\n",
    "    print(f\"  Individual: {'✅' if individual_results else '❌'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Worker Variability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No individual worker models found for variability analysis\n",
      "Check if individual models exist in the target timestamp directory.\n"
     ]
    }
   ],
   "source": [
    "if individual_results:\n",
    "    print(\"=== Worker Variability Analysis ===\")\n",
    "    \n",
    "    # Create comprehensive worker analysis (updated for 3 actions)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Worker performance ranking\n",
    "    worker_performance = [(worker, results['mean_reward'], results['std_reward']) \n",
    "                         for worker, results in individual_results.items()]\n",
    "    worker_performance.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    workers, means, stds = zip(*worker_performance)\n",
    "    x_pos = np.arange(len(workers))\n",
    "    \n",
    "    bars = axes[0, 0].bar(x_pos, means, yerr=stds, capsize=5, alpha=0.7)\n",
    "    axes[0, 0].set_title(f'Worker Performance Ranking ({config.target_timestamp})')\n",
    "    axes[0, 0].set_ylabel('Mean Episode Reward')\n",
    "    axes[0, 0].set_xticks(x_pos)\n",
    "    axes[0, 0].set_xticklabels(workers, rotation=45)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Color bars by performance\n",
    "    colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(bars)))\n",
    "    for bar, color in zip(bars, colors):\n",
    "        bar.set_color(color)\n",
    "    \n",
    "    # 2. Worker consistency (coefficient of variation)\n",
    "    cvs = [results['std_reward'] / abs(results['mean_reward']) if results['mean_reward'] != 0 else 0 \n",
    "           for results in individual_results.values()]\n",
    "    worker_names = list(individual_results.keys())\n",
    "    \n",
    "    axes[0, 1].bar(range(len(worker_names)), cvs, alpha=0.7, color='skyblue')\n",
    "    axes[0, 1].set_title('Worker Consistency (Lower = More Consistent)')\n",
    "    axes[0, 1].set_ylabel('Coefficient of Variation')\n",
    "    axes[0, 1].set_xticks(range(len(worker_names)))\n",
    "    axes[0, 1].set_xticklabels(worker_names, rotation=45)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Action distribution heatmap (updated for 3 actions)\n",
    "    action_matrix = []\n",
    "    action_names = ['LOCAL', 'OFFLOAD', 'DISCARD']\n",
    "    \n",
    "    for worker in worker_names:\n",
    "        actions = individual_results[worker]['action_distribution']\n",
    "        total_actions = sum(actions.values())\n",
    "        action_probs = [actions.get(i, 0) / total_actions if total_actions > 0 else 0 for i in range(3)]\n",
    "        action_matrix.append(action_probs)\n",
    "    \n",
    "    im = axes[0, 2].imshow(action_matrix, cmap='YlOrRd', aspect='auto', vmin=0, vmax=1)\n",
    "    axes[0, 2].set_title('Action Distribution Across Workers')\n",
    "    axes[0, 2].set_xlabel('Actions')\n",
    "    axes[0, 2].set_ylabel('Workers')\n",
    "    axes[0, 2].set_xticks(range(3))\n",
    "    axes[0, 2].set_xticklabels(action_names)\n",
    "    axes[0, 2].set_yticks(range(len(worker_names)))\n",
    "    axes[0, 2].set_yticklabels(worker_names)\n",
    "    plt.colorbar(im, ax=axes[0, 2], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Add percentage values to heatmap\n",
    "    for i in range(len(worker_names)):\n",
    "        for j in range(3):\n",
    "            text = axes[0, 2].text(j, i, f'{action_matrix[i][j]:.1%}',\n",
    "                                 ha=\"center\", va=\"center\", color=\"black\" if action_matrix[i][j] < 0.5 else \"white\",\n",
    "                                 fontsize=8)\n",
    "    \n",
    "    # 4. Reward distribution violin plot\n",
    "    all_worker_rewards = [individual_results[worker]['all_rewards'] for worker in worker_names]\n",
    "    parts = axes[1, 0].violinplot(all_worker_rewards, positions=range(len(worker_names)), showmeans=True)\n",
    "    axes[1, 0].set_title('Reward Distribution Shape by Worker')\n",
    "    axes[1, 0].set_ylabel('Episode Reward')\n",
    "    axes[1, 0].set_xticks(range(len(worker_names)))\n",
    "    axes[1, 0].set_xticklabels(worker_names, rotation=45)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Episode length analysis\n",
    "    episode_lengths = [individual_results[worker]['mean_length'] for worker in worker_names]\n",
    "    length_stds = [individual_results[worker]['std_length'] for worker in worker_names]\n",
    "    \n",
    "    axes[1, 1].bar(range(len(worker_names)), episode_lengths, yerr=length_stds, \n",
    "                   capsize=5, alpha=0.7, color='lightcoral')\n",
    "    axes[1, 1].set_title('Episode Length by Worker')\n",
    "    axes[1, 1].set_ylabel('Mean Episode Length')\n",
    "    axes[1, 1].set_xticks(range(len(worker_names)))\n",
    "    axes[1, 1].set_xticklabels(worker_names, rotation=45)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Worker correlation matrix\n",
    "    if len(individual_results) > 1:\n",
    "        # Create correlation matrix based on episode rewards\n",
    "        reward_matrix = np.array([individual_results[worker]['all_rewards'] for worker in worker_names])\n",
    "        correlation_matrix = np.corrcoef(reward_matrix)\n",
    "        \n",
    "        im2 = axes[1, 2].imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        axes[1, 2].set_title('Worker Performance Correlation')\n",
    "        axes[1, 2].set_xticks(range(len(worker_names)))\n",
    "        axes[1, 2].set_xticklabels(worker_names, rotation=45)\n",
    "        axes[1, 2].set_yticks(range(len(worker_names)))\n",
    "        axes[1, 2].set_yticklabels(worker_names)\n",
    "        plt.colorbar(im2, ax=axes[1, 2], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Add correlation values to heatmap\n",
    "        for i in range(len(worker_names)):\n",
    "            for j in range(len(worker_names)):\n",
    "                axes[1, 2].text(j, i, f'{correlation_matrix[i, j]:.2f}', \n",
    "                               ha='center', va='center', fontsize=8,\n",
    "                               color='white' if abs(correlation_matrix[i, j]) > 0.5 else 'black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical analysis of worker variability\n",
    "    print(\"\\\\n=== Worker Variability Statistics ===\")\n",
    "    worker_means = [individual_results[worker]['mean_reward'] for worker in worker_names]\n",
    "    print(f\"Inter-worker mean reward std: {np.std(worker_means):.3f}\")\n",
    "    print(f\"Best worker: {workers[0]} ({means[0]:.3f})\")\n",
    "    print(f\"Worst worker: {workers[-1]} ({means[-1]:.3f})\")\n",
    "    print(f\"Performance gap: {means[0] - means[-1]:.3f}\")\n",
    "    \n",
    "    # Action preference analysis\n",
    "    print(f\"\\\\n=== Action Preferences by Worker ===\")\n",
    "    for worker in worker_names:\n",
    "        actions = individual_results[worker]['action_distribution']\n",
    "        total = sum(actions.values())\n",
    "        if total > 0:\n",
    "            prefs = [actions.get(i, 0) / total for i in range(3)]\n",
    "            dominant_action = action_names[np.argmax(prefs)]\n",
    "            print(f\"{worker}: {dominant_action} ({max(prefs):.1%}) - LOCAL:{prefs[0]:.1%}, OFFLOAD:{prefs[1]:.1%}, DISCARD:{prefs[2]:.1%}\")\n",
    "    \n",
    "    # ANOVA test for significant differences between workers\n",
    "    worker_reward_lists = [individual_results[worker]['all_rewards'] for worker in worker_names]\n",
    "    f_stat, p_value = stats.f_oneway(*worker_reward_lists)\n",
    "    print(f\"\\\\nANOVA F-statistic: {f_stat:.3f}, p-value: {p_value:.3f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"✅ Significant differences detected between workers\")\n",
    "    else:\n",
    "        print(\"❌ No significant differences between workers\")\n",
    "\n",
    "else:\n",
    "    print(\"No individual worker models found for variability analysis\")\n",
    "    print(\"Check if individual models exist in the target timestamp directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Structure and Parameter Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Structure Analysis ===\n",
      "No models available for structure analysis\n"
     ]
    }
   ],
   "source": [
    "# Analyze model structures\n",
    "print(\"=== Model Structure Analysis ===\")\n",
    "\n",
    "# Load a few models for analysis\n",
    "models_to_analyze = {}\n",
    "\n",
    "# Global model - 🔧 Fixed: use 'global_models' instead of 'main_models'\n",
    "if 'global_final' in models_info['global_models']:\n",
    "    global_model = load_model(models_info['global_models']['global_final'])\n",
    "    if global_model:\n",
    "        models_to_analyze['Global'] = global_model\n",
    "\n",
    "# A few individual workers - 🔧 Fixed: use 'individual_models' instead of 'main_models'\n",
    "worker_count = 0\n",
    "for model_name, model_path in models_info['individual_models'].items():\n",
    "    if model_name.startswith('individual_worker_') and worker_count < 3:\n",
    "        worker_id = model_name.split('_')[-2]\n",
    "        worker_model = load_model(model_path)\n",
    "        if worker_model:\n",
    "            models_to_analyze[f'Worker_{worker_id}'] = worker_model\n",
    "            worker_count += 1\n",
    "\n",
    "# Analyze each model\n",
    "model_analyses = {}\n",
    "for name, model in models_to_analyze.items():\n",
    "    print(f\"\\nAnalyzing {name}...\")\n",
    "    analysis = analyze_model_parameters(model, name)\n",
    "    model_analyses[name] = analysis\n",
    "    print(f\"  Total parameters: {analysis['total_params']:,}\")\n",
    "    print(f\"  Trainable parameters: {analysis['trainable_params']:,}\")\n",
    "\n",
    "# Visualize parameter analysis\n",
    "if model_analyses:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Parameter count comparison\n",
    "    model_names = list(model_analyses.keys())\n",
    "    param_counts = [model_analyses[name]['total_params'] for name in model_names]\n",
    "    \n",
    "    axes[0, 0].bar(model_names, param_counts, alpha=0.7)\n",
    "    axes[0, 0].set_title('Total Parameters by Model')\n",
    "    axes[0, 0].set_ylabel('Parameter Count')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Weight distribution for first model\n",
    "    if model_names:\n",
    "        first_model = model_names[0]\n",
    "        weight_norms = [stats['norm'] for stats in model_analyses[first_model]['weight_stats'].values()]\n",
    "        layer_names = list(model_analyses[first_model]['weight_stats'].keys())\n",
    "        \n",
    "        axes[0, 1].bar(range(len(layer_names)), weight_norms, alpha=0.7)\n",
    "        axes[0, 1].set_title(f'Layer Weight Norms - {first_model}')\n",
    "        axes[0, 1].set_ylabel('L2 Norm')\n",
    "        axes[0, 1].set_xticks(range(len(layer_names)))\n",
    "        axes[0, 1].set_xticklabels([name.split('.')[-1] for name in layer_names], rotation=45)\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Model similarity heatmap (if multiple models)\n",
    "    if len(models_to_analyze) > 1:\n",
    "        similarity_matrix = np.zeros((len(model_names), len(model_names)))\n",
    "        \n",
    "        for i, name1 in enumerate(model_names):\n",
    "            for j, name2 in enumerate(model_names):\n",
    "                if i == j:\n",
    "                    similarity_matrix[i, j] = 1.0\n",
    "                elif i < j:\n",
    "                    comparison = compare_model_weights(\n",
    "                        models_to_analyze[name1], \n",
    "                        models_to_analyze[name2], \n",
    "                        name1, name2\n",
    "                    )\n",
    "                    similarity = comparison['overall_similarity']\n",
    "                    similarity_matrix[i, j] = similarity\n",
    "                    similarity_matrix[j, i] = similarity\n",
    "        \n",
    "        im = axes[1, 0].imshow(similarity_matrix, cmap='RdYlBu', vmin=0, vmax=1)\n",
    "        axes[1, 0].set_title('Model Weight Similarity Matrix')\n",
    "        axes[1, 0].set_xticks(range(len(model_names)))\n",
    "        axes[1, 0].set_xticklabels(model_names, rotation=45)\n",
    "        axes[1, 0].set_yticks(range(len(model_names)))\n",
    "        axes[1, 0].set_yticklabels(model_names)\n",
    "        plt.colorbar(im, ax=axes[1, 0], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Add similarity values\n",
    "        for i in range(len(model_names)):\n",
    "            for j in range(len(model_names)):\n",
    "                axes[1, 0].text(j, i, f'{similarity_matrix[i, j]:.3f}', \n",
    "                               ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # 4. Layer-wise parameter distribution\n",
    "    if model_names:\n",
    "        first_model = model_names[0]\n",
    "        param_dist = model_analyses[first_model]['param_distribution']\n",
    "        \n",
    "        # Create pie chart for parameter distribution\n",
    "        layer_params = list(param_dist.values())\n",
    "        layer_labels = [name.split('.')[-1] for name in param_dist.keys()]\n",
    "        \n",
    "        axes[1, 1].pie(layer_params, labels=layer_labels, autopct='%1.1f%%', startangle=90)\n",
    "        axes[1, 1].set_title(f'Parameter Distribution - {first_model}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed comparison\n",
    "    if len(models_to_analyze) > 1:\n",
    "        print(\"\\n=== Model Weight Similarity Analysis ===\")\n",
    "        for i, name1 in enumerate(model_names[:-1]):\n",
    "            for name2 in model_names[i+1:]:\n",
    "                comparison = compare_model_weights(\n",
    "                    models_to_analyze[name1], \n",
    "                    models_to_analyze[name2], \n",
    "                    name1, name2\n",
    "                )\n",
    "                print(f\"{name1} vs {name2}: {comparison['overall_similarity']:.3f} similarity\")\n",
    "\n",
    "else:\n",
    "    print(\"No models available for structure analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Structure and Parameter Analysis (Enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Enhanced Model Structure Analysis (Main Models Only) ===\n",
      "Successfully loaded 0 models for analysis\n",
      "No models available for enhanced structure analysis\n"
     ]
    }
   ],
   "source": [
    "# Enhanced model structure analysis for main models only\n",
    "print(\"=== Enhanced Model Structure Analysis (Main Models Only) ===\")\n",
    "\n",
    "# Load all available models from main directory\n",
    "models_to_analyze = {}\n",
    "\n",
    "# Global model - 🔧 Fixed: use 'global_models' instead of 'main_models'\n",
    "if 'global_final' in models_info['global_models']:\n",
    "    global_model = load_model(models_info['global_models']['global_final'])\n",
    "    if global_model:\n",
    "        models_to_analyze['Global'] = global_model\n",
    "\n",
    "# All individual workers - 🔧 Fixed: use 'individual_models' instead of 'main_models'\n",
    "for model_name, model_path in models_info['individual_models'].items():\n",
    "    if model_name.startswith('individual_worker_'):\n",
    "        worker_id = model_name.split('_')[-2]\n",
    "        worker_model = load_model(model_path)\n",
    "        if worker_model:\n",
    "            models_to_analyze[f'Worker_{worker_id}'] = worker_model\n",
    "\n",
    "print(f\"Successfully loaded {len(models_to_analyze)} models for analysis\")\n",
    "\n",
    "# Analyze each model\n",
    "model_analyses = {}\n",
    "for name, model in models_to_analyze.items():\n",
    "    print(f\"Analyzing {name}...\")\n",
    "    analysis = analyze_model_parameters(model, name)\n",
    "    model_analyses[name] = analysis\n",
    "    print(f\"  Total parameters: {analysis['total_params']:,}\")\n",
    "    print(f\"  Trainable parameters: {analysis['trainable_params']:,}\")\n",
    "\n",
    "# Enhanced visualization for main models\n",
    "if model_analyses:\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(15, 18))\n",
    "    \n",
    "    # 1. Parameter count comparison\n",
    "    model_names = list(model_analyses.keys())\n",
    "    param_counts = [model_analyses[name]['total_params'] for name in model_names]\n",
    "    \n",
    "    bars = axes[0, 0].bar(model_names, param_counts, alpha=0.7)\n",
    "    axes[0, 0].set_title('Total Parameters by Model')\n",
    "    axes[0, 0].set_ylabel('Parameter Count')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Color code global vs individual\n",
    "    for i, bar in enumerate(bars):\n",
    "        if 'Global' in model_names[i]:\n",
    "            bar.set_color('red')\n",
    "        else:\n",
    "            bar.set_color('blue')\n",
    "    \n",
    "    # 2. Weight distribution for global model (if available)\n",
    "    if 'Global' in model_names:\n",
    "        weight_norms = [stats['norm'] for stats in model_analyses['Global']['weight_stats'].values()]\n",
    "        layer_names = list(model_analyses['Global']['weight_stats'].keys())\n",
    "        \n",
    "        axes[0, 1].bar(range(len(layer_names)), weight_norms, alpha=0.7, color='red')\n",
    "        axes[0, 1].set_title('Layer Weight Norms - Global Model')\n",
    "        axes[0, 1].set_ylabel('L2 Norm')\n",
    "        axes[0, 1].set_xticks(range(len(layer_names)))\n",
    "        axes[0, 1].set_xticklabels([name.split('.')[-1] for name in layer_names], rotation=45)\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Model similarity heatmap\n",
    "    if len(models_to_analyze) > 1:\n",
    "        similarity_matrix = np.zeros((len(model_names), len(model_names)))\n",
    "        \n",
    "        for i, name1 in enumerate(model_names):\n",
    "            for j, name2 in enumerate(model_names):\n",
    "                if i == j:\n",
    "                    similarity_matrix[i, j] = 1.0\n",
    "                elif i < j:\n",
    "                    comparison = compare_model_weights(\n",
    "                        models_to_analyze[name1], \n",
    "                        models_to_analyze[name2], \n",
    "                        name1, name2\n",
    "                    )\n",
    "                    similarity = comparison['overall_similarity']\n",
    "                    similarity_matrix[i, j] = similarity\n",
    "                    similarity_matrix[j, i] = similarity\n",
    "        \n",
    "        im = axes[1, 0].imshow(similarity_matrix, cmap='RdYlBu', vmin=0, vmax=1)\n",
    "        axes[1, 0].set_title('Model Weight Similarity Matrix')\n",
    "        axes[1, 0].set_xticks(range(len(model_names)))\n",
    "        axes[1, 0].set_xticklabels(model_names, rotation=45)\n",
    "        axes[1, 0].set_yticks(range(len(model_names)))\n",
    "        axes[1, 0].set_yticklabels(model_names)\n",
    "        plt.colorbar(im, ax=axes[1, 0], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Add similarity values\n",
    "        for i in range(len(model_names)):\n",
    "            for j in range(len(model_names)):\n",
    "                axes[1, 0].text(j, i, f'{similarity_matrix[i, j]:.2f}', \n",
    "                               ha='center', va='center', fontsize=6)\n",
    "    \n",
    "    # 4. Parameter distribution pie chart\n",
    "    if 'Global' in model_names:\n",
    "        param_dist = model_analyses['Global']['param_distribution']\n",
    "        layer_params = list(param_dist.values())\n",
    "        layer_labels = [name.split('.')[-1] for name in param_dist.keys()]\n",
    "        \n",
    "        axes[1, 1].pie(layer_params, labels=layer_labels, autopct='%1.1f%%', startangle=90)\n",
    "        axes[1, 1].set_title('Parameter Distribution - Global Model')\n",
    "    \n",
    "    # 5. Weight statistics comparison (mean and std)\n",
    "    if len(model_names) > 1:\n",
    "        weight_means = []\n",
    "        weight_stds = []\n",
    "        \n",
    "        for name in model_names:\n",
    "            model_weight_means = [stats['mean'] for stats in model_analyses[name]['weight_stats'].values()]\n",
    "            model_weight_stds = [stats['std'] for stats in model_analyses[name]['weight_stats'].values()]\n",
    "            weight_means.append(np.mean(model_weight_means))\n",
    "            weight_stds.append(np.mean(model_weight_stds))\n",
    "        \n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = axes[2, 0].bar(x - width/2, weight_means, width, label='Mean Weight', alpha=0.8)\n",
    "        bars2 = axes[2, 0].bar(x + width/2, weight_stds, width, label='Weight Std', alpha=0.8)\n",
    "        \n",
    "        axes[2, 0].set_title('Average Weight Statistics by Model')\n",
    "        axes[2, 0].set_ylabel('Weight Value')\n",
    "        axes[2, 0].set_xticks(x)\n",
    "        axes[2, 0].set_xticklabels(model_names, rotation=45)\n",
    "        axes[2, 0].legend()\n",
    "        axes[2, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Model complexity comparison\n",
    "    if model_names:\n",
    "        # Calculate effective model complexity (parameters per layer)\n",
    "        complexities = []\n",
    "        layer_counts = []\n",
    "        \n",
    "        for name in model_names:\n",
    "            layer_count = len(model_analyses[name]['layer_info'])\n",
    "            param_count = model_analyses[name]['total_params']\n",
    "            complexity = param_count / layer_count if layer_count > 0 else 0\n",
    "            complexities.append(complexity)\n",
    "            layer_counts.append(layer_count)\n",
    "        \n",
    "        # Create scatter plot\n",
    "        colors = ['red' if 'Global' in name else 'blue' for name in model_names]\n",
    "        scatter = axes[2, 1].scatter(layer_counts, complexities, c=colors, alpha=0.7, s=100)\n",
    "        \n",
    "        # Add labels\n",
    "        for i, name in enumerate(model_names):\n",
    "            axes[2, 1].annotate(name, (layer_counts[i], complexities[i]), \n",
    "                               xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        axes[2, 1].set_xlabel('Number of Layers')\n",
    "        axes[2, 1].set_ylabel('Parameters per Layer')\n",
    "        axes[2, 1].set_title('Model Complexity Analysis')\n",
    "        axes[2, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed similarity analysis\n",
    "    if len(models_to_analyze) > 1:\n",
    "        print(\"\\n=== Model Weight Similarity Analysis ===\")\n",
    "        \n",
    "        # Global vs Individual comparisons\n",
    "        global_similarities = []\n",
    "        if 'Global' in model_names:\n",
    "            for name in model_names:\n",
    "                if name != 'Global' and 'Worker' in name:\n",
    "                    comparison = compare_model_weights(\n",
    "                        models_to_analyze['Global'], \n",
    "                        models_to_analyze[name], \n",
    "                        'Global', name\n",
    "                    )\n",
    "                    similarity = comparison['overall_similarity']\n",
    "                    global_similarities.append(similarity)\n",
    "                    print(f\"Global vs {name}: {similarity:.3f} similarity\")\n",
    "            \n",
    "            if global_similarities:\n",
    "                print(f\"Average Global-Individual similarity: {np.mean(global_similarities):.3f}\")\n",
    "                print(f\"Similarity std: {np.std(global_similarities):.3f}\")\n",
    "        \n",
    "        # Individual worker comparisons\n",
    "        worker_names = [name for name in model_names if 'Worker' in name]\n",
    "        if len(worker_names) > 1:\n",
    "            print(f\"\\n=== Individual Worker Similarities ===\")\n",
    "            worker_similarities = []\n",
    "            for i in range(len(worker_names)):\n",
    "                for j in range(i+1, len(worker_names)):\n",
    "                    comparison = compare_model_weights(\n",
    "                        models_to_analyze[worker_names[i]], \n",
    "                        models_to_analyze[worker_names[j]], \n",
    "                        worker_names[i], worker_names[j]\n",
    "                    )\n",
    "                    similarity = comparison['overall_similarity']\n",
    "                    worker_similarities.append(similarity)\n",
    "                    print(f\"{worker_names[i]} vs {worker_names[j]}: {similarity:.3f}\")\n",
    "            \n",
    "            if worker_similarities:\n",
    "                print(f\"Average inter-worker similarity: {np.mean(worker_similarities):.3f}\")\n",
    "\n",
    "else:\n",
    "    print(\"No models available for enhanced structure analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n======================================================================\n",
      "COMPREHENSIVE ABLATION ANALYSIS SUMMARY\n",
      "Target Models: 20250923_143004\n",
      "======================================================================\n",
      "\\n=== KEY FINDINGS ===\n",
      "1. 🏗️  All models use RNN architecture with 48D state and 3 actions\n",
      "2. ✅ Perfect environment compatibility (no action mapping needed)\n",
      "\\n=== RECOMMENDATIONS ===\n",
      "1. Continue monitoring model performance using 20250923_143004 architecture\n",
      "2. Consider ensemble methods combining best-performing individual workers\n",
      "\\n======================================================================\n",
      "Analysis completed at: 2025-09-28 02:50:28\n",
      "Models analyzed: 20250923_143004 timestamp (RNN architecture)\n",
      "Perfect environment compatibility: 3 actions supported\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive summary for target timestamp models\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE ABLATION ANALYSIS SUMMARY\")\n",
    "print(f\"Target Models: {config.target_timestamp}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create summary dataframe\n",
    "summary_results = []\n",
    "\n",
    "# Main models summary\n",
    "if 'global' in main_results:\n",
    "    summary_results.append([\n",
    "        'A3C Global',\n",
    "        main_results['global']['mean_reward'],\n",
    "        main_results['global']['std_reward'],\n",
    "        len(main_results['global']['all_rewards']),\n",
    "        config.target_timestamp,\n",
    "        'RNN'\n",
    "    ])\n",
    "\n",
    "if individual_results:\n",
    "    individual_means = [results['mean_reward'] for results in individual_results.values()]\n",
    "    individual_stds = [results['std_reward'] for results in individual_results.values()]\n",
    "    \n",
    "    summary_results.append([\n",
    "        'Individual Workers (Best)',\n",
    "        max(individual_means),\n",
    "        individual_stds[np.argmax(individual_means)],\n",
    "        config.evaluation_episodes,\n",
    "        config.target_timestamp,\n",
    "        'RNN'\n",
    "    ])\n",
    "    \n",
    "    summary_results.append([\n",
    "        'Individual Workers (Avg)',\n",
    "        np.mean(individual_means),\n",
    "        np.mean(individual_stds),\n",
    "        config.evaluation_episodes,\n",
    "        config.target_timestamp,\n",
    "        'RNN'\n",
    "    ])\n",
    "    \n",
    "    summary_results.append([\n",
    "        'Individual Workers (Worst)',\n",
    "        min(individual_means),\n",
    "        individual_stds[np.argmin(individual_means)],\n",
    "        config.evaluation_episodes,\n",
    "        config.target_timestamp,\n",
    "        'RNN'\n",
    "    ])\n",
    "\n",
    "# Create and display summary table\n",
    "if summary_results:\n",
    "    df_summary = pd.DataFrame(summary_results, \n",
    "                             columns=['Model Type', 'Mean Reward', 'Std Reward', 'Episodes', 'Timestamp', 'Architecture'])\n",
    "    \n",
    "    print(\"\\\\n=== PERFORMANCE SUMMARY ===\")\n",
    "    print(df_summary.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Key findings\n",
    "print(\"\\\\n=== KEY FINDINGS ===\")\n",
    "\n",
    "findings = []\n",
    "\n",
    "# Global vs Individual comparison\n",
    "if 'global' in main_results and individual_results:\n",
    "    global_mean = main_results['global']['mean_reward']\n",
    "    individual_mean = np.mean([results['mean_reward'] for results in individual_results.values()])\n",
    "    \n",
    "    if global_mean > individual_mean:\n",
    "        findings.append(f\"🏆 A3C Global model outperforms individual workers by {global_mean - individual_mean:.3f} reward points\")\n",
    "    else:\n",
    "        findings.append(f\"🔄 Individual workers outperform A3C global model by {individual_mean - global_mean:.3f} reward points\")\n",
    "\n",
    "# Worker variability\n",
    "if individual_results and len(individual_results) > 1:\n",
    "    individual_means = [results['mean_reward'] for results in individual_results.values()]\n",
    "    variability = np.std(individual_means)\n",
    "    performance_gap = max(individual_means) - min(individual_means)\n",
    "    \n",
    "    findings.append(f\"📊 Worker performance variability: σ={variability:.3f}, gap={performance_gap:.3f}\")\n",
    "    \n",
    "    if variability > 0.1:\n",
    "        findings.append(\"⚠️  High variability detected between individual workers\")\n",
    "    else:\n",
    "        findings.append(\"✅ Consistent performance across individual workers\")\n",
    "\n",
    "# Action distribution analysis\n",
    "if 'global' in main_results:\n",
    "    global_actions = main_results['global']['action_distribution']\n",
    "    total_global = sum(global_actions.values())\n",
    "    if total_global > 0:\n",
    "        action_probs = [global_actions.get(i, 0) / total_global for i in range(3)]\n",
    "        dominant_action = ['LOCAL', 'OFFLOAD', 'DISCARD'][np.argmax(action_probs)]\n",
    "        findings.append(f\"🎯 A3C Global model prefers {dominant_action} action ({max(action_probs):.1%} of time)\")\n",
    "\n",
    "if individual_results:\n",
    "    # Analyze individual worker action preferences\n",
    "    action_preferences = {}\n",
    "    for worker, results in individual_results.items():\n",
    "        actions = results['action_distribution']\n",
    "        total = sum(actions.values())\n",
    "        if total > 0:\n",
    "            prefs = [actions.get(i, 0) / total for i in range(3)]\n",
    "            dominant = np.argmax(prefs)\n",
    "            action_preferences[dominant] = action_preferences.get(dominant, 0) + 1\n",
    "    \n",
    "    if action_preferences:\n",
    "        most_common_pref = max(action_preferences, key=action_preferences.get)\n",
    "        action_name = ['LOCAL', 'OFFLOAD', 'DISCARD'][most_common_pref]\n",
    "        count = action_preferences[most_common_pref]\n",
    "        findings.append(f\"👥 {count}/{len(individual_results)} individual workers prefer {action_name} action\")\n",
    "\n",
    "# Architecture insights\n",
    "findings.append(f\"🏗️  All models use RNN architecture with {config.state_dim}D state and {config.action_dim} actions\")\n",
    "findings.append(f\"✅ Perfect environment compatibility (no action mapping needed)\")\n",
    "\n",
    "# Display findings\n",
    "for i, finding in enumerate(findings, 1):\n",
    "    print(f\"{i}. {finding}\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\\\n=== RECOMMENDATIONS ===\")\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "if 'global' in main_results and individual_results:\n",
    "    global_mean = main_results['global']['mean_reward']\n",
    "    individual_mean = np.mean([results['mean_reward'] for results in individual_results.values()])\n",
    "    \n",
    "    if global_mean > individual_mean:\n",
    "        recommendations.append(\"Deploy A3C global model for production as it shows superior performance\")\n",
    "        recommendations.append(\"Consider knowledge distillation from global to individual workers\")\n",
    "    else:\n",
    "        recommendations.append(\"Individual training may be more effective for this UAV environment\")\n",
    "        recommendations.append(\"Investigate A3C hyperparameters and shared learning dynamics\")\n",
    "\n",
    "if individual_results and len(individual_results) > 1:\n",
    "    variability = np.std([results['mean_reward'] for results in individual_results.values()])\n",
    "    if variability > 0.1:\n",
    "        recommendations.append(\"Investigate sources of worker variability (initialization, exploration, data)\")\n",
    "        recommendations.append(\"Consider worker-specific hyperparameter optimization\")\n",
    "    else:\n",
    "        recommendations.append(\"Workers show consistent performance - current training is stable\")\n",
    "\n",
    "# Action distribution recommendations\n",
    "if 'global' in main_results:\n",
    "    global_actions = main_results['global']['action_distribution']\n",
    "    total_global = sum(global_actions.values())\n",
    "    if total_global > 0:\n",
    "        discard_ratio = global_actions.get(2, 0) / total_global\n",
    "        if discard_ratio > 0.3:\n",
    "            recommendations.append(\"High DISCARD usage suggests need for environment/reward tuning\")\n",
    "        elif discard_ratio < 0.05:\n",
    "            recommendations.append(\"Low DISCARD usage indicates effective task processing\")\n",
    "\n",
    "recommendations.append(f\"Continue monitoring model performance using {config.target_timestamp} architecture\")\n",
    "recommendations.append(\"Consider ensemble methods combining best-performing individual workers\")\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(f\"Analysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Models analyzed: {config.target_timestamp} timestamp (RNN architecture)\")\n",
    "print(f\"Perfect environment compatibility: {config.action_dim} actions supported\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results to save - no models were successfully evaluated\n",
      "\\nTroubleshooting:\n",
      "1. Check if timestamp '20250923_143004' exists in runs/ directory\n",
      "2. Verify model files exist in the expected locations:\n",
      "   - A3C Global: runs/a3c_20250923_143004/models/global_final.pth\n",
      "   - Individual: runs/individual_20250923_143004/models/individual_worker_*_final.pth\n",
      "3. Check model compatibility (RNN architecture with correct dimensions)\n",
      "\\n=== TIMESTAMP CONFIGURATION ===\n",
      "To analyze different timestamp models, update the config:\n",
      "config = Config(target_timestamp='YYYYMMDD_HHMMSS')\n",
      "Then re-run the model discovery and evaluation cells.\n",
      "Current timestamp: 20250923_143004\n",
      "Available timestamps in runs/: ['20250814_021635', '20250814_023301', '20250814_024420', '20250814_030952', '20250814_120549', '20250814_120637', '20250814_120734', '20250814_132542', '20250814_132823', '20250815_024123', '20250815_031631']\n"
     ]
    }
   ],
   "source": [
    "# Save results to CSV for further analysis\n",
    "if summary_results:\n",
    "    df_results = pd.DataFrame(summary_results, \n",
    "                             columns=['Model Type', 'Mean Reward', 'Std Reward', 'Episodes', 'Timestamp', 'Architecture'])\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    results_file = f'ablation_analysis_{config.target_timestamp}_{timestamp}.csv'\n",
    "    df_results.to_csv(results_file, index=False)\n",
    "    print(f\"Results saved to: {results_file}\")\n",
    "    \n",
    "    # Display final results table\n",
    "    print(\"\\\\n=== FINAL RESULTS TABLE ===\")\n",
    "    print(df_results.round(3))\n",
    "    \n",
    "    # Additional analysis summary\n",
    "    print(f\"\\\\n=== ANALYSIS SCOPE ===\")\n",
    "    print(f\"Target timestamp: {config.target_timestamp}\")\n",
    "    print(f\"A3C Global models found: {len(models_info['global_models'])}\")\n",
    "    print(f\"Individual worker models found: {len(models_info['individual_models'])}\")\n",
    "    print(f\"Successfully evaluated models: {len(main_results) + len(individual_results)}\")\n",
    "    print(f\"Model architecture: RNN (RecurrentActorCritic)\")\n",
    "    print(f\"State dimension: {config.state_dim}\")\n",
    "    print(f\"Action dimension: {config.action_dim} (LOCAL, OFFLOAD, DISCARD)\")\n",
    "    print(f\"Environment compatibility: ✅ Perfect match\")\n",
    "    \n",
    "    # Model path summary\n",
    "    print(f\"\\\\n=== MODEL LOCATIONS ===\")\n",
    "    if models_info['global_models']:\n",
    "        for name, path in models_info['global_models'].items():\n",
    "            print(f\"Global: {path}\")\n",
    "    if models_info['individual_models']:\n",
    "        print(f\"Individual workers: {len(models_info['individual_models'])} models in runs/individual_{config.target_timestamp}/models/\")\n",
    "        \n",
    "else:\n",
    "    print(\"No results to save - no models were successfully evaluated\")\n",
    "    print(\"\\\\nTroubleshooting:\")\n",
    "    print(f\"1. Check if timestamp '{config.target_timestamp}' exists in runs/ directory\")\n",
    "    print(\"2. Verify model files exist in the expected locations:\")\n",
    "    print(f\"   - A3C Global: runs/a3c_{config.target_timestamp}/models/global_final.pth\")\n",
    "    print(f\"   - Individual: runs/individual_{config.target_timestamp}/models/individual_worker_*_final.pth\")\n",
    "    print(\"3. Check model compatibility (RNN architecture with correct dimensions)\")\n",
    "\n",
    "# Function to easily change timestamp for different analyses\n",
    "print(f\"\\\\n=== TIMESTAMP CONFIGURATION ===\")\n",
    "print(\"To analyze different timestamp models, update the config:\")\n",
    "print(\"config = Config(target_timestamp='YYYYMMDD_HHMMSS')\")\n",
    "print(\"Then re-run the model discovery and evaluation cells.\")\n",
    "print(f\"Current timestamp: {config.target_timestamp}\")\n",
    "\n",
    "available_timestamps = []\n",
    "runs_path = Path('runs')\n",
    "if runs_path.exists():\n",
    "    for item in runs_path.iterdir():\n",
    "        if item.is_dir() and ('_' in item.name):\n",
    "            timestamp = item.name.split('_', 1)[1]\n",
    "            if timestamp not in available_timestamps:\n",
    "                available_timestamps.append(timestamp)\n",
    "\n",
    "if available_timestamps:\n",
    "    print(f\"Available timestamps in runs/: {sorted(available_timestamps)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

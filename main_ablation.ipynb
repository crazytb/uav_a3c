{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UAV A3C Model Ablation Analysis\n",
    "\n",
    "This notebook provides comprehensive ablation analysis for UAV A3C models including:\n",
    "1. Global vs Individual model comparison\n",
    "2. Worker variability analysis\n",
    "3. Model structure and parameter analysis\n",
    "4. Temporal model performance evolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "PyTorch version: 2.2.1+cu121\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# Custom imports\n",
    "from drl_framework.networks import ActorCritic, RecurrentActorCritic\n",
    "from drl_framework.custom_env import CustomEnv\n",
    "from drl_framework.params import ENV_PARAMS, device\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded: {'main_models_dir': 'models', 'runs_dir': 'runs', 'state_dim': 7, 'action_dim': 3, 'hidden_dim': 128, 'n_workers': 10, 'evaluation_episodes': 100}\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.main_models_dir = \"models\"\n",
    "        self.runs_dir = \"runs\"\n",
    "        self.state_dim = 7  # Based on UAV environment\n",
    "        self.action_dim = 3  # LOCAL, OFFLOAD, DISCARD (updated to match environment)\n",
    "        self.hidden_dim = 128\n",
    "        self.n_workers = 10  # Number of individual workers\n",
    "        self.evaluation_episodes = 100\n",
    "        \n",
    "config = Config()\n",
    "print(f\"Configuration loaded: {vars(config)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Discovery and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Available Models ===\n",
      "Main models: ['individual_worker_0_final', 'individual_worker_5_final', 'individual_worker_6_final', 'individual_worker_9_final', 'individual_worker_3_final', 'global_final', 'individual_worker_4_final', 'individual_worker_2_final', 'individual_worker_1_final', 'individual_worker_7_final', 'individual_worker_8_final']\n",
      "Run directories: 23\n",
      "  individual_20250815_151722: ['individual_worker_0_final', 'individual_worker_3_final', 'individual_worker_4_final', 'individual_worker_2_final', 'individual_worker_1_final']\n",
      "  individual_20250819_175326: ['individual_worker_0_final', 'individual_worker_3_final', 'individual_worker_4_final', 'individual_worker_2_final', 'individual_worker_1_final']\n",
      "  a3c_20250815_161957: ['global_final']\n"
     ]
    }
   ],
   "source": [
    "def discover_models():\n",
    "    \"\"\"Discover all available models in the project\"\"\"\n",
    "    models_info = {\n",
    "        'main_models': {},\n",
    "        'run_models': {}\n",
    "    }\n",
    "    \n",
    "    # Main models directory\n",
    "    main_dir = Path(config.main_models_dir)\n",
    "    if main_dir.exists():\n",
    "        for pth_file in main_dir.glob(\"*.pth\"):\n",
    "            models_info['main_models'][pth_file.stem] = str(pth_file)\n",
    "    \n",
    "    # Run directories\n",
    "    runs_dir = Path(config.runs_dir)\n",
    "    if runs_dir.exists():\n",
    "        for run_dir in runs_dir.iterdir():\n",
    "            if run_dir.is_dir():\n",
    "                models_dir = run_dir / \"models\"\n",
    "                if models_dir.exists():\n",
    "                    run_models = {}\n",
    "                    for pth_file in models_dir.glob(\"*.pth\"):\n",
    "                        run_models[pth_file.stem] = str(pth_file)\n",
    "                    if run_models:\n",
    "                        models_info['run_models'][run_dir.name] = run_models\n",
    "    \n",
    "    return models_info\n",
    "\n",
    "def load_model(model_path, model_type='standard'):\n",
    "    \"\"\"Load a model from path\"\"\"\n",
    "    try:\n",
    "        # Determine model architecture\n",
    "        if model_type == 'recurrent':\n",
    "            model = RecurrentActorCritic(\n",
    "                state_dim=config.state_dim,\n",
    "                action_dim=config.action_dim,\n",
    "                hidden_dim=config.hidden_dim\n",
    "            )\n",
    "        else:\n",
    "            model = ActorCritic(\n",
    "                state_dim=config.state_dim,\n",
    "                action_dim=config.action_dim,\n",
    "                hidden_dim=config.hidden_dim\n",
    "            )\n",
    "        \n",
    "        # Load state dict\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        \n",
    "        # Handle different checkpoint formats\n",
    "        if isinstance(checkpoint, dict):\n",
    "            if 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            elif 'state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['state_dict'])\n",
    "            else:\n",
    "                model.load_state_dict(checkpoint)\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "        \n",
    "        model.eval()\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Discover all models\n",
    "models_info = discover_models()\n",
    "print(\"\\n=== Available Models ===\")\n",
    "print(f\"Main models: {list(models_info['main_models'].keys())}\")\n",
    "print(f\"Run directories: {len(models_info['run_models'])}\")\n",
    "for run_name, run_models in list(models_info['run_models'].items())[:3]:  # Show first 3\n",
    "    print(f\"  {run_name}: {list(run_models.keys())[:5]}{'...' if len(run_models) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized with state_dim: None, action_dim: 3\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, env, episodes=100, deterministic=True):\n",
    "    \"\"\"Evaluate a model's performance\"\"\"\n",
    "    total_rewards = []\n",
    "    episode_lengths = []\n",
    "    action_counts = {0: 0, 1: 0, 2: 0}  # LOCAL, OFFLOAD, DISCARD (3 actions based on environment)\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        done = False\n",
    "        \n",
    "        # Initialize hidden state for RNN models\n",
    "        if hasattr(model, 'init_hidden'):\n",
    "            hidden = model.init_hidden(1, device)\n",
    "        else:\n",
    "            hidden = None\n",
    "        \n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                if hidden is not None:\n",
    "                    # RNN model\n",
    "                    if hasattr(model, 'act'):\n",
    "                        action, _, _, hidden = model.act(state_tensor, hidden)\n",
    "                        action = action.item()\n",
    "                    else:\n",
    "                        logits, _, hidden = model.step(state_tensor, hidden)\n",
    "                        if deterministic:\n",
    "                            action = torch.argmax(logits, dim=1).item()\n",
    "                        else:\n",
    "                            dist = torch.distributions.Categorical(logits=logits)\n",
    "                            action = dist.sample().item()\n",
    "                else:\n",
    "                    # Standard model\n",
    "                    logits, _ = model(state_tensor)\n",
    "                    if deterministic:\n",
    "                        action = torch.argmax(logits, dim=1).item()\n",
    "                    else:\n",
    "                        dist = torch.distributions.Categorical(logits=logits)\n",
    "                        action = dist.sample().item()\n",
    "            \n",
    "            # Ensure action is within valid range\n",
    "            action = min(action, env.action_space.n - 1)\n",
    "            action_counts[action] = action_counts.get(action, 0) + 1\n",
    "            \n",
    "            state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "            \n",
    "            if episode_length > 1000:  # Prevent infinite loops\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "        episode_lengths.append(episode_length)\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': np.mean(total_rewards),\n",
    "        'std_reward': np.std(total_rewards),\n",
    "        'min_reward': np.min(total_rewards),\n",
    "        'max_reward': np.max(total_rewards),\n",
    "        'mean_length': np.mean(episode_lengths),\n",
    "        'std_length': np.std(episode_lengths),\n",
    "        'action_distribution': action_counts,\n",
    "        'all_rewards': total_rewards,\n",
    "        'all_lengths': episode_lengths\n",
    "    }\n",
    "\n",
    "# Initialize environment\n",
    "env = CustomEnv(\n",
    "    max_comp_units=ENV_PARAMS['max_comp_units'],\n",
    "    max_epoch_size=ENV_PARAMS['max_epoch_size'], \n",
    "    max_queue_size=ENV_PARAMS['max_queue_size'],\n",
    "    max_comp_units_for_cloud=ENV_PARAMS['max_comp_units_for_cloud'],\n",
    "    reward_weights=ENV_PARAMS['reward_weights'],\n",
    "    agent_velocities=ENV_PARAMS['agent_velocities']\n",
    ")\n",
    "print(f\"Environment initialized with state_dim: {env.observation_space.shape}, action_dim: {env.action_space.n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Global vs Individual Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading and Evaluating Main Models ===\n",
      "Loading global model from: models/global_final.pth\n",
      "Error loading model from models/global_final.pth: Error(s) in loading state_dict for ActorCritic:\n",
      "\tsize mismatch for shared.0.weight: copying a param with shape torch.Size([128, 45]) from checkpoint, the shape in current model is torch.Size([128, 7]).\n",
      "\tsize mismatch for policy.weight: copying a param with shape torch.Size([2, 128]) from checkpoint, the shape in current model is torch.Size([3, 128]).\n",
      "\tsize mismatch for policy.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([3]).\n",
      "Loading individual worker 0 from: models/individual_worker_0_final.pth\n",
      "Error loading model from models/individual_worker_0_final.pth: Error(s) in loading state_dict for ActorCritic:\n",
      "\tsize mismatch for shared.0.weight: copying a param with shape torch.Size([128, 45]) from checkpoint, the shape in current model is torch.Size([128, 7]).\n",
      "\tsize mismatch for policy.weight: copying a param with shape torch.Size([2, 128]) from checkpoint, the shape in current model is torch.Size([3, 128]).\n",
      "\tsize mismatch for policy.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([3]).\n",
      "Loading individual worker 5 from: models/individual_worker_5_final.pth\n",
      "Error loading model from models/individual_worker_5_final.pth: Error(s) in loading state_dict for ActorCritic:\n",
      "\tsize mismatch for shared.0.weight: copying a param with shape torch.Size([128, 45]) from checkpoint, the shape in current model is torch.Size([128, 7]).\n",
      "\tsize mismatch for policy.weight: copying a param with shape torch.Size([2, 128]) from checkpoint, the shape in current model is torch.Size([3, 128]).\n",
      "\tsize mismatch for policy.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([3]).\n",
      "Loading individual worker 6 from: models/individual_worker_6_final.pth\n",
      "Error loading model from models/individual_worker_6_final.pth: Error(s) in loading state_dict for ActorCritic:\n",
      "\tsize mismatch for shared.0.weight: copying a param with shape torch.Size([128, 45]) from checkpoint, the shape in current model is torch.Size([128, 7]).\n",
      "\tsize mismatch for policy.weight: copying a param with shape torch.Size([2, 128]) from checkpoint, the shape in current model is torch.Size([3, 128]).\n",
      "\tsize mismatch for policy.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([3]).\n",
      "Loading individual worker 9 from: models/individual_worker_9_final.pth\n",
      "Error loading model from models/individual_worker_9_final.pth: Error(s) in loading state_dict for ActorCritic:\n",
      "\tsize mismatch for shared.0.weight: copying a param with shape torch.Size([128, 45]) from checkpoint, the shape in current model is torch.Size([128, 7]).\n",
      "\tsize mismatch for policy.weight: copying a param with shape torch.Size([2, 128]) from checkpoint, the shape in current model is torch.Size([3, 128]).\n",
      "\tsize mismatch for policy.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([3]).\n",
      "Loading individual worker 3 from: models/individual_worker_3_final.pth\n",
      "Error loading model from models/individual_worker_3_final.pth: Error(s) in loading state_dict for ActorCritic:\n",
      "\tsize mismatch for shared.0.weight: copying a param with shape torch.Size([128, 45]) from checkpoint, the shape in current model is torch.Size([128, 7]).\n",
      "\tsize mismatch for policy.weight: copying a param with shape torch.Size([2, 128]) from checkpoint, the shape in current model is torch.Size([3, 128]).\n",
      "\tsize mismatch for policy.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([3]).\n",
      "Loading individual worker 4 from: models/individual_worker_4_final.pth\n",
      "Error loading model from models/individual_worker_4_final.pth: Error(s) in loading state_dict for ActorCritic:\n",
      "\tsize mismatch for shared.0.weight: copying a param with shape torch.Size([128, 45]) from checkpoint, the shape in current model is torch.Size([128, 7]).\n",
      "\tsize mismatch for policy.weight: copying a param with shape torch.Size([2, 128]) from checkpoint, the shape in current model is torch.Size([3, 128]).\n",
      "\tsize mismatch for policy.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([3]).\n",
      "Loading individual worker 2 from: models/individual_worker_2_final.pth\n",
      "Error loading model from models/individual_worker_2_final.pth: Error(s) in loading state_dict for ActorCritic:\n",
      "\tsize mismatch for shared.0.weight: copying a param with shape torch.Size([128, 45]) from checkpoint, the shape in current model is torch.Size([128, 7]).\n",
      "\tsize mismatch for policy.weight: copying a param with shape torch.Size([2, 128]) from checkpoint, the shape in current model is torch.Size([3, 128]).\n",
      "\tsize mismatch for policy.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([3]).\n",
      "Loading individual worker 1 from: models/individual_worker_1_final.pth\n",
      "Error loading model from models/individual_worker_1_final.pth: Error(s) in loading state_dict for ActorCritic:\n",
      "\tsize mismatch for shared.0.weight: copying a param with shape torch.Size([128, 45]) from checkpoint, the shape in current model is torch.Size([128, 7]).\n",
      "\tsize mismatch for policy.weight: copying a param with shape torch.Size([2, 128]) from checkpoint, the shape in current model is torch.Size([3, 128]).\n",
      "\tsize mismatch for policy.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([3]).\n",
      "Loading individual worker 7 from: models/individual_worker_7_final.pth\n",
      "Error loading model from models/individual_worker_7_final.pth: Error(s) in loading state_dict for ActorCritic:\n",
      "\tsize mismatch for shared.0.weight: copying a param with shape torch.Size([128, 45]) from checkpoint, the shape in current model is torch.Size([128, 7]).\n",
      "\tsize mismatch for policy.weight: copying a param with shape torch.Size([2, 128]) from checkpoint, the shape in current model is torch.Size([3, 128]).\n",
      "\tsize mismatch for policy.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([3]).\n",
      "Loading individual worker 8 from: models/individual_worker_8_final.pth\n",
      "Error loading model from models/individual_worker_8_final.pth: Error(s) in loading state_dict for ActorCritic:\n",
      "\tsize mismatch for shared.0.weight: copying a param with shape torch.Size([128, 45]) from checkpoint, the shape in current model is torch.Size([128, 7]).\n",
      "\tsize mismatch for policy.weight: copying a param with shape torch.Size([2, 128]) from checkpoint, the shape in current model is torch.Size([3, 128]).\n",
      "\tsize mismatch for policy.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([3]).\n",
      "\n",
      "Loaded 0 individual worker models\n"
     ]
    }
   ],
   "source": [
    "# Load and evaluate main models\n",
    "print(\"=== Loading and Evaluating Main Models ===\")\n",
    "main_results = {}\n",
    "\n",
    "# Load global model\n",
    "if 'global_final' in models_info['main_models']:\n",
    "    global_path = models_info['main_models']['global_final']\n",
    "    print(f\"Loading global model from: {global_path}\")\n",
    "    global_model = load_model(global_path)\n",
    "    if global_model:\n",
    "        print(\"Evaluating global model...\")\n",
    "        main_results['global'] = evaluate_model(global_model, env, config.evaluation_episodes)\n",
    "        print(f\"Global model - Mean reward: {main_results['global']['mean_reward']:.2f} ± {main_results['global']['std_reward']:.2f}\")\n",
    "\n",
    "# Load individual worker models\n",
    "individual_results = {}\n",
    "for model_name, model_path in models_info['main_models'].items():\n",
    "    if model_name.startswith('individual_worker_'):\n",
    "        worker_id = model_name.split('_')[-2]  # Extract worker ID\n",
    "        print(f\"Loading individual worker {worker_id} from: {model_path}\")\n",
    "        worker_model = load_model(model_path)\n",
    "        if worker_model:\n",
    "            print(f\"Evaluating worker {worker_id}...\")\n",
    "            individual_results[f'worker_{worker_id}'] = evaluate_model(worker_model, env, config.evaluation_episodes)\n",
    "            print(f\"Worker {worker_id} - Mean reward: {individual_results[f'worker_{worker_id}']['mean_reward']:.2f}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(individual_results)} individual worker models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to perform comparison - missing global or individual models\n"
     ]
    }
   ],
   "source": [
    "# Statistical comparison between global and individual models\n",
    "if 'global' in main_results and individual_results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Reward comparison\n",
    "    global_rewards = main_results['global']['all_rewards']\n",
    "    individual_rewards_all = []\n",
    "    worker_labels = []\n",
    "    \n",
    "    for worker, results in individual_results.items():\n",
    "        individual_rewards_all.extend(results['all_rewards'])\n",
    "        worker_labels.extend([worker] * len(results['all_rewards']))\n",
    "    \n",
    "    # Box plot comparison\n",
    "    axes[0, 0].boxplot([global_rewards, individual_rewards_all], \n",
    "                       labels=['Global', 'Individual (All)'])\n",
    "    axes[0, 0].set_title('Reward Distribution: Global vs Individual')\n",
    "    axes[0, 0].set_ylabel('Episode Reward')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Individual worker comparison\n",
    "    worker_rewards = [results['all_rewards'] for results in individual_results.values()]\n",
    "    worker_names = list(individual_results.keys())\n",
    "    \n",
    "    bp = axes[0, 1].boxplot(worker_rewards, labels=worker_names)\n",
    "    axes[0, 1].set_title('Reward Distribution Across Individual Workers')\n",
    "    axes[0, 1].set_ylabel('Episode Reward')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Action distribution comparison\n",
    "    action_names = ['LOCAL', 'OFFLOAD', 'DISCARD']  # Updated to match 3 actions\n",
    "    global_actions = [main_results['global']['action_distribution'].get(i, 0) for i in range(3)]\n",
    "    global_actions_norm = np.array(global_actions) / np.sum(global_actions) if np.sum(global_actions) > 0 else np.zeros(3)\n",
    "    \n",
    "    # Average individual action distribution\n",
    "    individual_actions = np.zeros(3)\n",
    "    for results in individual_results.values():\n",
    "        for i in range(3):\n",
    "            individual_actions[i] += results['action_distribution'].get(i, 0)\n",
    "    individual_actions_norm = individual_actions / np.sum(individual_actions) if np.sum(individual_actions) > 0 else np.zeros(3)\n",
    "    \n",
    "    x = np.arange(len(action_names))\n",
    "    width = 0.35\n",
    "    axes[1, 0].bar(x - width/2, global_actions_norm, width, label='Global', alpha=0.8)\n",
    "    axes[1, 0].bar(x + width/2, individual_actions_norm, width, label='Individual (Avg)', alpha=0.8)\n",
    "    axes[1, 0].set_title('Action Distribution Comparison')\n",
    "    axes[1, 0].set_ylabel('Probability')\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels(action_names)\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Performance summary\n",
    "    summary_data = []\n",
    "    summary_data.append(['Global', main_results['global']['mean_reward'], main_results['global']['std_reward']])\n",
    "    \n",
    "    for worker, results in individual_results.items():\n",
    "        summary_data.append([worker, results['mean_reward'], results['std_reward']])\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_data, columns=['Model', 'Mean Reward', 'Std Reward'])\n",
    "    axes[1, 1].axis('tight')\n",
    "    axes[1, 1].axis('off')\n",
    "    table = axes[1, 1].table(cellText=[[f\"{row[0]}\", f\"{row[1]:.2f}\", f\"{row[2]:.2f}\"] for row in summary_data],\n",
    "                            colLabels=['Model', 'Mean Reward', 'Std Reward'],\n",
    "                            cellLoc='center',\n",
    "                            loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.2, 1.5)\n",
    "    axes[1, 1].set_title('Performance Summary')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical tests\n",
    "    print(\"\\n=== Statistical Analysis ===\")\n",
    "    global_mean = main_results['global']['mean_reward']\n",
    "    individual_means = [results['mean_reward'] for results in individual_results.values()]\n",
    "    individual_mean = np.mean(individual_means)\n",
    "    \n",
    "    print(f\"Global model mean reward: {global_mean:.3f} ± {main_results['global']['std_reward']:.3f}\")\n",
    "    print(f\"Individual models mean reward: {individual_mean:.3f} ± {np.std(individual_means):.3f}\")\n",
    "    print(f\"Performance difference: {global_mean - individual_mean:.3f}\")\n",
    "    \n",
    "    # t-test between global and pooled individual rewards\n",
    "    t_stat, p_value = stats.ttest_ind(global_rewards, individual_rewards_all)\n",
    "    print(f\"T-test (Global vs Individual): t={t_stat:.3f}, p={p_value:.3f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"Significant difference detected between global and individual models\")\n",
    "    else:\n",
    "        print(\"No significant difference between global and individual models\")\n",
    "    \n",
    "else:\n",
    "    print(\"Unable to perform comparison - missing global or individual models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Worker Variability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No individual worker models found for variability analysis\n"
     ]
    }
   ],
   "source": [
    "if individual_results:\n",
    "    print(\"=== Worker Variability Analysis ===\")\n",
    "    \n",
    "    # Create comprehensive worker analysis\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Worker performance ranking\n",
    "    worker_performance = [(worker, results['mean_reward'], results['std_reward']) \n",
    "                         for worker, results in individual_results.items()]\n",
    "    worker_performance.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    workers, means, stds = zip(*worker_performance)\n",
    "    x_pos = np.arange(len(workers))\n",
    "    \n",
    "    bars = axes[0, 0].bar(x_pos, means, yerr=stds, capsize=5, alpha=0.7)\n",
    "    axes[0, 0].set_title('Worker Performance Ranking')\n",
    "    axes[0, 0].set_ylabel('Mean Episode Reward')\n",
    "    axes[0, 0].set_xticks(x_pos)\n",
    "    axes[0, 0].set_xticklabels(workers, rotation=45)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Color bars by performance\n",
    "    colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(bars)))\n",
    "    for bar, color in zip(bars, colors):\n",
    "        bar.set_color(color)\n",
    "    \n",
    "    # 2. Worker consistency (coefficient of variation)\n",
    "    cvs = [results['std_reward'] / abs(results['mean_reward']) if results['mean_reward'] != 0 else 0 \n",
    "           for results in individual_results.values()]\n",
    "    worker_names = list(individual_results.keys())\n",
    "    \n",
    "    axes[0, 1].bar(range(len(worker_names)), cvs, alpha=0.7)\n",
    "    axes[0, 1].set_title('Worker Consistency (Lower = More Consistent)')\n",
    "    axes[0, 1].set_ylabel('Coefficient of Variation')\n",
    "    axes[0, 1].set_xticks(range(len(worker_names)))\n",
    "    axes[0, 1].set_xticklabels(worker_names, rotation=45)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Action distribution heatmap\n",
    "    action_matrix = []\n",
    "    for worker in worker_names:\n",
    "        actions = individual_results[worker]['action_distribution']\n",
    "        total_actions = sum(actions.values())\n",
    "        action_probs = [actions.get(i, 0) / total_actions if total_actions > 0 else 0 for i in range(3)]\n",
    "        action_matrix.append(action_probs)\n",
    "    \n",
    "    im = axes[0, 2].imshow(action_matrix, cmap='YlOrRd', aspect='auto')\n",
    "    axes[0, 2].set_title('Action Distribution Across Workers')\n",
    "    axes[0, 2].set_xlabel('Actions')\n",
    "    axes[0, 2].set_ylabel('Workers')\n",
    "    axes[0, 2].set_xticks(range(3))\n",
    "    axes[0, 2].set_xticklabels(['LOCAL', 'OFFLOAD', 'DISCARD'])\n",
    "    axes[0, 2].set_yticks(range(len(worker_names)))\n",
    "    axes[0, 2].set_yticklabels(worker_names)\n",
    "    plt.colorbar(im, ax=axes[0, 2], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # 4. Reward distribution violin plot\n",
    "    all_worker_rewards = [individual_results[worker]['all_rewards'] for worker in worker_names]\n",
    "    parts = axes[1, 0].violinplot(all_worker_rewards, positions=range(len(worker_names)), showmeans=True)\n",
    "    axes[1, 0].set_title('Reward Distribution Shape by Worker')\n",
    "    axes[1, 0].set_ylabel('Episode Reward')\n",
    "    axes[1, 0].set_xticks(range(len(worker_names)))\n",
    "    axes[1, 0].set_xticklabels(worker_names, rotation=45)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Episode length analysis\n",
    "    episode_lengths = [individual_results[worker]['mean_length'] for worker in worker_names]\n",
    "    length_stds = [individual_results[worker]['std_length'] for worker in worker_names]\n",
    "    \n",
    "    axes[1, 1].bar(range(len(worker_names)), episode_lengths, yerr=length_stds, \n",
    "                   capsize=5, alpha=0.7, color='skyblue')\n",
    "    axes[1, 1].set_title('Episode Length by Worker')\n",
    "    axes[1, 1].set_ylabel('Mean Episode Length')\n",
    "    axes[1, 1].set_xticks(range(len(worker_names)))\n",
    "    axes[1, 1].set_xticklabels(worker_names, rotation=45)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Worker correlation matrix\n",
    "    if len(individual_results) > 1:\n",
    "        # Create correlation matrix based on episode rewards\n",
    "        reward_matrix = np.array([individual_results[worker]['all_rewards'] for worker in worker_names])\n",
    "        correlation_matrix = np.corrcoef(reward_matrix)\n",
    "        \n",
    "        im2 = axes[1, 2].imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        axes[1, 2].set_title('Worker Performance Correlation')\n",
    "        axes[1, 2].set_xticks(range(len(worker_names)))\n",
    "        axes[1, 2].set_xticklabels(worker_names, rotation=45)\n",
    "        axes[1, 2].set_yticks(range(len(worker_names)))\n",
    "        axes[1, 2].set_yticklabels(worker_names)\n",
    "        plt.colorbar(im2, ax=axes[1, 2], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Add correlation values to heatmap\n",
    "        for i in range(len(worker_names)):\n",
    "            for j in range(len(worker_names)):\n",
    "                axes[1, 2].text(j, i, f'{correlation_matrix[i, j]:.2f}', \n",
    "                               ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical analysis of worker variability\n",
    "    print(\"\\n=== Worker Variability Statistics ===\")\n",
    "    worker_means = [individual_results[worker]['mean_reward'] for worker in worker_names]\n",
    "    print(f\"Inter-worker mean reward std: {np.std(worker_means):.3f}\")\n",
    "    print(f\"Best worker: {workers[0]} ({means[0]:.3f})\")\n",
    "    print(f\"Worst worker: {workers[-1]} ({means[-1]:.3f})\")\n",
    "    print(f\"Performance gap: {means[0] - means[-1]:.3f}\")\n",
    "    \n",
    "    # ANOVA test for significant differences between workers\n",
    "    worker_reward_lists = [individual_results[worker]['all_rewards'] for worker in worker_names]\n",
    "    f_stat, p_value = stats.f_oneway(*worker_reward_lists)\n",
    "    print(f\"ANOVA F-statistic: {f_stat:.3f}, p-value: {p_value:.3f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"Significant differences detected between workers\")\n",
    "    else:\n",
    "        print(\"No significant differences between workers\")\n",
    "\n",
    "else:\n",
    "    print(\"No individual worker models found for variability analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Structure and Parameter Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_parameters(model, model_name):\n",
    "    \"\"\"Analyze model parameters and structure\"\"\"\n",
    "    param_info = {\n",
    "        'total_params': 0,\n",
    "        'trainable_params': 0,\n",
    "        'layer_info': [],\n",
    "        'param_distribution': {},\n",
    "        'weight_stats': {}\n",
    "    }\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        param_count = param.numel()\n",
    "        param_info['total_params'] += param_count\n",
    "        if param.requires_grad:\n",
    "            param_info['trainable_params'] += param_count\n",
    "        \n",
    "        # Layer information\n",
    "        param_info['layer_info'].append({\n",
    "            'name': name,\n",
    "            'shape': list(param.shape),\n",
    "            'params': param_count,\n",
    "            'requires_grad': param.requires_grad\n",
    "        })\n",
    "        \n",
    "        # Parameter distribution\n",
    "        param_info['param_distribution'][name] = param_count\n",
    "        \n",
    "        # Weight statistics\n",
    "        with torch.no_grad():\n",
    "            param_info['weight_stats'][name] = {\n",
    "                'mean': param.mean().item(),\n",
    "                'std': param.std().item(),\n",
    "                'min': param.min().item(),\n",
    "                'max': param.max().item(),\n",
    "                'norm': param.norm().item()\n",
    "            }\n",
    "    \n",
    "    return param_info\n",
    "\n",
    "def compare_model_weights(model1, model2, model1_name, model2_name):\n",
    "    \"\"\"Compare weights between two models\"\"\"\n",
    "    comparison = {\n",
    "        'layer_similarities': {},\n",
    "        'overall_similarity': 0,\n",
    "        'weight_differences': {}\n",
    "    }\n",
    "    \n",
    "    model1_params = dict(model1.named_parameters())\n",
    "    model2_params = dict(model2.named_parameters())\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    for name in model1_params.keys():\n",
    "        if name in model2_params:\n",
    "            w1 = model1_params[name].detach().flatten().numpy()\n",
    "            w2 = model2_params[name].detach().flatten().numpy()\n",
    "            \n",
    "            # Cosine similarity\n",
    "            similarity = cosine_similarity([w1], [w2])[0, 0]\n",
    "            comparison['layer_similarities'][name] = similarity\n",
    "            similarities.append(similarity)\n",
    "            \n",
    "            # Weight differences\n",
    "            diff = np.abs(w1 - w2)\n",
    "            comparison['weight_differences'][name] = {\n",
    "                'mean_diff': np.mean(diff),\n",
    "                'max_diff': np.max(diff),\n",
    "                'l2_distance': np.linalg.norm(w1 - w2)\n",
    "            }\n",
    "    \n",
    "    comparison['overall_similarity'] = np.mean(similarities)\n",
    "    return comparison\n",
    "\n",
    "# Analyze model structures\n",
    "print(\"=== Model Structure Analysis ===\")\n",
    "\n",
    "# Load a few models for analysis\n",
    "models_to_analyze = {}\n",
    "\n",
    "# Global model\n",
    "if 'global_final' in models_info['main_models']:\n",
    "    global_model = load_model(models_info['main_models']['global_final'])\n",
    "    if global_model:\n",
    "        models_to_analyze['Global'] = global_model\n",
    "\n",
    "# A few individual workers\n",
    "worker_count = 0\n",
    "for model_name, model_path in models_info['main_models'].items():\n",
    "    if model_name.startswith('individual_worker_') and worker_count < 3:\n",
    "        worker_id = model_name.split('_')[-2]\n",
    "        worker_model = load_model(model_path)\n",
    "        if worker_model:\n",
    "            models_to_analyze[f'Worker_{worker_id}'] = worker_model\n",
    "            worker_count += 1\n",
    "\n",
    "# Analyze each model\n",
    "model_analyses = {}\n",
    "for name, model in models_to_analyze.items():\n",
    "    print(f\"\\nAnalyzing {name}...\")\n",
    "    analysis = analyze_model_parameters(model, name)\n",
    "    model_analyses[name] = analysis\n",
    "    print(f\"  Total parameters: {analysis['total_params']:,}\")\n",
    "    print(f\"  Trainable parameters: {analysis['trainable_params']:,}\")\n",
    "\n",
    "# Visualize parameter analysis\n",
    "if model_analyses:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Parameter count comparison\n",
    "    model_names = list(model_analyses.keys())\n",
    "    param_counts = [model_analyses[name]['total_params'] for name in model_names]\n",
    "    \n",
    "    axes[0, 0].bar(model_names, param_counts, alpha=0.7)\n",
    "    axes[0, 0].set_title('Total Parameters by Model')\n",
    "    axes[0, 0].set_ylabel('Parameter Count')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Weight distribution for first model\n",
    "    if model_names:\n",
    "        first_model = model_names[0]\n",
    "        weight_norms = [stats['norm'] for stats in model_analyses[first_model]['weight_stats'].values()]\n",
    "        layer_names = list(model_analyses[first_model]['weight_stats'].keys())\n",
    "        \n",
    "        axes[0, 1].bar(range(len(layer_names)), weight_norms, alpha=0.7)\n",
    "        axes[0, 1].set_title(f'Layer Weight Norms - {first_model}')\n",
    "        axes[0, 1].set_ylabel('L2 Norm')\n",
    "        axes[0, 1].set_xticks(range(len(layer_names)))\n",
    "        axes[0, 1].set_xticklabels([name.split('.')[-1] for name in layer_names], rotation=45)\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Model similarity heatmap (if multiple models)\n",
    "    if len(models_to_analyze) > 1:\n",
    "        similarity_matrix = np.zeros((len(model_names), len(model_names)))\n",
    "        \n",
    "        for i, name1 in enumerate(model_names):\n",
    "            for j, name2 in enumerate(model_names):\n",
    "                if i == j:\n",
    "                    similarity_matrix[i, j] = 1.0\n",
    "                elif i < j:\n",
    "                    comparison = compare_model_weights(\n",
    "                        models_to_analyze[name1], \n",
    "                        models_to_analyze[name2], \n",
    "                        name1, name2\n",
    "                    )\n",
    "                    similarity = comparison['overall_similarity']\n",
    "                    similarity_matrix[i, j] = similarity\n",
    "                    similarity_matrix[j, i] = similarity\n",
    "        \n",
    "        im = axes[1, 0].imshow(similarity_matrix, cmap='RdYlBu', vmin=0, vmax=1)\n",
    "        axes[1, 0].set_title('Model Weight Similarity Matrix')\n",
    "        axes[1, 0].set_xticks(range(len(model_names)))\n",
    "        axes[1, 0].set_xticklabels(model_names, rotation=45)\n",
    "        axes[1, 0].set_yticks(range(len(model_names)))\n",
    "        axes[1, 0].set_yticklabels(model_names)\n",
    "        plt.colorbar(im, ax=axes[1, 0], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Add similarity values\n",
    "        for i in range(len(model_names)):\n",
    "            for j in range(len(model_names)):\n",
    "                axes[1, 0].text(j, i, f'{similarity_matrix[i, j]:.3f}', \n",
    "                               ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # 4. Layer-wise parameter distribution\n",
    "    if model_names:\n",
    "        first_model = model_names[0]\n",
    "        param_dist = model_analyses[first_model]['param_distribution']\n",
    "        \n",
    "        # Create pie chart for parameter distribution\n",
    "        layer_params = list(param_dist.values())\n",
    "        layer_labels = [name.split('.')[-1] for name in param_dist.keys()]\n",
    "        \n",
    "        axes[1, 1].pie(layer_params, labels=layer_labels, autopct='%1.1f%%', startangle=90)\n",
    "        axes[1, 1].set_title(f'Parameter Distribution - {first_model}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed comparison\n",
    "    if len(models_to_analyze) > 1:\n",
    "        print(\"\\n=== Model Weight Similarity Analysis ===\")\n",
    "        for i, name1 in enumerate(model_names[:-1]):\n",
    "            for name2 in model_names[i+1:]:\n",
    "                comparison = compare_model_weights(\n",
    "                    models_to_analyze[name1], \n",
    "                    models_to_analyze[name2], \n",
    "                    name1, name2\n",
    "                )\n",
    "                print(f\"{name1} vs {name2}: {comparison['overall_similarity']:.3f} similarity\")\n",
    "\n",
    "else:\n",
    "    print(\"No models available for structure analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Temporal Model Performance Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze models across different training runs (temporal evolution)\n",
    "print(\"=== Temporal Model Performance Evolution ===\")\n",
    "\n",
    "def extract_timestamp(run_name):\n",
    "    \"\"\"Extract timestamp from run directory name\"\"\"\n",
    "    try:\n",
    "        timestamp_str = run_name.split('_')[-1]\n",
    "        return datetime.strptime(timestamp_str, '%Y%m%d_%H%M%S')\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Collect temporal data\n",
    "temporal_data = {'a3c': [], 'individual': []}\n",
    "\n",
    "for run_name, run_models in models_info['run_models'].items():\n",
    "    timestamp = extract_timestamp(run_name)\n",
    "    if timestamp is None:\n",
    "        continue\n",
    "    \n",
    "    run_type = 'a3c' if run_name.startswith('a3c_') else 'individual'\n",
    "    \n",
    "    # Load and evaluate models from this run\n",
    "    run_results = {'timestamp': timestamp, 'models': {}}\n",
    "    \n",
    "    for model_name, model_path in run_models.items():\n",
    "        if 'final' in model_name:  # Only analyze final models\n",
    "            model = load_model(model_path)\n",
    "            if model:\n",
    "                print(f\"Evaluating {run_name}/{model_name}...\")\n",
    "                # Use fewer episodes for temporal analysis to speed up\n",
    "                results = evaluate_model(model, env, episodes=50)\n",
    "                run_results['models'][model_name] = results\n",
    "    \n",
    "    if run_results['models']:\n",
    "        temporal_data[run_type].append(run_results)\n",
    "\n",
    "# Sort by timestamp\n",
    "for run_type in temporal_data:\n",
    "    temporal_data[run_type].sort(key=lambda x: x['timestamp'])\n",
    "\n",
    "print(f\"Found {len(temporal_data['a3c'])} A3C runs and {len(temporal_data['individual'])} individual runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temporal evolution\n",
    "if temporal_data['a3c'] or temporal_data['individual']:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. A3C Global model evolution\n",
    "    if temporal_data['a3c']:\n",
    "        a3c_times = []\n",
    "        a3c_rewards = []\n",
    "        a3c_stds = []\n",
    "        \n",
    "        for run_data in temporal_data['a3c']:\n",
    "            if 'global_final' in run_data['models']:\n",
    "                a3c_times.append(run_data['timestamp'])\n",
    "                a3c_rewards.append(run_data['models']['global_final']['mean_reward'])\n",
    "                a3c_stds.append(run_data['models']['global_final']['std_reward'])\n",
    "        \n",
    "        if a3c_times:\n",
    "            axes[0, 0].errorbar(a3c_times, a3c_rewards, yerr=a3c_stds, \n",
    "                               marker='o', capsize=5, label='A3C Global')\n",
    "            axes[0, 0].set_title('A3C Global Model Performance Over Time')\n",
    "            axes[0, 0].set_ylabel('Mean Episode Reward')\n",
    "            axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "            axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Individual worker model evolution (average)\n",
    "    if temporal_data['individual']:\n",
    "        ind_times = []\n",
    "        ind_rewards_mean = []\n",
    "        ind_rewards_std = []\n",
    "        \n",
    "        for run_data in temporal_data['individual']:\n",
    "            worker_rewards = []\n",
    "            for model_name, results in run_data['models'].items():\n",
    "                if 'individual_worker_' in model_name and 'final' in model_name:\n",
    "                    worker_rewards.append(results['mean_reward'])\n",
    "            \n",
    "            if worker_rewards:\n",
    "                ind_times.append(run_data['timestamp'])\n",
    "                ind_rewards_mean.append(np.mean(worker_rewards))\n",
    "                ind_rewards_std.append(np.std(worker_rewards))\n",
    "        \n",
    "        if ind_times:\n",
    "            axes[0, 1].errorbar(ind_times, ind_rewards_mean, yerr=ind_rewards_std, \n",
    "                               marker='s', capsize=5, label='Individual Workers (Avg)', color='orange')\n",
    "            axes[0, 1].set_title('Individual Workers Average Performance Over Time')\n",
    "            axes[0, 1].set_ylabel('Mean Episode Reward')\n",
    "            axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "            axes[0, 1].legend()\n",
    "    \n",
    "    # 3. Combined comparison\n",
    "    if a3c_times and ind_times:\n",
    "        axes[1, 0].errorbar(a3c_times, a3c_rewards, yerr=a3c_stds, \n",
    "                           marker='o', capsize=3, label='A3C Global', alpha=0.8)\n",
    "        axes[1, 0].errorbar(ind_times, ind_rewards_mean, yerr=ind_rewards_std, \n",
    "                           marker='s', capsize=3, label='Individual (Avg)', alpha=0.8)\n",
    "        axes[1, 0].set_title('Model Performance Comparison Over Time')\n",
    "        axes[1, 0].set_ylabel('Mean Episode Reward')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        axes[1, 0].legend()\n",
    "    \n",
    "    # 4. Performance improvement trends\n",
    "    improvement_data = []\n",
    "    \n",
    "    if len(a3c_rewards) > 1:\n",
    "        a3c_trend = np.polyfit(range(len(a3c_rewards)), a3c_rewards, 1)[0]\n",
    "        improvement_data.append(['A3C Global', a3c_trend, len(a3c_rewards)])\n",
    "    \n",
    "    if len(ind_rewards_mean) > 1:\n",
    "        ind_trend = np.polyfit(range(len(ind_rewards_mean)), ind_rewards_mean, 1)[0]\n",
    "        improvement_data.append(['Individual Avg', ind_trend, len(ind_rewards_mean)])\n",
    "    \n",
    "    if improvement_data:\n",
    "        df_trends = pd.DataFrame(improvement_data, columns=['Model Type', 'Trend (reward/run)', 'Runs'])\n",
    "        \n",
    "        bars = axes[1, 1].bar(df_trends['Model Type'], df_trends['Trend (reward/run)'], \n",
    "                             alpha=0.7, color=['blue', 'orange'][:len(df_trends)])\n",
    "        axes[1, 1].set_title('Performance Improvement Trends')\n",
    "        axes[1, 1].set_ylabel('Reward Improvement per Run')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, df_trends['Trend (reward/run)']):\n",
    "            axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "                            f'{value:.4f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print temporal analysis summary\n",
    "    print(\"\\n=== Temporal Analysis Summary ===\")\n",
    "    \n",
    "    if a3c_times:\n",
    "        print(f\"A3C Global models: {len(a3c_times)} runs\")\n",
    "        print(f\"  Performance range: {min(a3c_rewards):.3f} to {max(a3c_rewards):.3f}\")\n",
    "        if len(a3c_rewards) > 1:\n",
    "            print(f\"  Trend: {np.polyfit(range(len(a3c_rewards)), a3c_rewards, 1)[0]:.4f} reward/run\")\n",
    "    \n",
    "    if ind_times:\n",
    "        print(f\"Individual worker models: {len(ind_times)} runs\")\n",
    "        print(f\"  Performance range: {min(ind_rewards_mean):.3f} to {max(ind_rewards_mean):.3f}\")\n",
    "        if len(ind_rewards_mean) > 1:\n",
    "            print(f\"  Trend: {np.polyfit(range(len(ind_rewards_mean)), ind_rewards_mean, 1)[0]:.4f} reward/run\")\n",
    "\n",
    "else:\n",
    "    print(\"No temporal data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE ABLATION ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create summary dataframe\n",
    "summary_results = []\n",
    "\n",
    "# Main models summary\n",
    "if 'global' in main_results:\n",
    "    summary_results.append([\n",
    "        'Global A3C',\n",
    "        main_results['global']['mean_reward'],\n",
    "        main_results['global']['std_reward'],\n",
    "        len(main_results['global']['all_rewards']),\n",
    "        'Main Models'\n",
    "    ])\n",
    "\n",
    "if individual_results:\n",
    "    individual_means = [results['mean_reward'] for results in individual_results.values()]\n",
    "    individual_stds = [results['std_reward'] for results in individual_results.values()]\n",
    "    \n",
    "    summary_results.append([\n",
    "        'Individual Workers (Best)',\n",
    "        max(individual_means),\n",
    "        individual_stds[np.argmax(individual_means)],\n",
    "        config.evaluation_episodes,\n",
    "        'Main Models'\n",
    "    ])\n",
    "    \n",
    "    summary_results.append([\n",
    "        'Individual Workers (Avg)',\n",
    "        np.mean(individual_means),\n",
    "        np.mean(individual_stds),\n",
    "        config.evaluation_episodes,\n",
    "        'Main Models'\n",
    "    ])\n",
    "    \n",
    "    summary_results.append([\n",
    "        'Individual Workers (Worst)',\n",
    "        min(individual_means),\n",
    "        individual_stds[np.argmin(individual_means)],\n",
    "        config.evaluation_episodes,\n",
    "        'Main Models'\n",
    "    ])\n",
    "\n",
    "# Temporal data summary\n",
    "if temporal_data['a3c'] and a3c_rewards:\n",
    "    summary_results.append([\n",
    "        'A3C (Best Run)',\n",
    "        max(a3c_rewards),\n",
    "        a3c_stds[np.argmax(a3c_rewards)],\n",
    "        50,  # Episodes used for temporal analysis\n",
    "        'Temporal Analysis'\n",
    "    ])\n",
    "\n",
    "if temporal_data['individual'] and ind_rewards_mean:\n",
    "    summary_results.append([\n",
    "        'Individual (Best Run)',\n",
    "        max(ind_rewards_mean),\n",
    "        ind_rewards_std[np.argmax(ind_rewards_mean)],\n",
    "        50,\n",
    "        'Temporal Analysis'\n",
    "    ])\n",
    "\n",
    "# Create and display summary table\n",
    "if summary_results:\n",
    "    df_summary = pd.DataFrame(summary_results, \n",
    "                             columns=['Model Type', 'Mean Reward', 'Std Reward', 'Episodes', 'Analysis Type'])\n",
    "    \n",
    "    print(\"\\n=== PERFORMANCE SUMMARY ===\")\n",
    "    print(df_summary.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Key findings\n",
    "print(\"\\n=== KEY FINDINGS ===\")\n",
    "\n",
    "findings = []\n",
    "\n",
    "# Global vs Individual comparison\n",
    "if 'global' in main_results and individual_results:\n",
    "    global_mean = main_results['global']['mean_reward']\n",
    "    individual_mean = np.mean([results['mean_reward'] for results in individual_results.values()])\n",
    "    \n",
    "    if global_mean > individual_mean:\n",
    "        findings.append(f\"🏆 Global A3C model outperforms individual workers by {global_mean - individual_mean:.3f} reward points\")\n",
    "    else:\n",
    "        findings.append(f\"🔄 Individual workers outperform global model by {individual_mean - global_mean:.3f} reward points\")\n",
    "\n",
    "# Worker variability\n",
    "if individual_results and len(individual_results) > 1:\n",
    "    individual_means = [results['mean_reward'] for results in individual_results.values()]\n",
    "    variability = np.std(individual_means)\n",
    "    performance_gap = max(individual_means) - min(individual_means)\n",
    "    \n",
    "    findings.append(f\"📊 Worker performance variability: σ={variability:.3f}, gap={performance_gap:.3f}\")\n",
    "    \n",
    "    if variability > 0.1:\n",
    "        findings.append(\"⚠️  High variability detected between individual workers\")\n",
    "    else:\n",
    "        findings.append(\"✅ Consistent performance across individual workers\")\n",
    "\n",
    "# Model similarity\n",
    "if len(models_to_analyze) > 1:\n",
    "    findings.append(f\"🔍 Analyzed {len(models_to_analyze)} models for structural similarity\")\n",
    "\n",
    "# Temporal trends\n",
    "if improvement_data:\n",
    "    for model_type, trend, runs in improvement_data:\n",
    "        if trend > 0:\n",
    "            findings.append(f\"📈 {model_type} shows positive improvement trend: +{trend:.4f} reward/run over {runs} runs\")\n",
    "        elif trend < 0:\n",
    "            findings.append(f\"📉 {model_type} shows declining trend: {trend:.4f} reward/run over {runs} runs\")\n",
    "        else:\n",
    "            findings.append(f\"➡️  {model_type} shows stable performance over {runs} runs\")\n",
    "\n",
    "# Display findings\n",
    "for i, finding in enumerate(findings, 1):\n",
    "    print(f\"{i}. {finding}\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n=== RECOMMENDATIONS ===\")\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "if 'global' in main_results and individual_results:\n",
    "    global_mean = main_results['global']['mean_reward']\n",
    "    individual_mean = np.mean([results['mean_reward'] for results in individual_results.values()])\n",
    "    \n",
    "    if global_mean > individual_mean:\n",
    "        recommendations.append(\"Use A3C global model for deployment as it shows superior performance\")\n",
    "        recommendations.append(\"Consider ensemble methods combining global and best individual workers\")\n",
    "    else:\n",
    "        recommendations.append(\"Individual training may be more effective for this environment\")\n",
    "        recommendations.append(\"Investigate A3C hyperparameters and training procedure\")\n",
    "\n",
    "if individual_results and len(individual_results) > 1:\n",
    "    variability = np.std([results['mean_reward'] for results in individual_results.values()])\n",
    "    if variability > 0.1:\n",
    "        recommendations.append(\"Investigate sources of worker variability (initialization, data, hyperparameters)\")\n",
    "        recommendations.append(\"Consider worker-specific hyperparameter tuning\")\n",
    "\n",
    "if temporal_data['a3c'] or temporal_data['individual']:\n",
    "    recommendations.append(\"Continue temporal monitoring to track model performance evolution\")\n",
    "    recommendations.append(\"Implement automated model selection based on validation performance\")\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Analysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV for further analysis\n",
    "if summary_results:\n",
    "    df_results = pd.DataFrame(summary_results, \n",
    "                             columns=['Model Type', 'Mean Reward', 'Std Reward', 'Episodes', 'Analysis Type'])\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    results_file = f'ablation_analysis_results_{timestamp}.csv'\n",
    "    df_results.to_csv(results_file, index=False)\n",
    "    print(f\"Results saved to: {results_file}\")\n",
    "    \n",
    "    # Display final results table\n",
    "    print(\"\\n=== FINAL RESULTS TABLE ===\")\n",
    "    print(df_results.round(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
